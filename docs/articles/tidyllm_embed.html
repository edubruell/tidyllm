<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Embedding Models in tidyllm • tidyllm</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Embedding Models in tidyllm">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyllm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/tidyllm.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/tidyllm_classifiers.html">Classifying Texts with tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm_embed.html">Embedding Models in tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm_video.html">Video and Audio Data with the Gemini API</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm-pdfquestions.html">Structured Question Answering from PDFs</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/edubruell/tidyllm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">



<script src="tidyllm_embed_files/kePrint-0.0.1/kePrint.js"></script><link href="tidyllm_embed_files/lightable-0.0.1/lightable.css" rel="stylesheet">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Embedding Models in tidyllm</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/edubruell/tidyllm/blob/HEAD/vignettes/articles/tidyllm_embed.Rmd" class="external-link"><code>vignettes/articles/tidyllm_embed.Rmd</code></a></small>
      <div class="d-none name"><code>tidyllm_embed.Rmd</code></div>
    </div>

    
    
<p>While most <strong>tidyllm</strong> use cases revolve around chat
models, it also supports <strong>embedding models</strong> — another
type of large language model. These models are designed to generate
numerical vectors, which map input text to points in a high-dimensional
space. Each point represents the semantic meaning of the text.:</p>
<ul>
<li>
<strong>Similar meanings are close together:</strong> For example, a
text about a “cat” and another about a “kitten” would be mapped to
nearby points.</li>
<li>
<strong>Different meanings are farther apart:</strong> Conversely, a
text about a “cat” and one about a “car” would have points that are much
farther apart.</li>
</ul>
<div class="section level2">
<h2 id="semantic-search-in-economics-paper-abstracts">Semantic Search in Economics Paper Abstracts<a class="anchor" aria-label="anchor" href="#semantic-search-in-economics-paper-abstracts"></a>
</h2>
<p>To demonstrate embeddings in action, we’ll implement a
<strong>semantic search</strong> on a dataset of <strong>22,960
economics paper abstracts published between 2010 and 2024</strong>.
Instead of relying on keyword matching, we’ll use embeddings to find
papers with similar topics based on their underlying meaning.</p>
<p>Let’s start by loading and exploring the data:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org" class="external-link">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://edubruell.github.io/tidyllm/">tidyllm</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://here.r-lib.org/" class="external-link">here</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">abstracts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://readr.tidyverse.org/reference/read_rds.html" class="external-link">read_rds</a></span><span class="op">(</span><span class="st">"abstracts_data.rds"</span><span class="op">)</span></span>
<span><span class="co">#The structure of our file:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">abstracts</span>,width <span class="op">=</span> <span class="fl">60</span><span class="op">)</span></span>
<span><span class="co">## <span style="color: #949494;"># A tibble: 22,960 × 8</span></span></span>
<span><span class="co">##    year  journal  authors volume firstpage lastpage abstract</span></span>
<span><span class="co">##    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>   </span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 1</span> 2024  Journal… Bauer,… 22     2075      2107     This pa…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 2</span> 2019  The Rev… Karlan… 86     1704      1746     We use …</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 3</span> 2022  Journal… Corset… 20     513       548      We stud…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 4</span> 2018  The Rev… Anagol… 85     1971      2004     We stud…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 5</span> 2024  America… Thores… 16     447       79       This pa…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 6</span> 2024  Journal… Ren, Y… 238    <span style="color: #BB0000;">NA</span>        <span style="color: #BB0000;">NA</span>       The rap…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 7</span> 2013  The Rev… Adhvar… 95     725       740      A key p…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 8</span> 2022  Econome… Brooks… 90     2187      2214     If expe…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 9</span> 2011  Health … Fletch… 20     553       570      We exam…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">10</span> 2010  Journal… Rohwed… 24     119       38       Early r…</span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 22,950 more rows</span></span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 1 more variable: pdf_link &lt;chr&gt;</span></span></span>
<span></span>
<span><span class="va">target_abstract</span> <span class="op">&lt;-</span>  <span class="st">"We use the expansion of the high-speed rail network in Germany  as a natural experiment to examine the causal effect of reductions in commuting time  betweenregions on the commuting decisions of workers and their choices regarding  where tolive and where to work. We exploit three key features in this setting:i) investmentin high-speed rail has, in some cases dramatically, reduced travel times between regions,ii) several small towns were connected to the high-speed rail network onlyfor political reasons, and iii) high-speed trains have left the transportation of goodsunaffected. Combining novel information on train schedules and the opening ofhigh-speed rail stations with panel data on all workers in Germany, we show that a reduction in travel time by one percent raises the number of commuters betweenregions by 0.25 percent. This effect is mainly driven by workers changing jobs to smaller cities while keeping their place of residence in larger ones. Our findings support the notion that benefits from infrastructure investments accrue in particular to peripheral regions, which gain access to a large pool of qualified workers with a preference for urban life. We find that the introduction of high-speed trains led to a modal shift towards rail transportation in particular on medium distances between 150 and 400 kilometers."</span></span></code></pre></div>
<p>With the dataset ready, we’ll now use tidyllm to generate embeddings
and perform a semantic search to find papers that are similar to the
<code>target_abstract</code> from a Paper on commuting and the expansion
of high-speed rail by <a href="https://drive.google.com/file/d/1rKhTsrc9Qsi2gRUQg86zKKuUWvzx5HE1/view" class="external-link">Heuerman
and Schmieder (2018)</a>. For this task, we’ll use the
<code>mxbai-embed-large</code> model. This model, developed by <a href="https://www.mixedbread.ai/" class="external-link">Mixedbread.ai</a>, achieves
state-of-the-art performance among efficiently sized models and
outperforms closed-source models like OpenAI’s
<code>text-embedding-ada-002</code>. If you have Ollama installed, you
can download it using
<code>ollama_download_model("mxbai-embed-large")</code>. It is important
to choose your embedding model carefully upfront, as each model produces
unique numerical representations of text that are not interchangeable
between models.</p>
<p>Alternatively, embedding APIs are also available for
<code><a href="../reference/mistral.html">mistral()</a></code>, <code><a href="../reference/gemini.html">gemini()</a></code>, and <code><a href="../reference/openai.html">openai()</a></code>
(as well as <code><a href="../reference/azure_openai.html">azure_openai()</a></code>).</p>
<div class="section level3">
<h3 id="step-1-computing-embeddings-for-one-abstract">Step 1: Computing Embeddings for one abstract<a class="anchor" aria-label="anchor" href="#step-1-computing-embeddings-for-one-abstract"></a>
</h3>
<p>To compute an embedding of the target abstract we use the
<code><a href="../reference/embed.html">embed()</a></code> function with <code><a href="../reference/ollama.html">ollama()</a></code> as
provider-function:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">target_tbl</span> <span class="op">&lt;-</span> <span class="va">target_abstract</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/embed.html">embed</a></span><span class="op">(</span><span class="va">ollama</span>,.model<span class="op">=</span><span class="st">"mxbai-embed-large:latest"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">target_tbl</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">target_tbl</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 1 × 2</span></span></span>
<span><span class="co">##   input                                           embeddings</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                                           <span style="color: #949494; font-style: italic;">&lt;list&gt;</span>    </span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> We use the expansion of the high-speed rail ne… <span style="color: #949494;">&lt;dbl&gt;</span></span></span></code></pre>
<p>The <code><a href="../reference/embed.html">embed()</a></code> function returns a <code>tibble</code> with
two columns: - <strong>input:</strong> The original text provided for
embedding. - <strong>embeddings:</strong> A list column containing the
numerical vector representation (embedding) for each input text.</p>
<p>In our case we have a single input and the embeddings column contains
a 1,024-dimenstional vector for this input:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">target_tbl</span><span class="op">$</span><span class="va">embeddings</span><span class="op">)</span></span>
<span><span class="co">## List of 1</span></span>
<span><span class="co">##  $ : num [1:1024] -0.838 0.669 0.202 -0.686 -0.985 ...</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="step-2-computing-embeddings-for-the-entire-abstract-corpus">Step 2: Computing Embeddings for the entire abstract corpus<a class="anchor" aria-label="anchor" href="#step-2-computing-embeddings-for-the-entire-abstract-corpus"></a>
</h3>
<p>When working with a large corpus like our <strong>22,960</strong>
abstracts, embedding all entries in a single pass using
<code><a href="../reference/embed.html">embed()</a></code> is impractical and often leads to errors. For
commercial APIs, there are typically strict limits on the number of
inputs allowed per request (usually there are caps of 50 or a 100
inputs). For local APIs, resource constraints such as memory and
processing power impose similar restrictions.</p>
<p>To efficiently handle this, we batch the data, processing a
manageable number of abstracts at a time. The
<code>generate_abstract_embeddings()</code> function below takes a
vector of abstracts as input and divides them into manageable batches of
200. For each batch, it uses the <code><a href="../reference/embed.html">embed()</a></code> function to
compute embeddings via <code><a href="../reference/ollama.html">ollama()</a></code>. Progress is logged to the
console to keep track of batch completion and provide a clear view of
the process.</p>
<p>Since long-running processes are prone to interruptions, such as
network timeouts or unexpected system errors, it saves the results to
disk as <code>.rds</code> files (consider
<code><a href="https://arrow.apache.org/docs/r/reference/write_parquet.html" class="external-link">arrow::write_parquet()</a></code> or a database for really big
workloads). On a MacBook Pro with an M1 Pro processor, this function
completes embedding the entire dataset in approximately <strong>25
minutes</strong> and writes 207 MB of data to disk. The time may vary
depending on system specifications and batch size. To compare multiple
target abstracts against the entire collection, we of course only need
to embed it once.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Our batches embedding function</span></span>
<span><span class="va">generate_abstract_embeddings</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">abstracts</span><span class="op">)</span><span class="op">{</span></span>
<span>  </span>
<span>  <span class="co">#Preapre abstract batches</span></span>
<span>  <span class="va">embedding_batches</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble</a></span><span class="op">(</span>abstract <span class="op">=</span> <span class="va">abstracts</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html" class="external-link">group_by</a></span><span class="op">(</span>batch <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">floor</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html" class="external-link">n</a></span><span class="op">(</span><span class="op">)</span> <span class="op">/</span> <span class="fl">200</span><span class="op">)</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_split.html" class="external-link">group_split</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co">#Work with batches of 200 abstracts</span></span>
<span>  <span class="va">n_batches</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">embedding_batches</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"Processing {n_batches} batches of 200 abstracts"</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\n"</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co">#Embed the batches via ollama mxbai-embed-large </span></span>
<span>  <span class="va">embedding_batches</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>    <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">walk</a></span><span class="op">(</span><span class="op">~</span><span class="op">{</span></span>
<span>      <span class="va">batch_number</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">.x</span>,<span class="va">batch</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/unique.html" class="external-link">unique</a></span><span class="op">(</span><span class="op">)</span> </span>
<span>      <span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"Generate Text Embeddings for Abstract Batch: {batch_number}/{n_batches}"</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\n"</span><span class="op">)</span></span>
<span>      </span>
<span>      <span class="va">emb_matrix</span> <span class="op">&lt;-</span> <span class="va">.x</span><span class="op">$</span><span class="va">abstract</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>        <span class="fu"><a href="../reference/embed.html">embed</a></span><span class="op">(</span><span class="va">ollama</span>,.model<span class="op">=</span><span class="st">"mxbai-embed-large:latest"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>        <span class="fu"><a href="https://readr.tidyverse.org/reference/read_rds.html" class="external-link">write_rds</a></span><span class="op">(</span><span class="fu"><a href="https://here.r-lib.org//reference/here.html" class="external-link">here</a></span><span class="op">(</span><span class="st">"embedded_asbtracts"</span>,<span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="va">batch_number</span>,<span class="st">".rds"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">#Run the function over all abstracts</span></span>
<span><span class="va">abstracts</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">abstract</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">generate_abstract_embeddings</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Processing 115 batches of 200 abstracts</span></span>
<span><span class="co">## Generate Text Embeddings for Abstract Batch: 1/115</span></span>
<span><span class="co">## Generate Text Embeddings for Abstract Batch: 2/115 </span></span>
<span><span class="co">## Generate Text Embeddings for Abstract Batch: 3/115</span></span>
<span><span class="co">## ...</span></span></code></pre>
<p>After the function has finished, we only need to load the computed
embeddings:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedded_asbtracts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://here.r-lib.org//reference/here.html" class="external-link">here</a></span><span class="op">(</span><span class="st">"embedded_asbtracts"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.files.html" class="external-link">dir</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/map_dfr.html" class="external-link">map_dfr</a></span><span class="op">(</span><span class="op">~</span><span class="fu"><a href="https://readr.tidyverse.org/reference/read_rds.html" class="external-link">read_rds</a></span><span class="op">(</span><span class="fu"><a href="https://here.r-lib.org//reference/here.html" class="external-link">here</a></span><span class="op">(</span><span class="st">"embedded_asbtracts"</span>,<span class="va">.x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="step-3-performing-the-semantic-search">Step 3: Performing the Semantic Search<a class="anchor" aria-label="anchor" href="#step-3-performing-the-semantic-search"></a>
</h3>
<p>With the embeddings precomputed, we can now perform a semantic search
to find abstracts most similar to our target. For the search we will use
cosine similarity to compare embedding vectors. <strong>Cosine
similarity</strong> measures the similarity between two vectors by
calculating the cosine of the angle between them. It ranges from
<strong>-1</strong> (opposite directions) to <strong>1</strong>
(identical directions). The formula is:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">cosine_similarity</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐚</mi><mo>,</mo><mi>𝐛</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>a</mi><mi>i</mi></msub><msub><mi>b</mi><mi>i</mi></msub></mrow><mrow><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt><mo>⋅</mo><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>b</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">
\text{cosine_similarity}(\mathbf{a}, \mathbf{b}) = \frac{\sum_{i=1}^n a_i b_i}{\sqrt{\sum_{i=1}^n a_i^2} \cdot \sqrt{\sum_{i=1}^n b_i^2}}
</annotation></semantics></math></p>
<p>Where:</p>
<ul>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∑</mo><msub><mi>a</mi><mi>i</mi></msub><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\sum a_i b_i</annotation></semantics></math>
is the <strong>dot product</strong>.</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mrow><mo>∑</mo><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt><annotation encoding="application/x-tex">\sqrt{\sum a_i^2}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mrow><mo>∑</mo><msubsup><mi>b</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt><annotation encoding="application/x-tex">\sqrt{\sum b_i^2}</annotation></semantics></math>
are the magnitudes of the vectors.</li>
</ul>
<p>Vectors in an embedding space represent semantic meanings of texts.
In this space:</p>
<ul>
<li>Cosine similarity focuses on direction rather than magnitude.</li>
<li>Two vectors pointing in similar directions (small angle) will have a
cosine similarity close to <strong>1</strong>.</li>
<li>Vectors at 90 degrees (orthogonal, no semantic overlap) have a
cosine similarity of <strong>0</strong>.</li>
<li>Vectors pointing in opposite directions (large angle, entirely
dissimilar) have a cosine similarity of <strong>-1</strong>.</li>
</ul>
<p>With the model we use each embedding vector represents a point in a
high-dimensional space with 1,024 dimensions. Even though these
dimensions are not spatially interpretable like in 2D or 3D, the
underlying principle still holds: cosine similarity measures how much
two vectors “lean” in the same direction. If two vectors have a cosine
similarity of 0.25, it means the angle between them is relatively small,
implying they share 25% of their directional alignment.</p>
<p>To compute cosine similarity between two vectors we express it in a
simple function:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cosine_similarity</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">a</span>, <span class="va">b</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">a</span> <span class="op">*</span> <span class="va">b</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">a</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">b</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>We apply this function to compute the cosine similarity of abstracts
in the corpus to the target paper to find the 10 most similar
abstracts:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">top10_similar</span> <span class="op">&lt;-</span> <span class="va">embedded_asbtracts</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>cosine_sim <span class="op">=</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map_dbl</a></span><span class="op">(</span><span class="va">embeddings</span>, </span>
<span>                              <span class="op">~</span><span class="fu">cosine_similarity</span><span class="op">(</span><span class="va">.x</span>, </span>
<span>                                                 <span class="va">target_tbl</span><span class="op">$</span><span class="va">embeddings</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/arrange.html" class="external-link">arrange</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/desc.html" class="external-link">desc</a></span><span class="op">(</span><span class="va">cosine_sim</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html" class="external-link">slice</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/rename.html" class="external-link">rename</a></span><span class="op">(</span>abstract<span class="op">=</span><span class="va">input</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate-joins.html" class="external-link">left_join</a></span><span class="op">(</span><span class="va">abstracts</span> <span class="op">|&gt;</span></span>
<span>              <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="va">year</span>,<span class="va">authors</span>,<span class="va">journal</span>,<span class="va">abstract</span><span class="op">)</span>, by<span class="op">=</span><span class="st">"abstract"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">embeddings</span><span class="op">)</span></span></code></pre></div>
<p>The top ten most similar articles in the corpus based on our search
are these:</p>
<table class="table table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
Top 10 Most Similar Abstracts
</caption>
<thead><tr>
<th style="text-align:left;">
abstract
</th>
<th style="text-align:right;">
cosine_sim
</th>
<th style="text-align:left;">
year
</th>
<th style="text-align:left;">
authors
</th>
<th style="text-align:left;">
journal
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
We use the expansion of the high-speed rail (HSR) network in Germany as
a natural experiment to examine the causal effect of reductions in
commuting time between regions on the commuting decisions of workers and
their choices regarding where to live and where to work. We exploit
three key features in this setting: (i) investment in HSR has, in some
cases dramatically, reduced travel times between regions, (ii) several
small towns were connected to the HSR network only for political
reasons, and (iii) high-speed trains have left the transportation of
goods unaffected. Combining novel information on train schedules and the
opening of HSR stations with panel data on all workers in Germany, we
show that a reduction in travel time by 1% raises the number of
commuters between regions by 0.25%. This effect is mainly driven by
workers changing jobs to smaller cities while keeping their place of
residence in larger ones. Our findings support the notion that benefits
from infrastructure investments accrue in particular to peripheral
regions, which gain access to a large pool of qualified workers with a
preference for urban life. We find that the introduction of high-speed
trains led to a modal shift toward rail transportation in particular on
medium distances between 150 and 400 km.
</td>
<td style="text-align:right;">
0.9935939
</td>
<td style="text-align:left;">
2019
</td>
<td style="text-align:left;">
Heuermann, Daniel; Schmieder, Johannes
</td>
<td style="text-align:left;">
Journal of Economic Geography
</td>
</tr>
<tr>
<td style="text-align:left;">
We analyze the economic impact of the German high-speed rail (HSR)
connecting Cologne and Frankfurt, which provides plausibly exogenous
variation in access to surrounding economic mass. We find a causal
effect of about 8.5% on average of the HSR on the GDP of three counties
with intermediate stops. We make further use of the variation in
bilateral transport costs between all counties in our study area induced
by the HSR to identify the strength and spatial scope of agglomeration
forces. Our most careful estimate points to an elasticity of output with
respect to market potential of 12.5%. The strength of the spillover
declines by 50% every 30 min of travel time, diminishing to 1% after
about 200 min. Our results further imply an elasticity of per-worker
output with respect to economic density of 3.8%, although the effects
seem driven by worker and firm selection.
</td>
<td style="text-align:right;">
0.8984364
</td>
<td style="text-align:left;">
2018
</td>
<td style="text-align:left;">
Ahlfeldt, Gabriel; Feddersen, Arne
</td>
<td style="text-align:left;">
Journal of Economic Geography
</td>
</tr>
<tr>
<td style="text-align:left;">
We investigate whether localities gain or lose employment when there are
connected to a transportation network, such as a high-speed railway
line. We argue that long-haul economies—implying that the marginal
transportation cost decreases with network distance—play a pivotal role
in understanding the location choices of firms. We develop a new spatial
model to show that improvements in transportation infrastructure have
nontrivial impacts on the location choices of firms. Using data on
Japan’s Shinkansen, we show that ‘in-between’ municipalities that are
connected to the Shinkansen witness a sizable decrease in employment.
</td>
<td style="text-align:right;">
0.8691969
</td>
<td style="text-align:left;">
2022
</td>
<td style="text-align:left;">
Koster, Hans; Tabuchi, Takatoshi; Thisse, Jacques
</td>
<td style="text-align:left;">
Journal of Economic Geography
</td>
</tr>
<tr>
<td style="text-align:left;">
How does intercity passenger transportation shape urban employment and
specialization patterns? To shed light on this question I study China’s
High Speed Railway (HSR), an unprecedentedly large-scale network that
connected 81 cities from 2003 to 2014 with trains running at speeds over
200 km/h. Using a difference-in-differences approach, I find that an HSR
connection increases city-wide passenger flows by 10% and employment by
7%. To deal with the issues of endogenous railway placement and
simultaneous public investments accompanying HSR connection, I examine
the impact of a city’s market access changes purely driven by the HSR
connection of other cities. The estimates suggest that HSR-induced
expansion in market access increases urban employment with an elasticity
between 2 and 2.5. Further evidence on sectoral employment suggests that
industries with a higher reliance on nonroutine cognitive skills benefit
more from HSR-induced market access to other cities.
</td>
<td style="text-align:right;">
0.8684935
</td>
<td style="text-align:left;">
2017
</td>
<td style="text-align:left;">
Lin, Yatang
</td>
<td style="text-align:left;">
Journal of Urban Economics
</td>
</tr>
<tr>
<td style="text-align:left;">
We use the natural experiment provided by the opening and progressive
extension of the Regional Express Rail (RER) between 1970 and 2000 in
the Paris metropolitan region, and in particular the departure from the
original plans due to budget constraints and technical considerations,
to identify the causal impact of urban rail transport on firm location,
employment and population growth. We apply a difference-in-differences
method to a particular subsample, selected to minimize the endogeneity
that is routinely found in the evaluation of the effects of transport
infrastructure. We find that the RER opening caused a 8.8% rise in
employment in the municipalities connected to the network between 1975
and 1990. While we find no effect on overall population growth, our
results suggest that the arrival of the RER may have increased
competition for land, since high-skilled households were more likely to
locate in the vicinity of a RER station.
</td>
<td style="text-align:right;">
0.8664813
</td>
<td style="text-align:left;">
2017
</td>
<td style="text-align:left;">
Mayer, Thierry; Trevien, Corentin
</td>
<td style="text-align:left;">
Journal of Urban Economics
</td>
</tr>
<tr>
<td style="text-align:left;">
Infrastructure investment may reshape economic activities. In this
article, I examine the distributional impacts of high-speed rail
upgrades in China, which have improved passengers’ access to high-speed
train services in the city nodes but have left the peripheral counties
along the upgraded railway lines bypassed by the services. By exploiting
the quasi-experimental variation in whether counties were affected by
this project, my analysis suggests that the affected counties on the
upgraded railway lines experienced reductions in GDP and GDP per capita
following the upgrade, which was largely driven by the concurrent drop
in fixed asset investments. This article provides the first empirical
evidence on how transportation costs of people affect urban peripheral
patterns.
</td>
<td style="text-align:right;">
0.8579637
</td>
<td style="text-align:left;">
2017
</td>
<td style="text-align:left;">
Qin, Yu
</td>
<td style="text-align:left;">
Journal of Economic Geography
</td>
</tr>
<tr>
<td style="text-align:left;">
Many US cities have made large investments in light rail transit in
order to improve commuting networks. I analyse the labour market effects
of light rail in four US metros. I propose a new instrumental variable
to overcome endogeneity in transit station location, enabling causal
identification of neighbourhood effects. Light rail stations are found
to drastically improve employment outcomes in the surrounding
neighbourhood. To incorporate endogenous sorting by workers, I estimate
a structural neighbourhood choice model. Light rail systems tend to
raise rents in accessible locations, displacing lower skilled workers to
isolated neighbourhoods, which reduces aggregate metropolitan employment
in equilibrium.
</td>
<td style="text-align:right;">
0.8559837
</td>
<td style="text-align:left;">
2021
</td>
<td style="text-align:left;">
Tyndall, Justin
</td>
<td style="text-align:left;">
Journal of Urban Economics
</td>
</tr>
<tr>
<td style="text-align:left;">
I study Los Angeles Metro Rail’s effects using panel data on bilateral
commuting flows, a quantitative spatial model, and historically
motivated quasi-experimental research designs. The model separates
transit’s commuting effects from local productivity or amenity effects,
and spatial shift-share instruments identify inelastic labor and housing
supply. Metro Rail connections increase commuting by 16% but do not have
large effects on local productivity or amenities. Metro Rail generates
$94 million in annual benefits by 2000 or 12â€“25% of annualized costs.
Accounting for reduced congestion and slow transit adoption adds, at
most, another $200 million in annual benefits.
</td>
<td style="text-align:right;">
0.8432086
</td>
<td style="text-align:left;">
2023
</td>
<td style="text-align:left;">
Severen, Christopher
</td>
<td style="text-align:left;">
The Review of Economics and Statistics
</td>
</tr>
<tr>
<td style="text-align:left;">
We examine the effect of commuting distance on workers’ labour supply
patterns, distinguishing between weekly labour supply, number of
workdays per week and daily labour supply. We account for endogeneity of
distance by using employer-induced changes in distance. In Germany,
distance has a slight positive effect on daily and weekly labour supply,
but no effect on the number of workdays. The effect of distance on
labour supply patterns is stronger for female workers, but it is still
small.
</td>
<td style="text-align:right;">
0.8416284
</td>
<td style="text-align:left;">
2010
</td>
<td style="text-align:left;">
Gutiérrez-i-Puigarnau, Eva; van Ommeren, Jos
</td>
<td style="text-align:left;">
Journal of Urban Economics
</td>
</tr>
<tr>
<td style="text-align:left;">
We estimate the causal impact of wage variations on commuting distance
of workers. We test whether higher wages across years lead workers to
live further away from their working place. We use employer–employee
data for the French Ile-de-France region (surrounding Paris), from 2003
to 2008, and we deal with the endogenous relation between income and
commuting using an instrumental variable strategy. We estimate that
increases in wages coming from exogenous exposure to trade activities
lead workers to increase their commuting distance and to settle closer
to the city of Paris historical center. Our results cast novel insights
upon the causal mechanisms from wage to spatial allocation of workers.
</td>
<td style="text-align:right;">
0.8367841
</td>
<td style="text-align:left;">
2022
</td>
<td style="text-align:left;">
Aboulkacem, El-Mehdi; Nedoncelle, Clément
</td>
<td style="text-align:left;">
Journal of Economic Geography
</td>
</tr>
</tbody>
</table>
<p>Unsurprisingly, the top result is the target abstract itself,
presented with slight formatting differences. However, the remaining top
results strongly align with the target’s thematic focus on the economic
and social impacts of transportation (or specifically high speed reail)
infrastructure. The second result discusses the economic effects of
German High-Speed Rail (HSR), while the third and fourth focus on
Japan’s Shinkansen and China’s HSR network, respectively. These findings
highlight the model’s capability to identify semantically rich
connections across diverse contexts, illustrating its ability to capture
complex thematic overlaps.</p>
</div>
<div class="section level3">
<h3 id="sidenote-multimodal-embeddings">Sidenote: Multimodal Embeddings<a class="anchor" aria-label="anchor" href="#sidenote-multimodal-embeddings"></a>
</h3>
<p>While most embedding functions work only on text, the
<strong>Voyage.ai</strong> functions in tidyllm allow embedding both
text and images in the same space. This feature is particularly useful
for <strong>cross-modal search</strong>, where text descriptions and
images need to be compared on a semantic level.</p>
<p>With <strong>tidyllm</strong>, you can use the <code><a href="../reference/img.html">img()</a></code>
function to create image objects and mix them with text in a list. When
passing such a list to <code><a href="../reference/voyage_embedding.html">voyage_embedding()</a></code>, the function
automatically switches to Voyage’s multimodal API. Suppose we want to
compare a textual description of an object with an image embedding to
see if they align in meaning.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Define a text description and an image of the same object</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"a dish consisting of a sausage served in the slit of a partially sliced bun"</span>, <span class="fu"><a href="../reference/img.html">img</a></span><span class="op">(</span><span class="fu"><a href="https://here.r-lib.org//reference/here.html" class="external-link">here</a></span><span class="op">(</span><span class="st">"hotdog.jpg"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span> <span class="fu"><a href="../reference/embed.html">embed</a></span><span class="op">(</span><span class="va">voyage</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 2 × 2</span></span></span>
<span><span class="co">##   input                                                               embeddings</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                                                               <span style="color: #949494; font-style: italic;">&lt;list&gt;</span>    </span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> A dish consisting of a sausage served in the slit of a partially s… <span style="color: #949494;">&lt;dbl&gt;</span>     </span></span>
<span><span class="co">## <span style="color: #BCBCBC;">2</span> [IMG] hotdog.jpg                                                    <span style="color: #949494;">&lt;dbl&gt;</span></span></span></code></pre>
<p>Since the text description and the image refer to the same concept (a
hotdog), their embeddings should be <strong>close together in the
high-dimensional space</strong>, enabling similarity-based retrieval and
comparison.</p>
</div>
<div class="section level3">
<h3 id="outlook-clustering-and-beyond">Outlook: Clustering and Beyond<a class="anchor" aria-label="anchor" href="#outlook-clustering-and-beyond"></a>
</h3>
<p>While this article focused on semantic search, embeddings open the
door to a wide range of advanced analytical techniques.</p>
<p>Here are some potential further use-cases for embeddings:</p>
<ul>
<li><p><strong>Clustering for Topic Discovery:</strong> Embeddings can
be leveraged with unsupervised learning methods like K-means or
hierarchical clustering to automatically group text into topics or
themes. This approach is particularly beneficial for exploratory
research, enabling users to uncover hidden patterns and structures
within large corpora without predefined labels.</p></li>
<li><p><strong>Dimensionality Reduction and Visualization:</strong>
High-dimensional embedding vectors often need simplification for human
interpretation. Techniques like <strong>Principal Component Analysis
(PCA)</strong>, <strong>t-SNE</strong>, or <strong>UMAP</strong> allow
us to project embeddings into lower-dimensional spaces (e.g., 2D or 3D).
These visualizations can reveal clusters, trends, and outliers,
providing insights at a glance.</p></li>
<li><p><strong>Retrieval augmented generation (RAG):</strong> Embeddings
play a crucial role in retrieval-augmented generation, a technique that
enhances large language models (LLMs). In this workflow, an embedding
search retrieves semantically similar documents from a corpus, which are
then appended to the LLM’s prompt. This process helps the model generate
contextually rich and accurate responses, especially in
knowledge-intensive tasks.</p></li>
<li><p><strong>Supporting Workflows in Qualitative Research:</strong>
Embeddings can also transform workflows in qualitative research, as
discussed in <a href="https://www.iaw.edu/iaw-diskussionspapiere.html?file=files/dokumente/ab%20Januar%202023/iaw_dp_143.pdf" class="external-link">this
paper by Kugler et al. (2023)</a>. The integration of Natural Language
Processing (NLP) tools enables researchers to automate coding steps that
traditionally require manual effort. Specifically, embeddings can assist
in categorizing text data according to predefined themes, making
research workflows more efficient and transparent. While these models
bring significant advantages, challenges remain. The study highlights
that off-the-shelf language models often struggle to discern implicit
references and closely related topics as effectively as human
researchers. However, more modern embedding models than the ones used in
the study might help to deal with some of these challenges.</p></li>
</ul>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Eduard Brüll.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>

<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Structured Question Answering from PDFs • tidyllm</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Structured Question Answering from PDFs">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyllm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.4</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/tidyllm.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/tidyllm_classifiers.html">Classifying Texts with tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm_embed.html">Embedding Models in tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm_video.html">Video and Audio Data with the Gemini API</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm-pdfquestions.html">Structured Question Answering from PDFs</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/edubruell/tidyllm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Structured Question Answering from PDFs</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/edubruell/tidyllm/blob/HEAD/vignettes/articles/tidyllm-pdfquestions.Rmd" class="external-link"><code>vignettes/articles/tidyllm-pdfquestions.Rmd</code></a></small>
      <div class="d-none name"><code>tidyllm-pdfquestions.Rmd</code></div>
    </div>

    
    
<p>Navigating through a large collection of academic papers can be
time-consuming, especially when you’re trying to extract specific
insights or determine the relevance of each document to your research.
With <strong>tidyllm</strong>, you can streamline this process by
automating the extraction of structured answers directly from PDF
documents using large language models.</p>
<p>Imagine you have a folder of papers on the economic effects of
generative AI, and you need to assess how each paper is related to your
own research interests. This article will guide you through setting up a
workflow that processes the first few pages of each paper, asks an AI
model targeted questions, and returns the answers in a structured format
— perfect for converting into a table for easy review and analysis.</p>
<div class="section level2">
<h2 id="example-workflow">Example Workflow<a class="anchor" aria-label="anchor" href="#example-workflow"></a>
</h2>
<p>Imagine your folder looks something like this—many downloaded papers,
but no structure yet:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org" class="external-link">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://edubruell.github.io/tidyllm/">tidyllm</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/list.files.html" class="external-link">dir</a></span><span class="op">(</span><span class="st">"aipapers"</span><span class="op">)</span></span>
<span><span class="co">##  [1] "2018_Felten_etal_AILinkOccupations.pdf"                                                           </span></span>
<span><span class="co">##  [2] "2024_Bick_etal_RapidAdoption.pdf"                                                                 </span></span>
<span><span class="co">##  [3] "2024_Caplin_etal_ABCsofAI.pdf"                                                                    </span></span>
<span><span class="co">##  [4] "2301.07543v1.pdf"                                                                                 </span></span>
<span><span class="co">##  [5] "2302.06590v1.pdf"                                                                                 </span></span>
<span><span class="co">##  [6] "2303.10130v5.pdf"                                                                                 </span></span>
<span><span class="co">##  [7] "488.pdf"                                                                                          </span></span>
<span><span class="co">##  [8] "88684e36-en.pdf"                                                                                  </span></span>
<span><span class="co">##  [9] "ABCs_AI_Oct2024.pdf"                                                                              </span></span>
<span><span class="co">## [10] "acemoglu-restrepo-2019-automation-and-new-tasks-how-technology-displaces-and-reinstates-labor.pdf"</span></span>
<span><span class="co">## [11] "BBD_GenAI_NBER_Sept2024.pdf"                                                                      </span></span>
<span><span class="co">## [12] "Deming-Ong-Summers-AESG-2024.pdf"                                                                 </span></span>
<span><span class="co">## [13] "dp22036.pdf"                                                                                      </span></span>
<span><span class="co">## [14] "FeltenRajSeamans_AIAbilities_AEA.pdf"                                                             </span></span>
<span><span class="co">## [15] "JEL-2023-1736_published_version.pdf"                                                              </span></span>
<span><span class="co">## [16] "Noy_Zhang_1.pdf"                                                                                  </span></span>
<span><span class="co">## [17] "sd-2024-09-falck-etal-kuenstliche-intelligenz-unternehmen.pdf"                                    </span></span>
<span><span class="co">## [18] "ssrn-4700751.pdf"                                                                                 </span></span>
<span><span class="co">## [19] "SSRN-id4573321.pdf"                                                                               </span></span>
<span><span class="co">## [20] "The Simple Macroeconomics of AI.pdf"                                                              </span></span>
<span><span class="co">## [21] "w24001.pdf"                                                                                       </span></span>
<span><span class="co">## [22] "w24871.pdf"                                                                                       </span></span>
<span><span class="co">## [23] "w31161.pdf"                                                                                       </span></span>
<span><span class="co">## [24] "w32430.pdf"</span></span></code></pre></div>
<p>Our goal is to get a first overview of these papers and what they are
about and to give them good file names.</p>
<div class="section level3">
<h3 id="step-1-generating-messages-for-each-document">Step 1: Generating Messages for Each Document<a class="anchor" aria-label="anchor" href="#step-1-generating-messages-for-each-document"></a>
</h3>
<p>First, we need to prepare a list of messages for all PDFs in the
folder by applying <code><a href="../reference/llm_message.html">llm_message()</a></code> with a prompt to the first
five pages of each document. This step sets up a list of messages, where
each entry specifies a task to retrieve structured answers on one file.
Even though the <code>gpt-4o-mini model</code> we will use in this
example can process up to 128,000 tokens -approximately 80-90 pages of
English text- we limit the input to five pages for demonstration
purposes and to maintain focus on the introduction that is usually
enough to get a first overview of a paper.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">document_tasks</span> <span class="op">&lt;-</span> <span class="va">files</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="op">~</span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Below are the first 5 pages of a document. </span></span>
<span><span class="st">                     Summarise the document based on the provided schema."</span>, </span>
<span>            .pdf <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>              filename <span class="op">=</span> <span class="va">.x</span>,</span>
<span>              start_page <span class="op">=</span> <span class="fl">1</span>,</span>
<span>              end_page <span class="op">=</span> <span class="fl">5</span><span class="op">)</span><span class="op">)</span> <span class="co">#Maximally 5 pages since we are not sure how long each document is</span></span>
<span><span class="op">)</span> </span></code></pre></div>
</div>
<div class="section level3">
<h3 id="step-2-defining-the-schema-for-structured-output">Step 2: Defining the Schema for Structured Output<a class="anchor" aria-label="anchor" href="#step-2-defining-the-schema-for-structured-output"></a>
</h3>
<p>In this step, we define a schema that outlines the expected data
types for each field in the model’s responses. This schema enables the
large language model to return answers in a structured, consistent
format that can later be converted into a table, making it easy to
analyze and compare results across documents.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">document_schema</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tidyllm_schema.html">tidyllm_schema</a></span><span class="op">(</span></span>
<span>  name <span class="op">=</span> <span class="st">"DocumentAnalysisSchema"</span>,</span>
<span>  Title <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"The full title of the provided document"</span><span class="op">)</span>,</span>
<span>  Authors <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"A semicolon-seperated list of Authors"</span><span class="op">)</span>,</span>
<span>  SuggestedFilename <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"Suggest a filename in the format \"ReleaseYear_Author_etal_ShortTitle.pdf\". Use the publication year if available; otherwise, use XXXX."</span><span class="op">)</span>,</span>
<span>  Type <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_fct</a></span><span class="op">(</span><span class="st">"Is the document a Policy Report or a Research Paper based on  it’s style, structure, and purpose?"</span>,.levels <span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Policy"</span>, <span class="st">"Research"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  Empirics <span class="op">=</span>  <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"A 100 word description of empirical methods used, including data collection, statistical techniques, or analysis methods mentioned."</span><span class="op">)</span>,</span>
<span>  Theory <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"A 100 word outline the primary theoretical framework or models discussed if any."</span><span class="op">)</span>,</span>
<span>  MainPoint <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"A one sentence summary ot the main point raised"</span><span class="op">)</span>,</span>
<span>  Contribution <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"A short explanation of the main contributions claimed in the document"</span><span class="op">)</span>,</span>
<span>  KeyCitations <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"The four most frequently cited or critical references in the first 5 pages"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>In <code><a href="../reference/tidyllm_schema.html">tidyllm_schema()</a></code>, we specify each field field with a
description. The seperate field functions indicate what kind of type we
expect for the answer (i.e. <code><a href="../reference/field_chr.html">field_chr()</a></code> for text or
<code><a href="../reference/field_chr.html">field_dbl()</a></code> for numbers). For fields where categorical
responses are needed, we can use <code><a href="../reference/field_chr.html">field_fct()</a></code> to define
specific allowed options with the <code>.levels</code> argument. For
instance <code><a href="../reference/field_chr.html">field_fct()</a></code>, in the schema above creates a
categorical field with the choices “Policy” and “Research.” To indicate
that a field should return a list of values, you can set
<code>.vector=TRUE</code> in the field functions. He, we intentionally
avoid lists to maintain a flat structure. This ensures that the output
can be easily converted to a single-row <code>tibble</code>.
<code>name</code> is a special field, which creates an identifier for a
schema. By default it is <code>"tidyllm_schema"</code></p>
</div>
<div class="section level3">
<h3 id="step-3-running-the-analysis-on-a-sample-document">Step 3: Running the Analysis on a Sample Document<a class="anchor" aria-label="anchor" href="#step-3-running-the-analysis-on-a-sample-document"></a>
</h3>
<p>To test the setup, we run the analysis on a single document with the
standard <code><a href="../reference/chat.html">chat()</a></code> function, using the schema to ensure
structured output.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_task</span> <span class="op">&lt;-</span> <span class="va">document_tasks</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.json_schema <span class="op">=</span> <span class="va">document_schema</span>,</span>
<span>              .model       <span class="op">=</span> <span class="st">"gpt-4o-mini"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="step-4-extracting-and-formatting-the-results-for-our-example-task">Step 4: Extracting and Formatting the Results for our example
task<a class="anchor" aria-label="anchor" href="#step-4-extracting-and-formatting-the-results-for-our-example-task"></a>
</h3>
<p>We use <code><a href="../reference/get_reply_data.html">get_reply_data()</a></code> to extract the model’s
structured responses from the model reply.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/get_reply_data.html">get_reply_data</a></span><span class="op">(</span><span class="va">example_task</span><span class="op">)</span></span>
<span><span class="co">## $Title</span></span>
<span><span class="co">## [1] "A Method to Link Advances in Artificial Intelligence to Occupational Abilities"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $Authors</span></span>
<span><span class="co">## [1] "Edward W. Felten; Manav Raj; Robert Seamans"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $SuggestedFilename</span></span>
<span><span class="co">## [1] "2018_Felten_etal_AILinkOccupations.pdf"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $Type</span></span>
<span><span class="co">## [1] "Research"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $Empirics</span></span>
<span><span class="co">## [1] "The study employs two main databases: the Electronic Frontier Foundation (EFF) AI Progress Measurement dataset and the Occupational Information Network (O*NET). The EFF dataset tracks task-specific AI performance metrics across various categories from 2010 to 2015, while O*NET provides contemporary occupational definitions. The authors calculate the correlation between AI advancements and occupational changes using an analysis of variance methodology to derive impact scores for occupations."</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $Theory</span></span>
<span><span class="co">## [1] "The theoretical framework relies on linking advancements in various AI fields (e.g., image recognition) to a set of 52 abilities outlined by O*NET. The authors construct a matrix correlating EFF AI categories to O*NET abilities, assessing how AI impacts specific skills important for different occupations, theoretically grounded in human capital and job task frameworks."</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $MainPoint</span></span>
<span><span class="co">## [1] "The paper introduces a novel methodology to quantitatively assess the impact of AI advancements on occupational abilities, facilitating better understanding for researchers and policymakers."</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $Contribution</span></span>
<span><span class="co">## [1] "This research contributes a systematic approach for linking AI advancements to occupational changes, enhancing the understanding of AI's role in labor markets and the skills needed across different jobs. The methodology allows for further analysis of the varying impacts of AI on occupations, potentially aiding policy development."</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $KeyCitations</span></span>
<span><span class="co">## [1] "Autor and Handel (2013); Brynjolfsson et al. (2018); Acemoglu and Restrepo (2017); Frey and Osborne (2017)"</span></span></code></pre></div>
<p>The model seems to have reasonably answered our questions in the
structured format we provided for our first example task. Here we can
also look at the token usage of the example task:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/get_metadata.html">get_metadata</a></span><span class="op">(</span><span class="va">example_task</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 1 × 7</span></span></span>
<span><span class="co">##   model  timestamp           prompt_tokens completion_tokens total_tokens stream</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dttm&gt;</span>                      <span style="color: #949494; font-style: italic;">&lt;int&gt;</span>             <span style="color: #949494; font-style: italic;">&lt;int&gt;</span>        <span style="color: #949494; font-style: italic;">&lt;int&gt;</span> <span style="color: #949494; font-style: italic;">&lt;lgl&gt;</span> </span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> gpt-4… 2025-03-22 <span style="color: #949494;">22:07:53</span>          <span style="text-decoration: underline;">4</span>265               337         <span style="text-decoration: underline;">4</span>602 FALSE </span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 1 more variable: api_specific &lt;list&gt;</span></span></span></code></pre>
<p>At a price for <code>gpt-4o-mini</code> of $0.15 / million input
tokens and $0.60 / million output tokens the cost of our example task is
less than a cent.</p>
</div>
<div class="section level3">
<h3 id="step-5-scaling-up-to-a-whole-batch-of-papers">Step 5: Scaling up to a whole batch of papers<a class="anchor" aria-label="anchor" href="#step-5-scaling-up-to-a-whole-batch-of-papers"></a>
</h3>
<p>After confirming that our single-document analysis is working well,
we can extend this workflow to process a larger batch of documents.
Batch processing is particularly valuable when handling a large
collection of files, as it allows us to submit multiple messages at
once, which are then processed together on the model provider’s
servers.</p>
<p>Batch APIs, like those from Anthropic and OpenAI, often offer up to
50% savings compared to single-interaction requests. In
<strong>tidyllm</strong>, we can use <code><a href="../reference/send_batch.html">send_batch()</a></code> to submit
batch requests. The OpenAI Batch API, supports up to 50,000 requests in
a single batch with a maximum file size of 100 MB. Additionally, batch
API rate limits are separate from the standard per-model limits, meaning
batch usage doesn’t impact your regular API rate allocations.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">document_tasks</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="../reference/send_batch.html">send_batch</a></span><span class="op">(</span><span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.json_schema <span class="op">=</span> <span class="va">document_schema</span>,</span>
<span>                     .model       <span class="op">=</span> <span class="st">"gpt-4o-mini"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://readr.tidyverse.org/reference/read_rds.html" class="external-link">write_rds</a></span><span class="op">(</span><span class="st">"document_batch.rds"</span><span class="op">)</span></span></code></pre></div>
<p>After the batch is sent, the output of <code><a href="../reference/send_batch.html">send_batch()</a></code>
contains the input list of message histories along with batch metadata,
such as the <code>batchID</code> as an attribute as well as unique names
for each list element that can be used to stitch together messages with
replies, once they are ready. If you provide a named list of messages,
<strong>tidyllm</strong> will use these names as identifiers in the
batch (provided that these names are unique for each list element).
Batches are processed within 24 hours, usually much faster. The batch
request for this example was processed within 10 minutes.</p>
<blockquote>
<p>⚠️ <strong>Note:</strong> We save the RDS file to disk to preserve
the state of our batch request, including all messages and their unique
identifiers. This file acts as a checkpoint, allowing us to easily
reload and check the batch status or retrieve results across R sessions
without needing to resend the entire batch if we close the session in
the mean time.</p>
</blockquote>
<p>To check whether a batch was compeleted you can load the output file
into <code><a href="../reference/check_openai_batch.html">check_openai_batch()</a></code>:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://readr.tidyverse.org/reference/read_rds.html" class="external-link">read_rds</a></span><span class="op">(</span><span class="st">"document_batch.rds"</span><span class="op">)</span><span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/check_batch.html">check_batch</a></span><span class="op">(</span><span class="va">openai</span><span class="op">)</span></span></code></pre></div>
<p>Alternatively you can list all OpenAI batches with
<code><a href="../reference/list_openai_batches.html">list_openai_batches()</a></code>or
<code>list_batchers(openai())</code>. Of course, you can also look into
the <a href="https://platform.openai.com/batches/" class="external-link">batches dashboard of
the OpenAI platform</a> for the same overview. Since OpenAI batches are
sent as <code>.jsonl</code> and saved on the OpenAI server, you can also
look into the file tab of the dashboard to delete old files from time to
time.</p>
</div>
<div class="section level3">
<h3 id="step-6-getting-data-from-the-entire-batch">Step 6: Getting data from the entire batch<a class="anchor" aria-label="anchor" href="#step-6-getting-data-from-the-entire-batch"></a>
</h3>
<p>Once the batch is complete, we fetch all responses with
<code><a href="../reference/fetch_batch.html">fetch_batch()</a></code>:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://readr.tidyverse.org/reference/read_rds.html" class="external-link">read_rds</a></span><span class="op">(</span><span class="st">"document_batch.rds"</span><span class="op">)</span><span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/fetch_batch.html">fetch_batch</a></span><span class="op">(</span><span class="va">openai</span><span class="op">)</span></span></code></pre></div>
<p>We can then process the results into a table by mapping
<code><a href="../reference/get_reply_data.html">get_reply_data()</a></code> and <code><a href="https://tibble.tidyverse.org/reference/as_tibble.html" class="external-link">as_tibble()</a></code> over the
batch output:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">docuemnt_table</span> <span class="op">&lt;-</span> <span class="va">results</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">get_reply_data</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://purrr.tidyverse.org/reference/map_dfr.html" class="external-link">map_dfr</a></span><span class="op">(</span><span class="va">as_tibble</span><span class="op">)</span></span>
<span>  </span>
<span><span class="va">docuemnt_table</span></span>
<span><span class="co">## <span style="color: #949494;"># A tibble: 24 × 9</span></span></span>
<span><span class="co">##    Title  Authors SuggestedFilename Type  Empirics Theory MainPoint Contribution</span></span>
<span><span class="co">##    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>             <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span> <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>    <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>       </span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 1</span> A Met… Edward… 2018_Felten_etal… Rese… The pap… Theor… The pape… The primary…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 2</span> The R… Alexan… 2024_Bick_etal_R… Rese… The emp… The t… The pape… The primary…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 3</span> The A… Andrew… 2024_Caplin_etal… Rese… The stu… The t… AI can s… The paper c…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 4</span> Large… John J… 2023_Horton_etal… Rese… The doc… The c… The pape… This resear…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 5</span> The I… Sida P… 2023_Peng_etal_I… Rese… The stu… The p… The stud… This resear…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 6</span> GPTs … Tyna E… 2023_Eloundou_et… Rese… The stu… The t… The intr… This paper …</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 7</span> Autom… Philip… 2023_Lergetporer… Rese… This st… The u… Workers … This paper …</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 8</span> Artif… Andrew… 2024_Green_etal_… Rese… The doc… The r… AI is tr… The primary…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;"> 9</span> The A… Andrew… 2024_Caplin_etal… Rese… The res… The s… The abil… This resear…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">10</span> Autom… Daron … 2019_Acemoglu_Re… Rese… The doc… The p… The impa… The authors…</span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 14 more rows</span></span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 1 more variable: KeyCitations &lt;chr&gt;</span></span></span></code></pre></div>
<p>This table can be exported to Excel with
<code><a href="https://docs.ropensci.org/writexl//reference/write_xlsx.html" class="external-link">writexl::write_xlsx()</a></code> for further review. Additionally, we
could programmatically rename the PDFs using the model’s suggested
filenames, helping maintain an organized document structure for future
analysis.</p>
</div>
</div>
<div class="section level2">
<h2 id="further-notes-on-working-with-large-documents">Further notes on working with large documents<a class="anchor" aria-label="anchor" href="#further-notes-on-working-with-large-documents"></a>
</h2>
<div class="section level3">
<h3 id="context-length">Context length<a class="anchor" aria-label="anchor" href="#context-length"></a>
</h3>
<p>When working with long documents like research papers, reports, or
books, one common challenge is context length—the maximum amount of text
the model can process in a single query. If a document exceeds this
limit, the model will only see a portion of it, which may lead to
missing important sections or incomplete answers.</p>
<p>In most models, context length is measured in
<strong>tokens</strong>, the basic units of text. For example, many
small local models have a maximum context length of around 8,192 tokens,
roughly covering 30–35 pages. This means that for a long academic paper,
the model may only see the beginning of the document, potentially
omitting later sections like bibliographies or appendices, where key
references or results might appear. Moreover, appending a whole document
to a prompt might leave your actual prompt out of the context.</p>
<p>To manage this, a common approach is to limit the number of pages
sent to the model. In this workflow, we focus on the first five pages
for an initial overview. This typically includes the abstract,
introduction, methodology, results, and discussion—enough to capture the
essence of the paper. While this approach ensures that the model can
process core content, it may omit information found in later
sections.</p>
<p>Alternatively, for very large documents, you could split them into
smaller sections and process each separately, covering more content
without exceeding the model’s context window. However, splitting can
disrupt the document’s flow, which may affect how well the model retains
context across sections.</p>
</div>
<div class="section level3">
<h3 id="gemini-for-image-heavy-pdfs">Gemini for image-heavy PDFs<a class="anchor" aria-label="anchor" href="#gemini-for-image-heavy-pdfs"></a>
</h3>
<p>For <code><a href="../reference/gemini.html">gemini()</a></code> there is an alternative to adding the text
from a PDF with <code><a href="../reference/llm_message.html">llm_message()</a></code>. You can directly upload a
PDF to Google’s servers with <code><a href="../reference/gemini_upload_file.html">gemini_upload_file()</a></code> and use
it in the context of your messages. The advantage of this approach is
that Gemini can handle images in PDFs or are even image-only PDFs (such
as scanned documents). See the article on <a href="https://edubruell.github.io/tidyllm/articles/tidyllm_video.html">Video
and Audio Data with the Gemini API</a> for an example how to use the
<code><a href="../reference/gemini_upload_file.html">gemini_upload_file()</a></code> feature.</p>
</div>
<div class="section level3">
<h3 id="local-models">Local Models<a class="anchor" aria-label="anchor" href="#local-models"></a>
</h3>
<p>Using open-source local models for PDF processing is also possible,
though remote models like <code>gpt-4o-mini</code> tend to handle longer
documents more effectively. Smaller local models in
<code><a href="../reference/ollama.html">ollama()</a></code>, may struggle with large content and complex,
structured queries, even if they support extended context lengths. For
more demanding tasks, larger local models may perform better with
complex queries, but they require substantial hardware resources to run
smoothly. Since Ollama has a default context length of just 2,048
tokens, you will likely also need to adjust the context length with the
<code>.num_ctx</code> option in <code><a href="../reference/ollama.html">ollama()</a></code>. Sending a message
that is longer than the context length might lead to strange errors,
since the input will be truncated by Ollama, which might lead to cases
where only parts of the document are processed without your
instructions. Note that increasing context length will likely slow down
processing due to higher memory usage. In these cases, reducing the
number of pages may help.</p>
<blockquote>
<p>⚠️ <strong>Note:</strong> When using paid remote models, it’s
important to consider data privacy and security, especially if you’re
processing sensitive documents, as uploading data to external servers
may introduce risks — local models provide more control in such
cases.</p>
</blockquote>
</div>
</div>
<div class="section level2">
<h2 id="outlook">Outlook<a class="anchor" aria-label="anchor" href="#outlook"></a>
</h2>
<p>This structured question-answering workflow not only streamlines the
extraction of key insights from academic papers, but can also be adapted
for other document-heavy tasks. Whether you’re working with reports,
policy documents, or news articles this approach can quickly help you
summarize and categorize information for further analysis.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Eduard Brüll.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>

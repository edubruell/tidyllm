<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Get Started • tidyllm</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Get Started">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyllm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/tidyllm.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/tidyllm_classifiers.html">Classifying Texts with tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm_embed.html">Embedding Models in tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm_video.html">Video and Audio Data with the Gemini API</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm-pdfquestions.html">Structured Question Answering from PDFs</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/edubruell/tidyllm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Get Started</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/edubruell/tidyllm/blob/HEAD/vignettes/tidyllm.Rmd" class="external-link"><code>vignettes/tidyllm.Rmd</code></a></small>
      <div class="d-none name"><code>tidyllm.Rmd</code></div>
    </div>

    
    
<p>⚠️ There is a bad bug in the latest CRAN release in the
<code><a href="../reference/fetch_openai_batch.html">fetch_openai_batch()</a></code> function that is now fixed in the
latest Github version. For the CRAN version the
<code><a href="../reference/fetch_openai_batch.html">fetch_openai_batch()</a></code> function throws errors if the logprobs
are turned off.</p>
<div class="section level2">
<h2 id="introduction-to-tidyllm">Introduction to tidyllm<a class="anchor" aria-label="anchor" href="#introduction-to-tidyllm"></a>
</h2>
<p><strong>tidyllm</strong> is an R package providing a unified
interface for interacting with various large language model APIs. This
vignette will guide you through the basic setup and usage of
<strong>tidyllm</strong>.</p>
<div class="section level3">
<h3 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h3>
<p>To install <strong>tidyllm</strong> from CRAN, use:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"tidyllm"</span><span class="op">)</span></span></code></pre></div>
<p>Or, to install the current development version directly from GitHub
using devtools:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Install devtools if not already installed</span></span>
<span><span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/ns-load.html" class="external-link">requireNamespace</a></span><span class="op">(</span><span class="st">"devtools"</span>, quietly <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"devtools"</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Install TidyLLM from GitHub</span></span>
<span><span class="fu">devtools</span><span class="fu">::</span><span class="fu"><a href="https://remotes.r-lib.org/reference/install_github.html" class="external-link">install_github</a></span><span class="op">(</span><span class="st">"edubruell/tidyllm"</span><span class="op">)</span></span></code></pre></div>
<div class="section level4">
<h4 id="setting-up-api-keys-or-ollama">Setting up API Keys or ollama<a class="anchor" aria-label="anchor" href="#setting-up-api-keys-or-ollama"></a>
</h4>
<p>Before using <strong>tidyllm</strong>, set up API keys for the
services you plan to use. Here’s how to set them up for different
providers:</p>
<ol style="list-style-type: decimal">
<li>For Claude models you can get an API key in the <a href="https://console.anthropic.com/settings/keys" class="external-link">Anthropic
Console</a>:</li>
</ol>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>ANTHROPIC_API_KEY <span class="op">=</span> <span class="st">"YOUR-ANTHROPIC-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>For OpenAI you can obtain an API key by signing up at <a href="https://platform.openai.com/account/api-keys" class="external-link">OpenAI</a> and set
it with:</li>
</ol>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>OPENAI_API_KEY <span class="op">=</span> <span class="st">"YOUR-OPENAI-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>For Google Gemini you can setup an API key in the <a href="https://aistudio.google.com/app/apikey" class="external-link">Google AI Studio</a>
</li>
</ol>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>GOOGLE_API_KEY <span class="op">=</span> <span class="st">'YOUR-GOOGLE-API-KEY'</span><span class="op">)</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>For Mistral you can set an API key on the <a href="https://console.mistral.ai/api-keys/" class="external-link">Mistral console page</a> and
set it by</li>
</ol>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>MISTRAL_API_KEY <span class="op">=</span> <span class="st">"YOUR-MISTRAL-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>For groq (not be confused with <a href="https://x.ai/" class="external-link">grok</a>) you
can setup you API keys in the <a href="https://console.groq.com/playground" class="external-link">Groq Console</a>:</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>GROQ_API_KEY <span class="op">=</span> <span class="st">"YOUR-GROQ-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<ol start="6" style="list-style-type: decimal">
<li>For Perplexity you can get an API key in the <a href="https://www.perplexity.ai/settings/api" class="external-link">API settings</a>:</li>
</ol>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>PERPLEXITY_API_KEY <span class="op">=</span> <span class="st">"YOUR-PERPLEXITY-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<p>Alternatively, for persistent storage, add these keys to your
<code>.Renviron</code> file:</p>
<p>For this, run <code><a href="https://usethis.r-lib.org/reference/edit.html" class="external-link">usethis::edit_r_environ()</a></code>, and add a line
with with an API key in this file, for example:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ANTHROPIC_API_KEY</span><span class="op">=</span><span class="st">"YOUR-ANTHROPIC-API-KEY"</span></span></code></pre></div>
<p>If you want to work with local large lange models via
<code>ollama</code> you need to install it from <a href="https://ollama.com/" class="external-link">the official project website</a>. Ollama sets
up a local large language model server that you can use to run
open-source models on your own devices.</p>
</div>
<div class="section level4">
<h4 id="basic-usage">Basic Usage<a class="anchor" aria-label="anchor" href="#basic-usage"></a>
</h4>
<p>Let’s start with a simple example using tidyllm to interact with
different language models:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://edubruell.github.io/tidyllm/">tidyllm</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Start a conversation with Claude</span></span>
<span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What is the capital of France?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#Standard way that llm_messages are printed</span></span>
<span><span class="va">conversation</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system:</span></span>
<span><span class="co">## You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user:</span></span>
<span><span class="co">## What is the capital of France?</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant:</span></span>
<span><span class="co">## The capital of France is Paris.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Continue the conversation with ChatGPT</span></span>
<span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="va">conversation</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What's a famous landmark in this city?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">openai</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/get_reply.html">get_reply</a></span><span class="op">(</span><span class="va">conversation</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "A famous landmark in Paris is the Eiffel Tower."</span></span></code></pre>
<p><strong>tidyllm</strong> is built around a
<strong>message-centric</strong> interface design, where most
interactions work with a message history, that is created by functions
like <code><a href="../reference/llm_message.html">llm_message()</a></code> or modified by API-functions like
<code><a href="../reference/chat.html">chat()</a></code>. These API-functions always work with a combination
of verbs and providers:</p>
<ul>
<li>
<strong>Verbs</strong> (e.g., <code><a href="../reference/chat.html">chat()</a></code>,
<code><a href="../reference/embed.html">embed()</a></code>, <code><a href="../reference/send_batch.html">send_batch()</a></code>) define the type of
action you want to perform.</li>
<li>
<strong>Providers</strong> (e.g., <code><a href="../reference/openai.html">openai()</a></code>,
<code><a href="../reference/claude.html">claude()</a></code>, <code><a href="../reference/ollama.html">ollama()</a></code>) specify the API or service
to handle the action.</li>
</ul>
<p>Alternatively there are also provider-specific functions like
<code><a href="../reference/openai_chat.html">openai_chat()</a></code> or <code><a href="../reference/claude_chat.html">claude_chat()</a></code> that do the
work in the background of the main verbs that you can also call
directly. The documentation for these provider-specific functions offers
a comprehensive overview of the full range of actions and input types
supported for each API.</p>
</div>
<div class="section level4">
<h4 id="sending-images-to-models">Sending Images to Models<a class="anchor" aria-label="anchor" href="#sending-images-to-models"></a>
</h4>
<p><strong>tidyllm</strong> also supports sending images to multimodal
models. Let’s send this picture here:
<img src="picture.jpeg" alt="A photograhp showing lake Garda and the scenery near Torbole, Italy." width="70%"></p>
<p>Here we let ChatGPT guess where the picture was made:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Describe an image using a llava model on ollama</span></span>
<span><span class="va">image_description</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Describe this picture? Can you guess where it was made?"</span>,</span>
<span>                                 .imagefile <span class="op">=</span> <span class="st">"picture.jpeg"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the last reply</span></span>
<span><span class="fu"><a href="../reference/get_reply.html">get_reply</a></span><span class="op">(</span><span class="va">image_description</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "The picture shows a beautiful landscape with a lake, mountains, and a town nestled below. The sun is shining brightly, casting a serene glow over the water. The area appears lush and green, with agricultural fields visible. \n\nThis type of scenery is reminiscent of northern Italy, particularly around Lake Garda, which features similar large mountains, picturesque water, and charming towns."</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="adding-pdfs-to-messages">Adding PDFs to messages<a class="anchor" aria-label="anchor" href="#adding-pdfs-to-messages"></a>
</h4>
<p>The <code><a href="../reference/llm_message.html">llm_message()</a></code> function also supports extracting text
from PDFs and including it in the message. This allows you to easily
provide context from a PDF document when interacting with an AI
assistant.</p>
<p>To use this feature, you need to have the <code>pdftools</code>
package installed. If it is not already installed, you can install it
with:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"pdftools"</span><span class="op">)</span></span></code></pre></div>
<p>To include text from a PDF in your prompt, simply pass the file path
to the <code>.pdf</code> argument of the <code>chat</code> function:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Please summarize the key points from the provided PDF document."</span>, </span>
<span>     .pdf <span class="op">=</span> <span class="st">"die_verwandlung.pdf"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system:</span></span>
<span><span class="co">## You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user:</span></span>
<span><span class="co">## Please summarize the key points from the provided PDF</span></span>
<span><span class="co">## document.</span></span>
<span><span class="co">##  -&gt; Attached Media Files:  die_verwandlung.pdf </span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant:</span></span>
<span><span class="co">## Here are the key points from the provided PDF document 'Die</span></span>
<span><span class="co">## Verwandlung' by Franz Kafka:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1. The story centers around Gregor Samsa, who wakes up one</span></span>
<span><span class="co">## morning to find that he has been transformed into a giant</span></span>
<span><span class="co">## insect-like creature.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 2. Gregor's transformation causes distress and disruption</span></span>
<span><span class="co">## for his family. They struggle to come to terms with the</span></span>
<span><span class="co">## situation and how to deal with Gregor in his new state.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 3. Gregor's family, especially his sister Grete, initially</span></span>
<span><span class="co">## tries to care for him, but eventually decides they need</span></span>
<span><span class="co">## to get rid of him. They lock him in his room and discuss</span></span>
<span><span class="co">## finding a way to remove him.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 4. Gregor becomes increasingly isolated and neglected by</span></span>
<span><span class="co">## his family. He becomes weaker and less mobile due to his</span></span>
<span><span class="co">## injuries and lack of proper care.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 5. Eventually, Gregor dies, and his family is relieved.</span></span>
<span><span class="co">## They then begin to make plans to move to a smaller, more</span></span>
<span><span class="co">## affordable apartment and start looking for new jobs and</span></span>
<span><span class="co">## opportunities.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<p>The package will automatically extract the text from the PDF and
include it in the prompt sent to the an API. The text will be wrapped in
<code>&lt;pdf&gt;</code> tags to clearly indicate the content from the
PDF:</p>
<pre><code>Please summarize the key points from the provided PDF document.

&lt;pdf filename="example_document.pdf"&gt;
Extracted text from the PDF file...
&lt;/pdf&gt;</code></pre>
</div>
<div class="section level4">
<h4 id="sending-r-outputs-to-language-models">Sending R Outputs to Language Models<a class="anchor" aria-label="anchor" href="#sending-r-outputs-to-language-models"></a>
</h4>
<p>You can automatically include R code outputs in your prompts.
<code><a href="../reference/llm_message.html">llm_message()</a></code> has an optional argument <code>.f</code> in
which you can specify a (anonymous) function, which will be run and
which console output will be captured and appended to the message when
you run it.</p>
<p>In addition you can use <code>.capture_plot</code> to send the last
plot pane to a model.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org" class="external-link">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a plot for the mtcars example data</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">mtcars</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span><span class="va">wt</span>, <span class="va">mpg</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html" class="external-link">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, formula <span class="op">=</span> <span class="st">'y ~ x'</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Weight"</span>,y<span class="op">=</span><span class="st">"Miles per gallon"</span><span class="op">)</span></span></code></pre></div>
<p><img src="tidyllm_files/figure-html/routputs_base-1.png" width="700"></p>
<p>Now we can send the plot and data summary to a language model:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org" class="external-link">tidyverse</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Analyze this plot and data summary:"</span>, </span>
<span>                  .capture_plot <span class="op">=</span> <span class="cn">TRUE</span>, <span class="co">#Send the plot pane to a model</span></span>
<span>                  .f <span class="op">=</span> <span class="op">~</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">)</span><span class="op">}</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="co">#Run summary(data) and send the output</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system:</span></span>
<span><span class="co">## You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user:</span></span>
<span><span class="co">## Analyze this plot and data summary:</span></span>
<span><span class="co">##  -&gt; Attached Media Files:  file1568f6c1b4565.png, RConsole.txt </span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant:</span></span>
<span><span class="co">## Based on the plot and data summary provided, here's an</span></span>
<span><span class="co">## analysis:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1. Relationship between Weight and MPG:</span></span>
<span><span class="co">## The scatter plot shows a clear negative correlation between</span></span>
<span><span class="co">## weight (wt) and miles per gallon (mpg). As the weight of the</span></span>
<span><span class="co">## car increases, the fuel efficiency (mpg) decreases.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 2. Linear Trend:</span></span>
<span><span class="co">## The blue line in the plot represents a linear regression</span></span>
<span><span class="co">## fit. The downward slope confirms the negative relationship</span></span>
<span><span class="co">## between weight and mpg.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 3. Data Distribution:</span></span>
<span><span class="co">## - The weight of cars in the dataset ranges from 1.513 to</span></span>
<span><span class="co">## 5.424 (likely in thousands of pounds).</span></span>
<span><span class="co">## - The mpg values range from 10.40 to 33.90.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 4. Variability:</span></span>
<span><span class="co">## There's some scatter around the regression line, indicating</span></span>
<span><span class="co">## that while weight is a strong predictor of mpg, other</span></span>
<span><span class="co">## factors also influence fuel efficiency.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 5. Other Variables:</span></span>
<span><span class="co">## While not shown in the plot, the summary statistics provide</span></span>
<span><span class="co">## information on other variables:</span></span>
<span><span class="co">## - Cylinder count (cyl) ranges from 4 to 8, with a median of</span></span>
<span><span class="co">## 6.</span></span>
<span><span class="co">## - Horsepower (hp) ranges from 52 to 335, with a mean of</span></span>
<span><span class="co">## 146.7.</span></span>
<span><span class="co">## - Transmission type (am) is binary (0 or 1), likely</span></span>
<span><span class="co">## indicating automatic vs. manual.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 6. Model Fit:</span></span>
<span><span class="co">## The grey shaded area around the regression line represents</span></span>
<span><span class="co">## the confidence interval. It widens at the extremes of the</span></span>
<span><span class="co">## weight range, indicating less certainty in predictions for</span></span>
<span><span class="co">## very light or very heavy vehicles.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 7. Outliers:</span></span>
<span><span class="co">## There are a few potential outliers, particularly at the</span></span>
<span><span class="co">## lower and higher ends of the weight spectrum, that deviate</span></span>
<span><span class="co">## from the general trend.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## In conclusion, this analysis strongly suggests that</span></span>
<span><span class="co">## weight is a significant factor in determining a car's fuel</span></span>
<span><span class="co">## efficiency, with heavier cars generally having lower mpg.</span></span>
<span><span class="co">## However, the presence of scatter in the data indicates that</span></span>
<span><span class="co">## other factors (possibly related to engine characteristics,</span></span>
<span><span class="co">## transmission type, or aerodynamics) also play a role in</span></span>
<span><span class="co">## determining fuel efficiency.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="getting-replies-from-the-api">Getting replies from the API<a class="anchor" aria-label="anchor" href="#getting-replies-from-the-api"></a>
</h4>
<p>Retrieve an assistant reply as text from a message history with
<code><a href="../reference/get_reply.html">get_reply()</a></code>. Specify an index to choose which assistant
message to get:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Imagine a German adress."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/groq.html">groq</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Imagine another address"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">conversation</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system:</span></span>
<span><span class="co">## You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user:</span></span>
<span><span class="co">## Imagine a German adress.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant:</span></span>
<span><span class="co">## Let's imagine a German address:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Herr Müller</span></span>
<span><span class="co">## Musterstraße 12</span></span>
<span><span class="co">## 53111 Bonn</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user:</span></span>
<span><span class="co">## Imagine another address</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant:</span></span>
<span><span class="co">## Let's imagine another German address:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Frau Schmidt</span></span>
<span><span class="co">## Fichtenweg 78</span></span>
<span><span class="co">## 42103 Wuppertal</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<p>By default <code><a href="../reference/get_reply.html">get_reply()</a></code> gets the last assistant message.
Alternatively you can also use <code><a href="../reference/get_reply.html">last_reply()</a></code> as a shortcut
for the latest response.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Getting the first reply</span></span>
<span><span class="va">conversation</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/get_reply.html">get_reply</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "Let's imagine a German address: \n\nHerr Müller\nMusterstraße 12\n53111 Bonn"</span></span></code></pre>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#By default it gets the last reply</span></span>
<span><span class="va">conversation</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/get_reply.html">get_reply</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "Let's imagine another German address:\n\nFrau Schmidt\nFichtenweg 78\n42103 Wuppertal"</span></span></code></pre>
<p>Or you can convert the text (without attachments) to a
<code>tibble</code> with <code><a href="https://tibble.tidyverse.org/reference/as_tibble.html" class="external-link">as_tibble()</a></code>:</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conversation</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://tibble.tidyverse.org/reference/as_tibble.html" class="external-link">as_tibble</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 5 × 2</span></span></span>
<span><span class="co">##   role      content                                                             </span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>     <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                                                               </span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> system    <span style="color: #949494;">"</span>You are a helpful assistant<span style="color: #949494;">"</span>                                       </span></span>
<span><span class="co">## <span style="color: #BCBCBC;">2</span> user      <span style="color: #949494;">"</span>Imagine a German adress.<span style="color: #949494;">"</span>                                          </span></span>
<span><span class="co">## <span style="color: #BCBCBC;">3</span> assistant <span style="color: #949494;">"</span>Let's imagine a German address: \n\nHerr Müller\nMusterstraße 12\n…</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">4</span> user      <span style="color: #949494;">"</span>Imagine another address<span style="color: #949494;">"</span>                                           </span></span>
<span><span class="co">## <span style="color: #BCBCBC;">5</span> assistant <span style="color: #949494;">"</span>Let's imagine another German address:\n\nFrau Schmidt\nFichtenweg …</span></span></code></pre>
<p>You can use the <code><a href="../reference/get_metadata.html">get_metadata()</a></code> function to retrieve
metadata on models and token usage from assistant replies:</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conversation</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/get_metadata.html">get_metadata</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 2 × 6</span></span></span>
<span><span class="co">##   model  timestamp           prompt_tokens completion_tokens</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dttm&gt;</span>                      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>             <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> groq-… 2024-11-08 <span style="color: #949494;">14:25:43</span>            20                45</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">2</span> claud… 2024-11-08 <span style="color: #949494;">14:26:02</span>            80                40</span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 2 more variables: total_tokens &lt;dbl&gt;,</span></span></span>
<span><span class="co">## <span style="color: #949494;">#   api_specific &lt;list&gt;</span></span></span></code></pre>
<p>By default it collects metadata for the whole message history, but
you can also set an <code>.index</code> to only get metadata for a
specific reply. Alternatively you can print out metadata with the
standard print method with the <code>.meta</code>-argument in
<code><a href="https://rdrr.io/r/base/print.html" class="external-link">print()</a></code> or via the <code>tidyllm_print_metadata</code>
option. The list column <code>api_specific</code> contains special
metadata that is only available to some APIs (like citations for models
with search grounding on <code><a href="../reference/perplexity.html">perplexity()</a></code> or special
information on model loading for local APIs).</p>
</div>
<div class="section level4">
<h4 id="working-with-structured-model-outputs">Working with structured model outputs<a class="anchor" aria-label="anchor" href="#working-with-structured-model-outputs"></a>
</h4>
<p>To make model responses easy to interpret and integrate into your
workflow, <strong>tidyllm</strong> supports defining schemas to ensure
that models reply with structured outputs in <a href="https://de.wikipedia.org/wiki/JavaScript_Object_Notation" class="external-link">JSON
(JavaScript Object Notation)</a> following your specifications. JSON is
a standard format for organizing data in simple key-value pairs, which
is both human-readable and machine-friendly.</p>
<p>Currently, <code><a href="../reference/openai.html">openai()</a></code>, <code><a href="../reference/gemini.html">gemini()</a></code> and
<code><a href="../reference/ollama.html">ollama()</a></code> are the only API provider functions in
<strong>tidyllm</strong> that support schema enforcement through the
<code>.json_schema</code> argument. This ensures that replies conform to
a pre-defined consistent data formatting.</p>
<p>To create schemas, you can use the <code><a href="../reference/tidyllm_schema.html">tidyllm_schema()</a></code>
function, which translates your data format specifications into the <a href="https://json-schema.org/" class="external-link">JSON-schema format</a> the API requires.
This helper function standardizes the data layout by ensuring flat
(non-nested) JSON structures with defined data types. Here’s how to
define a schema:</p>
<ul>
<li>
<strong><code>name</code></strong>: A name identifier for the
schema. By default this name is <code>"tidyllm_schema"</code>.</li>
<li>
<strong><code>...</code> (fields)</strong>: Named arguments for
field names and their data types, including:
<ul>
<li>
<code>"character"</code> or <code>"string"</code>: Text fields.</li>
<li>
<code>"factor(...)"</code>: Enumerations with allowable values, like
<code>factor(Germany, France)</code>.</li>
<li>
<code>"logical"</code>: <code>TRUE</code> or <code>FALSE</code>
</li>
<li>
<code>"numeric"</code>: Numeric fields.</li>
<li>
<code>"type[]"</code>: Lists of a given type, such as
<code>"character[]"</code>.</li>
</ul>
</li>
</ul>
<p>Alternatively, you can use a set of <strong>field functions</strong>
that offer additional flexibility and readability when defining fields.
These functions allow you to provide descriptions for each field, giving
models more context how fields in the assistant reply should be
filled:</p>
<ul>
<li>
<code><a href="../reference/field_chr.html">field_chr()</a></code>: For text fields.</li>
<li>
<code><a href="../reference/field_chr.html">field_dbl()</a></code>: For numeric fields.</li>
<li>
<code><a href="../reference/field_chr.html">field_lgl()</a></code>: For boolean fields.</li>
<li>
<code><a href="../reference/field_chr.html">field_fct()</a></code>: For enumerations (factors), with a
<code>.levels</code> argument to specify valid values.</li>
<li>All functions accept <code>.description</code> (for adding
descriptions) and <code>.vector = TRUE</code> (for defining lists).</li>
</ul>
<p>If the <strong>ellmer</strong> package is installed, you can also use
<strong>ellmer type definitions</strong> directly in
<code><a href="../reference/tidyllm_schema.html">tidyllm_schema()</a></code>. For example, you can pass objects like
<code><a href="https://ellmer.tidyverse.org/reference/type_boolean.html" class="external-link">ellmer::type_string()</a></code> or <code><a href="https://ellmer.tidyverse.org/reference/type_boolean.html" class="external-link">ellmer::type_object()</a></code>
as field definitions. Moreover, you can supply an entire
<code><a href="https://ellmer.tidyverse.org/reference/type_boolean.html" class="external-link">ellmer::type_object()</a></code> schema directly as the
<code>.json_schema</code> argument in API calls.</p>
<p>Here’s an example schema defining an address format:</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">address_schema</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tidyllm_schema.html">tidyllm_schema</a></span><span class="op">(</span></span>
<span>  name <span class="op">=</span> <span class="st">"AddressSchema"</span>,</span>
<span>  street <span class="op">=</span> <span class="st">"character"</span>,</span>
<span>  houseNumber <span class="op">=</span> <span class="st">"numeric"</span>,</span>
<span>  postcode <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"A postal code for a city"</span><span class="op">)</span>,</span>
<span>  city <span class="op">=</span> <span class="st">"character"</span>,</span>
<span>  region <span class="op">=</span> <span class="st">"character"</span>,</span>
<span>  country <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_fct</a></span><span class="op">(</span><span class="st">"A European Country"</span>, .levels<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Germany"</span>,<span class="st">"France"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">address</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Imagine an address in JSON format that matches the schema."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>        <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span><span class="op">)</span>,.json_schema <span class="op">=</span> <span class="va">address_schema</span><span class="op">)</span></span>
<span><span class="va">address</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system:</span></span>
<span><span class="co">## You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user:</span></span>
<span><span class="co">## Imagine an address in JSON format that matches the schema.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant:</span></span>
<span><span class="co">## {"street":"Hauptstraße","houseNumber":123,"postcode":"10115","city":"Berlin","region":"Berlin","country":"Germany"}</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<p>The model responded in JSON format, organizing data into key-value
pairs like specified. You can then convert this JSON output into an R
list for easier handling with <code><a href="../reference/get_reply_data.html">get_reply_data()</a></code>:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">address</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/get_reply_data.html">get_reply_data</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 6</span></span>
<span><span class="co">##  $ street     : chr "Hauptstraße"</span></span>
<span><span class="co">##  $ houseNumber: int 123</span></span>
<span><span class="co">##  $ postcode   : chr "10115"</span></span>
<span><span class="co">##  $ city       : chr "Berlin"</span></span>
<span><span class="co">##  $ region     : chr "Berlin"</span></span>
<span><span class="co">##  $ country    : chr "Germany"</span></span></code></pre>
<p>Ollama, OpenAI and the Google Gemini API work with a schema provided
by <code><a href="../reference/tidyllm_schema.html">tidyllm_schema()</a></code>. Other API providers like
<code><a href="../reference/groq.html">groq()</a></code>, and <code><a href="../reference/mistral.html">mistral()</a></code> only support structured
outputs through a simpler JSON mode, accessible with the
<code>.json</code> argument in these provider functions. Since these
APIs do not currently support native schema enforcement, you’ll need to
prompt the model to follow a specified format directly in your messages.
Although <code><a href="../reference/get_reply_data.html">get_reply_data()</a></code> can help extract structured data
from these responses when you set <code>.json=TRUE</code> in each of the
API provider functions, the model may not always adhere strictly to the
specified structure. Once these APIs support native schema enforcement,
<strong>tidyllm</strong> will integrate full schema functionality for
them. Alternatively, also supports the more flexible schema objects from
the <code>ellmer</code> package in its latest development version.</p>
</div>
<div class="section level4">
<h4 id="api-parameters">API parameters<a class="anchor" aria-label="anchor" href="#api-parameters"></a>
</h4>
<p>Different API functions support different model parameters like, how
deterministic the response should be via parameters like temperature.
You can set these parameters via arguments in the verbs or provider
functions. Please read API-documentation and the documentation of the
model functions for specific examples. Common arguments, such as
temperature, can be specified directly through the main verbs like
<code><a href="../reference/chat.html">chat()</a></code>. For example, <code><a href="../reference/chat.html">chat()</a></code> supports arguments
such as <code>.model</code>,<code>.temperature</code>,
<code>.json_schema</code> and more (see the full list in the
<code><a href="../reference/chat.html">chat()</a></code> documentation) directly.</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="va">temp_example</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Explain how temperature parameters work </span></span>
<span><span class="st">in large language models and why temperature 0 gives you deterministic outputs </span></span>
<span><span class="st">in one sentence."</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co">#per default it is non-zero</span></span>
<span>  <span class="va">temp_example</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">ollama</span>,.temperature<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system:</span></span>
<span><span class="co">## You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user:</span></span>
<span><span class="co">## Explain how temperature parameters work</span></span>
<span><span class="co">## in large language models and why temperature 0 gives you</span></span>
<span><span class="co">## deterministic</span></span>
<span><span class="co">## outputs in one sentence.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant:</span></span>
<span><span class="co">## In large language models, temperature parameters control</span></span>
<span><span class="co">## the randomness of generated text by scaling the output</span></span>
<span><span class="co">## probabilities, with higher temperatures introducing more</span></span>
<span><span class="co">## uncertainty and lower temperatures favoring more likely</span></span>
<span><span class="co">## outcomes; specifically, setting temperature to 0 effectively</span></span>
<span><span class="co">## eliminates all randomness, resulting in deterministic</span></span>
<span><span class="co">## outputs because it sets the probability of each token to its</span></span>
<span><span class="co">## maximum likelihood value.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="co">#Retrying with .temperature=0</span></span>
<span>  <span class="va">temp_example</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">ollama</span>,.temperature<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system:</span></span>
<span><span class="co">## You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user:</span></span>
<span><span class="co">## Explain how temperature parameters work</span></span>
<span><span class="co">## in large language models and why temperature 0 gives you</span></span>
<span><span class="co">## deterministic</span></span>
<span><span class="co">## outputs in one sentence.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant:</span></span>
<span><span class="co">## In large language models, temperature parameters control</span></span>
<span><span class="co">## the randomness of generated text by scaling the output</span></span>
<span><span class="co">## probabilities, with higher temperatures introducing more</span></span>
<span><span class="co">## uncertainty and lower temperatures favoring more likely</span></span>
<span><span class="co">## outcomes; specifically, setting temperature to 0 effectively</span></span>
<span><span class="co">## eliminates all randomness, resulting in deterministic</span></span>
<span><span class="co">## outputs because it sets the probability of each token to its</span></span>
<span><span class="co">## maximum likelihood value.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<p>Provider-specific arguments—such as <code>.ollama_server</code> for
<code><a href="../reference/ollama.html">ollama()</a></code> or <code>.fileid</code> for <code><a href="../reference/gemini.html">gemini()</a></code>
are set directly in the provider function:</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/ollama.html">ollama</a></span><span class="op">(</span>.ollama_server <span class="op">=</span> <span class="st">"http://localhost:11434"</span><span class="op">)</span>, </span>
<span>       .temperature <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<p>When an argument is provided in both <code><a href="../reference/chat.html">chat()</a></code> and the
provider, the value specified in <code><a href="../reference/chat.html">chat()</a></code> takes precedence.
For instance:</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#This uses GPT-4o</span></span>
<span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model<span class="op">=</span><span class="st">"gpt-4o-mini"</span><span class="op">)</span>,</span>
<span>       .model<span class="op">=</span><span class="st">"gpt-4o"</span><span class="op">)</span></span></code></pre></div>
<p>If a common argument set in chat is not supported by a provider
<code><a href="../reference/chat.html">chat()</a></code> will raise an error. For example, sending a
<code>.json_schema</code> to a provider that does not support it will
raise an error:</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">address</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Imagine an address in JSON format that matches the schema."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>        <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/groq.html">groq</a></span><span class="op">(</span><span class="op">)</span>,.json_schema <span class="op">=</span> <span class="va">address_schema</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="embeddings">Embeddings<a class="anchor" aria-label="anchor" href="#embeddings"></a>
</h4>
<p><a href="https://cohere.com/llmu/text-embeddings" class="external-link">Embedding
models</a> in <strong>tidyllm</strong> transform textual inputs into
vector representations, capturing semantic information that can enhance
similarity comparisons, clustering, and retrieval tasks. You can
generate embeddings with the <code><a href="../reference/embed.html">embed()</a></code>-function. These
functions return a semantic vector representation either for each
message in a message history or, more typically for this application,
for each entry in a character vector:</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"What is the meaning of life?"</span>,</span>
<span>  <span class="st">"How much wood would a woodchuck chuck?"</span>,</span>
<span>  <span class="st">"How does the brain work?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/embed.html">embed</a></span><span class="op">(</span><span class="va">ollama</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 3 × 2</span></span></span>
<span><span class="co">##   input                                  embeddings </span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>                                  <span style="color: #949494; font-style: italic;">&lt;list&gt;</span>     </span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> What is the meaning of life?           <span style="color: #949494;">&lt;dbl [384]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">2</span> How much wood would a woodchuck chuck? <span style="color: #949494;">&lt;dbl [384]&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">3</span> How does the brain work?               <span style="color: #949494;">&lt;dbl [384]&gt;</span></span></span></code></pre>
<p>The output is a <code>tibble</code> with two columns, the text input
for the embedding and a list column that contains a vector of semantic
embeddings for each input.</p>
</div>
<div class="section level4">
<h4 id="batch-requests">Batch requests<a class="anchor" aria-label="anchor" href="#batch-requests"></a>
</h4>
<p>Anthropic, OpenAI and Mistral offer batch request options that are
around 50% cheaper than standard single-interaction APIs. Batch
processing allows you to submit multiple message histories at once,
which are then processed together on the model providers servers,
usually within a 24-hour period. In <strong>tidyllm</strong>, you can
use the <code><a href="../reference/send_batch.html">send_batch()</a></code> function to submit these batch
requests to either API.</p>
<p>Here’s an example of how to send a batch request to Claude’s batch
API:</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Create a message batch and save it to disk to fetch it later</span></span>
<span><span class="fu">glue</span><span class="op">(</span><span class="st">"Write a poem about {x}"</span>, x<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"cats"</span>,<span class="st">"dogs"</span>,<span class="st">"hamsters"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">llm_message</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/send_batch.html">send_batch</a></span><span class="op">(</span><span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">saveRDS</a></span><span class="op">(</span><span class="st">"claude_batch.rds"</span><span class="op">)</span></span></code></pre></div>
<p>The <code><a href="../reference/send_batch.html">send_batch()</a></code> function returns the same list of
message histories that was input, but marked with an attribute that
contains a batch-id from the Claude API as well as unique names for each
list element that can be used to stitch together messages with replies,
once they are ready. If you provide a named list of messages, tidyllm
will use these names as identifiers in the batch, if these names are
unique.</p>
<p><strong>Tip:</strong> Saving batch requests to a file allows you to
persist them across R sessions, making it easier to manage large jobs
and access results later.</p>
<p>After sending a batch request, you can check its status with
<code><a href="../reference/check_batch.html">check_batch()</a></code>. For example:</p>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Check the status of the batch</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">readRDS</a></span><span class="op">(</span><span class="st">"claude_batch.rds"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>   <span class="fu"><a href="../reference/check_batch.html">check_batch</a></span><span class="op">(</span><span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 1 × 8</span></span></span>
<span><span class="co">##   batch_id          status created_at          expires_at          req_succeeded</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>             <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dttm&gt;</span>              <span style="color: #949494; font-style: italic;">&lt;dttm&gt;</span>                      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> msgbatch_02A1B2C… ended  2024-11-01 <span style="color: #949494;">10:30:00</span> 2024-11-02 <span style="color: #949494;">10:30:00</span>             3</span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 3 more variables: req_errored &lt;dbl&gt;, req_expired &lt;dbl&gt;, req_canceled &lt;dbl&gt;</span></span></span></code></pre>
<p>The status output shows details such as the number of successful,
errored, expired, and canceled requests in the batch, as well as the
current status. You can also see all your batch requests with
<code><a href="../reference/list_batches.html">list_batches()</a></code> (or in the batches dashboard of the your API
provider). Once the processing of a batch is completed you can fetch its
results with <code><a href="../reference/fetch_batch.html">fetch_batch()</a></code>:</p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conversations</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">readRDS</a></span><span class="op">(</span><span class="st">"claude_batch.rds"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/fetch_batch.html">fetch_batch</a></span><span class="op">(</span><span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">poems</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map_chr</a></span><span class="op">(</span><span class="va">conversations</span>, <span class="va">get_reply</span><span class="op">)</span></span></code></pre></div>
<p>The output is a list of message histories, each now updated with new
assistant replies. You can further process these responses with
<strong>tidyllm’s</strong> standard tools. Before launching a large
batch operation, it’s good practice to run a few test requests and
review outputs with the standard <code><a href="../reference/chat.html">chat()</a></code> function. This
approach helps confirm that prompt settings and model configurations
produce the desired responses, minimizing potential errors or resource
waste.</p>
</div>
<div class="section level4">
<h4 id="streaming-back-responses-experimental">Streaming back responses (Experimental)<a class="anchor" aria-label="anchor" href="#streaming-back-responses-experimental"></a>
</h4>
<p><code><a href="../reference/chat.html">chat()</a></code> supports real-time streaming of reply tokens to
the console while the model works with the <code>.stream=TRUE</code>
argument in <code><a href="../reference/chat.html">chat()</a></code>, for most api providers.</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Write a lengthy magazine advertisement for an R package called tidyllm"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">claude</span>,.stream<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>While this feature offers slightly better feedback on model behavior
in real-time, it’s not particularly useful for data-analysis workflows.
Metadata is not collected for streaming mode. We consider this feature
experimental and recommend using non-streaming responses for production
tasks. Note that error handling in streaming callbacks varies by API and
differs in quality at this time. Metadata is currently not supported for
streaming responses.</p>
</div>
<div class="section level4">
<h4 id="choosing-the-right-model-and-api">Choosing the Right Model and API<a class="anchor" aria-label="anchor" href="#choosing-the-right-model-and-api"></a>
</h4>
<p>tidyllm supports multiple APIs, each offering distinct large language
models with varying strengths. The choice of which model or API to use
often depends on the specific task, cost considerations, and data
privacy concerns.</p>
<ol style="list-style-type: decimal">
<li><p><strong>OpenAI API:</strong> Models by <a href="https://platform.openai.com/docs/api-reference/chat" class="external-link">OpenAI
API</a>, particularly the GPT-4o model, are extremely versatile and
perform well across a wide range of tasks, including text generation,
code completion, and multimodal analysis. In addition the o1-reasoning
models offer very good performance for a set of specific task (at a
relatively high price). There is also an <code><a href="../reference/azure_openai.html">azure_openai()</a></code>
provider function if you prefer to use the OpenAI API on Microsoft
Azure.</p></li>
<li><p><strong>Anthropic API:</strong> <a href="https://docs.anthropic.com/en/docs/welcome" class="external-link">Claude</a> is known
for generating thoughtful, nuanced responses, making it ideal for tasks
that require more human-like reasoning, such as summarization or
creative writing. However, it can sometimes be more verbose than
necessary, and it lacks direct JSON support, which requires additional
prompting and validation to ensure structured output.</p></li>
<li><p><strong>Google Gemini API:</strong> Google Gemini is great for
long-context tasks — it can handle up to a million tokens! In addition,
you can use the <code>.grounding_threshold</code>-parameter in the
<code><a href="../reference/gemini_chat.html">gemini_chat()</a></code> function to ground responses based on Google
searches. The lower the threshold is the more Gemini relies on the
search instead of its internal knowledge:</p></li>
</ol>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What is tidyllm and who maintains this package?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/gemini_chat.html">gemini_chat</a></span><span class="op">(</span>.grounding_threshold <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span></span></code></pre></div>
<p>Moreover, with the Gemini API you are able to upload a wide range of
media files and use them in the prompts of your models with functions
like <code><a href="../reference/gemini_upload_file.html">gemini_upload_file()</a></code>. Using this, Gemini models can be
used to process video and audio together with your messages. Here is an
example of summarizing a speech:</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Upload a file for use with gemini</span></span>
<span><span class="va">upload_info</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/gemini_upload_file.html">gemini_upload_file</a></span><span class="op">(</span><span class="st">"example.mp3"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#Make the file available during a Gemini API call</span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Summarize this speech"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/gemini.html">gemini</a></span><span class="op">(</span>.fileid <span class="op">=</span> <span class="va">upload_info</span><span class="op">$</span><span class="va">name</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co">#Delte the file from the Google servers after you are done</span></span>
<span><span class="fu"><a href="../reference/gemini_delete_file.html">gemini_delete_file</a></span><span class="op">(</span><span class="va">upload_info</span><span class="op">$</span><span class="va">name</span><span class="op">)</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li><p><strong>Mistral API (EU-based):</strong> <a href="https://docs.mistral.ai/" class="external-link">Mistral</a> offers lighter-weight,
open-source models developed and hosted in the EU, making it
particularly appealing if data protection (e.g., GDPR compliance) is a
concern. While the models may not be as powerful as GPT-4o or Claude
Sonnet, Mistral offers good performance for standard text generation
tasks.</p></li>
<li><p><strong>Groq API (Fast):</strong> <a href="https://console.groq.com/docs/quickstart" class="external-link">Groq</a> offers a unique
advantage with its custom AI accelerator hardware, that get you the
fastest output available on any API. It delivers high performance at low
costs, especially for tasks that require fast execution. It hosts many
strong open-source models, like <strong>lamma3:70b</strong>. There is
also a <code><a href="../reference/groq_transcribe.html">groq_transcribe()</a></code> function available that allows you
to transcribe audio files with the Whipser-Large model on the Groq
API.</p></li>
<li><p><strong>Perplexity API (Search and Citations):</strong> <a href="https://docs.perplexity.ai/api-reference/chat-completions" class="external-link">Perplexity</a>
combines current finetuned Llama models with real-time web search
capabilities. This allows for up-to-date information retrieval and
integration into responses. All answers contain links to citations which
can be accesed via the <code>api_specific</code> column of
<code><a href="../reference/get_metadata.html">get_metadata()</a></code></p></li>
<li><p><strong>Ollama (Local Models):</strong> If data privacy is a
priority, running open-source models like <strong>gemma2::9B</strong>
locally via <a href="https://ollama.com/" class="external-link">ollama</a> gives you full
control over model execution and data. However, the trade-off is that
local models require significant computational resources, and are often
not quite as powerful as the large API-providers. The <a href="https://ollama.com/blog" class="external-link">ollama blog</a> regularly has posts about
new models and their advantages that you can download via
<code><a href="../reference/ollama_download_model.html">ollama_download_model()</a></code>.</p></li>
<li><p><strong>Other OpenAI-compatible Local Models:</strong> Besides
ollama, there are many solutions to run local models that are mostly
compatible to the OpenAI API like <a href="https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md" class="external-link">llama.cpp</a>,
<a href="https://github.com/vllm-project/vllm" class="external-link">vllm</a> and <a href="https://www.reddit.com/r/LocalLLaMA/comments/16csz5n/best_openai_api_compatible_application_server/" class="external-link">many
more</a>. To use such an API you can set the base url of the api with
<code>.api_url</code> as well as the path to the model-endpoint with
<code>.api_path</code> argument in the <code><a href="../reference/openai.html">openai()</a></code> provider
function. Set <code>.compatible=TRUE</code> to skip api-key checks and
rate-limit tracking. Compatibility with local models solutions may vary
depending on the specific API’s implementation, and full functionality
cannot be guaranteed. Ideally you can save complicated configurations
like these in an object:</p></li>
</ol>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">my_provider</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model<span class="op">=</span><span class="st">"llama3.2:90b"</span>,</span>
<span>          .api_url<span class="op">=</span><span class="st">"http://localhost:11434"</span>,</span>
<span>          .compatible <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>          .api_path <span class="op">=</span> <span class="st">"/v1/chat/custom/"</span></span>
<span>          <span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hi there"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">my_provider</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="setting-a-default-provider">Setting a Default Provider<a class="anchor" aria-label="anchor" href="#setting-a-default-provider"></a>
</h4>
<p>You can also specify default provider with options:</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Set default providers</span></span>
<span><span class="co">#chat provider</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>tidyllm_chat_default <span class="op">=</span> <span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#embedding provider</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>tidyllm_embed_default <span class="op">=</span> <span class="fu"><a href="../reference/ollama.html">ollama</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"all-minilm"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#send batch provider</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>tidyllm_sbatch_default <span class="op">=</span> <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span>.temperature<span class="op">=</span><span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#check batch provider</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>tidyllm_cbatch_default <span class="op">=</span> <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#fetch batch provider</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>tidyllm_fbatch_default <span class="op">=</span> <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#List batches provider</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>tidyllm_lbatch_default <span class="op">=</span> <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Now you can use chat() or embed() without explicitly specifying a provider</span></span>
<span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello, what is the weather today?"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"What is AI?"</span>, <span class="st">"Define machine learning."</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="../reference/embed.html">embed</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Now you can use batch functions without explicitly specifying a provider</span></span>
<span><span class="va">batch_messages</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Write a poem about the sea."</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Summarize the theory of relativity."</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Invent a name for a new genre of music."</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Send batch using default for send_batch()</span></span>
<span><span class="va">batch_results</span> <span class="op">&lt;-</span> <span class="va">batch_messages</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/send_batch.html">send_batch</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Check batch status using default for check_batch()</span></span>
<span><span class="va">status</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/check_batch.html">check_batch</a></span><span class="op">(</span><span class="va">batch_results</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fetch completed results using default for fetch_batch()</span></span>
<span><span class="va">completed_results</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/fetch_batch.html">fetch_batch</a></span><span class="op">(</span><span class="va">batch_results</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># List all batches using default for list_batches()</span></span>
<span><span class="va">all_batches</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/list_batches.html">list_batches</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Eduard Brüll.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>

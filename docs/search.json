[{"path":"https://edubruell.github.io/tidyllm/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Eduard Brüll Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"example-workflow","dir":"Articles","previous_headings":"","what":"Example Workflow","title":"Structured Question Answering from PDFs","text":"Imagine folder looks something like —many downloaded papers, structure yet: goal get first overview papers give good file names.","code":"library(tidyverse) library(tidyllm) dir(\"aipapers\") ##  [1] \"2018_Felten_etal_AILinkOccupations.pdf\"                                                            ##  [2] \"2024_Bick_etal_RapidAdoption.pdf\"                                                                  ##  [3] \"2024_Caplin_etal_ABCsofAI.pdf\"                                                                     ##  [4] \"2301.07543v1.pdf\"                                                                                  ##  [5] \"2302.06590v1.pdf\"                                                                                  ##  [6] \"2303.10130v5.pdf\"                                                                                  ##  [7] \"488.pdf\"                                                                                           ##  [8] \"88684e36-en.pdf\"                                                                                   ##  [9] \"ABCs_AI_Oct2024.pdf\"                                                                               ## [10] \"acemoglu-restrepo-2019-automation-and-new-tasks-how-technology-displaces-and-reinstates-labor.pdf\" ## [11] \"BBD_GenAI_NBER_Sept2024.pdf\"                                                                       ## [12] \"Deming-Ong-Summers-AESG-2024.pdf\"                                                                  ## [13] \"dp22036.pdf\"                                                                                       ## [14] \"FeltenRajSeamans_AIAbilities_AEA.pdf\"                                                              ## [15] \"JEL-2023-1736_published_version.pdf\"                                                               ## [16] \"Noy_Zhang_1.pdf\"                                                                                   ## [17] \"sd-2024-09-falck-etal-kuenstliche-intelligenz-unternehmen.pdf\"                                     ## [18] \"ssrn-4700751.pdf\"                                                                                  ## [19] \"SSRN-id4573321.pdf\"                                                                                ## [20] \"The Simple Macroeconomics of AI.pdf\"                                                               ## [21] \"w24001.pdf\"                                                                                        ## [22] \"w24871.pdf\"                                                                                        ## [23] \"w31161.pdf\"                                                                                        ## [24] \"w32430.pdf\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"step-1-setting-up-the-document-prompt","dir":"Articles","previous_headings":"Example Workflow","what":"Step 1: Setting up the Document Prompt","title":"Structured Question Answering from PDFs","text":"First, create prompt designed elicit detailed responses document. structured prompt asks specific metadata like title, authors, type, along deeper content-related questions empirical methods theoretical frameworks.","code":"document_prompt <- ' Below are the first 5 pages of a document. Answer the questions below in detail. Provide each answer as a standalone response.   1. Title: Provide the full, exact title of the document as it appears on the first page. 2. Authors: List all authors as stated in the document, including any institutional affiliations if mentioned. 3. Suggested Filename: Suggest a filename in the format \"ReleaseYear_Author_etal_ShortTitle.pdf\". Use the publication year if available; otherwise, use XXXX. 4. Document Type: Specify if this is a \"Policy Report\" or a \"Research Paper\" based on the document’s style, structure, and purpose. 5. Key Citations: Identify the four most frequently cited or critical references in the first 5 pages that support the document’s primary claims or background.  Additionally, answer the following questions about the document’s content. Each answer should be concise but comprehensive, ideally 150 words:  6. Empirical Methods: Describe any empirical methods used, including data collection, statistical techniques, or analysis methods mentioned. 7. Theoretical Framework: Outline the primary theoretical framework or models discussed, if any, that underpin the analysis or arguments. 8. Main Point: Summarize the central argument or main point the document presents. 9. Key Contribution: Explain the unique contribution of this document, particularly what it adds to the field or topic. '"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"step-2-generating-messages-for-each-document","dir":"Articles","previous_headings":"Example Workflow","what":"Step 2: Generating Messages for Each Document","title":"Structured Question Answering from PDFs","text":"Next, prepare list messages PDFs folder applying llm_message() prompt first five pages document. step sets list messages, entry specifies task retrieve structured answers one file. Even though gpt-4o-mini model use example can process 128,000 tokens -approximately 80-90 pages English text- limit input five pages demonstration purposes maintain focus introduction usually enough get first overview paper.","code":"document_tasks <- map(files,~llm_message(document_prompt,              .pdf = list(               filename = .x,               start_page = 1,               end_page = 5)) #Maximally 5 pages since we are not sure how long each document is )"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"step-3-defining-the-schema-for-structured-output","dir":"Articles","previous_headings":"Example Workflow","what":"Step 3: Defining the Schema for Structured Output","title":"Structured Question Answering from PDFs","text":"step, define schema outlines expected data types field model’s responses. schema enables large language model return answers structured, consistent format can later converted table, making easy analyze compare results across documents. tidyllm_schema(), specify field along expected data type. Supported types include character (string accepted synonym), logical, numeric. fields categorical responses needed, can use factor() define specific allowed options. instance, factor(Policy, Research) creates categorical field choices “Policy” “Research.” indicate field return list values, append [] type. example, setting Authors = \"character[]\" allows multiple entries list format Authors field. However, intentionally avoid lists maintain flat structure. ensures output can easily converted single-row tibble. name special field, creates identifier schema. default \"tidyllm_schema\"","code":"document_schema <- tidyllm_schema(   name = \"DocumentAnalysisSchema\",   Title = \"character\",   Authors = \"character\",   SuggestedFilename = \"character\",   Type = \"factor(Policy, Research)\",   Empirics = \"character\",   Theory = \"character\",   MainPoint = \"character\",   Contribution = \"character\",   KeyCitations = \"character\" )"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"step-4-running-the-analysis-on-a-sample-document","dir":"Articles","previous_headings":"Example Workflow","what":"Step 4: Running the Analysis on a Sample Document","title":"Structured Question Answering from PDFs","text":"test setup, run analysis single document standard chat() function, using schema ensure structured output.","code":"example_task <- document_tasks[[1]] |>    chat(openai(.json_schema = document_schema,               .model       = \"gpt-4o-mini\"))"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"step-5-extracting-and-formatting-the-results-for-our-example-task","dir":"Articles","previous_headings":"Example Workflow","what":"Step 5: Extracting and Formatting the Results for our example task","title":"Structured Question Answering from PDFs","text":"use get_reply_data() extract model’s structured responses model reply. model seems reasonably answered questions structured format provided first example task. can also look token usage example task: price gpt-4o-mini $0.15 / million input tokens $0.60 / million output tokens cost example task less cent.","code":"get_reply_data(example_task) ## $Title ## [1] \"A Method to Link Advances in Artificial Intelligence to Occupational Abilities\" ##  ## $Authors ## [1] \"Edward W. Felten (Princeton University), Manav Raj (NYU Stern School of Business), Robert Seamans (NYU Stern School of Business)\" ##  ## $SuggestedFilename ## [1] \"2018_Felten_etal_AILinkOccupations.pdf\" ##  ## $Type ## [1] \"Research\" ##  ## $Empirics ## [1] \"The paper employs empirical methods using two databases: the Electronic Frontier Foundation AI Progress Measurement dataset and the Occupational Information Network (O*NET) from the US Department of Labor. The authors collect data on AI progress metrics and correlate these with occupational definitions and the abilities required for various jobs to derive impact scores for the effect of AI advancements on occupations.\" ##  ## $Theory ## [1] \"The primary theoretical framework is based on task variation in occupations and the effects of AI on the bundle of skills required for specific occupations. The authors complement the work of previous scholars by linking advancements in AI technologies directly to job abilities, rather than relying solely on expert predictions about AI's future impact.\" ##  ## $MainPoint ## [1] \"The document presents a new methodology to link advancements in artificial intelligence to the abilities required in various occupations. It highlights how AI affects the nature of labor, suggesting significant implications for understanding job susceptibility to automation and changes in job requirements.\" ##  ## $Contribution ## [1] \"The key contribution of this research is the development of a systematic methodology to assess how AI advancements impact specific occupational abilities. This approach enables researchers, practitioners, and policymakers to analyze and compare the potential threats and opportunities posed by AI across different job sectors, guiding informed decisions and policies regarding technology and labor market adaptation.\" ##  ## $KeyCitations ## [1] \"1. Autor, David H., and Anna Salomons. 2017; 2. Brynjolfsson, Erik, Tom Mitchell, and Daniel Rock. 2018; 3. Frey, Carl Benedikt, and Michael A. Osborne. 2017; 4. Graetz, Georg, and Guy Michaels. 2015.\" get_reply_data(example_task) ## # A tibble: 1 × 5 ##   model         timestamp           prompt_tokens completion_tokens total_tokens ##   <chr>         <dttm>                      <int>             <int>        <int> ## 1 gpt-4o-mini-… 2024-11-13 07:41:29          4356               273         4629"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"step-6-scaling-up-to-a-whole-batch-of-papers","dir":"Articles","previous_headings":"Example Workflow","what":"Step 6: Scaling up to a whole batch of papers","title":"Structured Question Answering from PDFs","text":"confirming single-document analysis working well, can extend workflow process larger batch documents. Batch processing particularly valuable handling large collection files, allows us submit multiple messages , processed together model provider’s servers. Batch APIs, like Anthropic OpenAI, often offer 50% savings compared single-interaction requests. tidyllm, can use send_batch() submit batch requests. OpenAI Batch API, supports 50,000 requests single batch maximum file size 100 MB. Additionally, batch API rate limits separate standard per-model limits, meaning batch usage doesn’t impact regular API rate allocations. batch sent, output send_batch() contains input list message histories along batch metadata, batchID attribute well unique names list element can used stitch together messages replies, ready. provide named list messages, tidyllm use names identifiers batch (provided names unique list element). Batches processed within 24 hours, usually much faster. batch request example processed within 10 minutes. ⚠️ Note: save RDS file disk preserve state batch request, including messages unique identifiers. file acts checkpoint, allowing us easily reload check batch status retrieve results across R sessions without needing resend entire batch close session mean time. check whether batch compeleted can load output file check_openai_batch(): Alternatively can list OpenAI batches list_openai_batches()list_batchers(openai()). course, can also look batches dashboard OpenAI platform overview. Since OpenAI batches sent .jsonl saved OpenAI server, can also look file tab dashboard delete old files time time.","code":"document_tasks |>    send_batch(openai(.json_schema = document_schema,                      .model       = \"gpt-4o-mini\")) |>   write_rds(\"document_batch.rds\") read_rds(\"document_batch.rds\")|>   check_batch(openai)"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"step-7-getting-data-from-the-entire-batch","dir":"Articles","previous_headings":"Example Workflow","what":"Step 7: Getting data from the entire batch","title":"Structured Question Answering from PDFs","text":"batch complete, fetch responses fetch_batch(): can process results table mapping get_reply_data() as_tibble() batch output: table can exported Excel writexl::write_xlsx() review. Additionally, programmatically rename PDFs using model’s suggested filenames, helping maintain organized document structure future analysis.","code":"results <- read_rds(\"document_batch.rds\")|>   fetch_batch(openai) docuemnt_table <- results |>   map(get_reply_data) |>   map_dfr(as_tibble)    docuemnt_table ## # A tibble: 24 × 9 ##    Title  Authors SuggestedFilename Type  Empirics Theory MainPoint Contribution ##    <chr>  <chr>   <chr>             <chr> <chr>    <chr>  <chr>     <chr>        ##  1 A Met… Edward… 2018_Felten_etal… Rese… The doc… The t… The docu… This paper'… ##  2 The R… Alexan… 2024_Bick_etal_R… Rese… The pap… The f… The cent… The documen… ##  3 THE A… Andrew… 2024_Caplin_etal… Rese… The doc… The d… The cent… The paper's… ##  4 Large… John J… 2023_Horton_etal… Rese… The pap… The p… The cent… The documen… ##  5 The I… Sida P… 2023_Peng_etal_G… Rese… Conduct… Theor… The stud… This resear… ##  6 GPTs … Tyna E… 2023_Eloundou_et… Rese… The stu… The d… The cent… The paper p… ##  7 Autom… Philip… 2023_Lergetporer… Rese… The stu… The t… The docu… This resear… ##  8 Artif… Andrew… 2024_Green_etal_… Rese… The rep… The t… The cent… This docume… ##  9 THE A… Andrew… 2024_Caplin_etal… Rese… The res… The p… The docu… This docume… ## 10 Autom… Daron … 2019_Acemoglu_Re… Rese… The pap… The t… The docu… This paper … ## # ℹ 14 more rows ## # ℹ 1 more variable: KeyCitations <chr>"},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"context-length","dir":"Articles","previous_headings":"Further notes on working with large documents","what":"Context length","title":"Structured Question Answering from PDFs","text":"working long documents like research papers, reports, books, one common challenge context length—maximum amount text model can process single query. document exceeds limit, model see portion , may lead missing important sections incomplete answers. models, context length measured tokens, basic units text. example, many small local models maximum context length around 8,192 tokens, roughly covering 30–35 pages. means long academic paper, model may see beginning document, potentially omitting later sections like bibliographies appendices, key references results might appear. Moreover, appending whole document prompt might leave actual prompt context. manage , common approach limit number pages sent model. workflow, focus first five pages initial overview. typically includes abstract, introduction, methodology, results, discussion—enough capture essence paper. approach ensures model can process core content, may omit information found later sections. Alternatively, large documents, split smaller sections process separately, covering content without exceeding model’s context window. However, splitting can disrupt document’s flow, may affect well model retains context across sections.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"gemini-for-image-heavy-pdfs","dir":"Articles","previous_headings":"Further notes on working with large documents","what":"Gemini for image-heavy PDFs","title":"Structured Question Answering from PDFs","text":"gemini() alternative adding text PDF llm_message(). can directly upload PDF Google’s servers gemini_upload_file() use context messages. advantage approach Gemini can handle images PDFs even image-PDFs (scanned documents). See article Video Audio Data Gemini API example use gemini_upload_file() feature.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"local-models","dir":"Articles","previous_headings":"Further notes on working with large documents","what":"Local Models","title":"Structured Question Answering from PDFs","text":"Using open-source local models PDF processing also possible, though remote models like gpt-4o-mini tend handle longer documents effectively. Smaller local models, like gemma2:9B ollama(), may struggle large content complex, structured queries, even support extended context lengths. demanding tasks, larger local models like llama3:70B may perform better complex queries, require substantial hardware resources run smoothly. Since Ollama default context length just 2,048 tokens, likely need adjust context length .num_ctx option ollama(). Sending message longer context length might lead strange errors, since input truncated Ollama, might lead cases parts document processed without instructions. Note increasing context length likely slow processing due higher memory usage. cases, reducing number pages may help. ⚠️ Note: using paid remote models, ’s important consider data privacy security, especially ’re processing sensitive documents, uploading data external servers may introduce risks — local models provide control cases.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"outlook","dir":"Articles","previous_headings":"","what":"Outlook","title":"Structured Question Answering from PDFs","text":"structured question-answering workflow streamlines extraction key insights academic papers, can also adapted document-heavy tasks. Whether ’re working reports, policy documents, news articles approach can quickly help summarize categorize information analysis.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-synthetic-data.html","id":"implementing-an-information-treatment","dir":"Articles","previous_headings":"","what":"Implementing an information treatment","title":"Generate Synthetic Data with tidyllm","text":"next step implement generate_synthetic_infotreatment() function designed simulate respondents might update answers receiving new information, known information treatment. example, treatment based study reports high exposure legal professionals automation generative AI. function takes two arguments: conversation, represents ongoing interaction LLM questionaire based responses generate_synthetic_answers(), treated, boolean flag indicating whether respondent receives information treatment. function starts retrieving synthetic respondent’s previous answers key questions output. provides AI automation-related prompt gauge respondent’s initial perception occupation’s automatable potential (“prior” belief). respondent treated group, receive additional information legal professionals’ AI exposure. presenting information, function prompts respondent reconsider initial answer, thus capturing “posterior” belief. Finally, function returns prior posterior beliefs along respondent’s demographic information, offering insights information treatment affects perceptions AI automation. simplified version function (without error-handling cleanup logic) might look like: lawyer familiar AI, low prior automatibility occupation, choose read info material update posterior. Ironically, now lawyer ’ve fully “replaced” generative AI synthetic survey respondent seems believe job safe automation. now loop lawyer profile, make answer survey, add questions. basic setup generate synthetic data tidyllm","code":"generate_synthetic_infotreatment <- function(conversation, treated) {      # Extract key initial answers (gender, birth year, familiarity with AI)   answers_opener <- tibble(     gender      = get_reply(conversation, 1),     birth_year  = get_reply(conversation, 2),     ai_familiar = get_reply(conversation, 3)   )      # Ask the prior belief question (before treatment)   prior <- conversation |>     llm_message(\"Among all occupations, how automatable do you think is your occupation?                                      0 = Not Automatable                    1 = Among the 10 lowest percent                   2 = Among the 20 lowest percent                   3 = Among the 30 lowest percent                   4 = Among the 40 lowest percent                   5 = Right in the middle                   6 = Among the top 40 percent                   7 = Among the top 30 percent                   8 = Among the top 20 percent                   9 = Among the top 10 percent                   10 = At the very top                   99 = Prefer not to say                   \") |>     ollama_chat(.model = \"gemma2\")      # Extract the prior answer (belief before the treatment)   prior_answer <- prior |> last_reply() |> str_squish()      # Default to use the conversation state of the prior answer for the untreated group   post_treatment <- prior      # Initialize the info-updating variable (0 means no treatment)   info_updating <- \"0\"      # Apply the information treatment if the treated flag is TRUE   if (treated) {      post_treatment <- prior |>       llm_message(\"A recent study titled *Occupational, Industry, and Geographic Exposure to Artificial Intelligence* by Ed Felten (Princeton), Manav Raj (University of Pennsylvania), and Robert Seamans (New York University) identified legal professionals, including lawyers and judges, as some of the occupations with the highest exposure to AI technologies.   According to the study, legal professionals are among the top 20 among 774 occupations most exposed to generative AI, suggesting that tasks traditionally performed by lawyers, such as legal research and document review, could be increasingly automated in the coming years.  Have you read this information? 1 = YES 2 = NO 99 = Prefer not to say \") |>       ollama_chat(.model = \"gemma2\")          # Update info-updating based on whether the participant confirms reading the information     info_updating <- last_reply(post_treatment)   }      # Ask the posterior belief question (after treatment)   # Untreated ar also asked if they want to update   post_treatment |>     llm_message(\"Do you want to correct your previous answer? Which of these do you pick?                                      0 = Not Automatable                    1 = Among the 10 lowest percent                   2 = Among the 20 lowest percent                   3 = Among the 30 lowest percent                   4 = Among the 40 lowest percent                   5 = Right in the middle                   6 = Among the top 40 percent                   7 = Among the top 30 percent                   8 = Among the top 20 percent                   9 = Among the top 10 percent                   10 = At the very top                   99 = Prefer not to say                   \") |>     ollama(.model = \"gemma2\")      # Extract the posterior answer (belief after the treatment)   posterior_answer <- last_reply(post_treatment)      # Combine demographic data, prior and posterior beliefs, and info-updating status   answers_opener |>     mutate(prior = prior_answer,            info_updating = info_updating,            posterior = posterior_answer) }  #Let's generate this treatment under the assumption that our first example lawyer was treated profile1_info_treatment <- profile1_questionaire |>    generate_synthetic_infotreatment(treated = TRUE)  #Print the result tibble profile1_info_treatment ## # A tibble: 1 × 6 ##   gender birth_year ai_familiar prior info_updating posterior ##   <chr>  <chr>      <chr>       <chr> <chr>         <chr>     ## 1 \"1 \\n\" 1991       4           3     2             2"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-synthetic-data.html","id":"validity-and-limitations","dir":"Articles","previous_headings":"","what":"Validity and Limitations","title":"Generate Synthetic Data with tidyllm","text":"synthetic data LLMs offers valuable insights pretesting surveys, ’s important recognize limitations approach. LLM-generated responses approximations might miss nuances come real human respondents. instance, model might accurately reflect personal biases, experiences, diverse legal practices influence real lawyers’ perspectives automation. Additionally, AI models trained vast datasets, might overgeneralization, especially niche professions data (.e. specialized lawyers). Therefore, synthetic data can streamline early iterations survey design, complement, replace, actual human feedback later stages research.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-synthetic-data.html","id":"outlook","dir":"Articles","previous_headings":"","what":"Outlook","title":"Generate Synthetic Data with tidyllm","text":"Looking ahead, integration synthetic data generation tools like tidyllm traditional survey workflows offers exciting possibilities researchers. LLMs become advanced capable simulating nuanced human behaviors, accuracy synthetic responses likely improve. lead faster, efficient iterations survey design, enabling researchers refine questions test hypotheses diverse, simulated populations real-world deployment. Moreover, future advancements may allow greater customization synthetic respondents, capturing complex demographic variables behavioral patterns. instance, enhancing ability simulate specific professions, backgrounds, even emotional states, synthetic data evolve robust tool experimental pretesting fields beyond survey research, behavioral economics, political polling, educational assessment.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"introduction-to-tidyllm","dir":"Articles","previous_headings":"","what":"Introduction to tidyllm","title":"Get Started","text":"tidyllm R package providing unified interface interacting various large language model APIs. vignette guide basic setup usage tidyllm.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"installation","dir":"Articles","previous_headings":"Introduction to tidyllm","what":"Installation","title":"Get Started","text":"install tidyllm CRAN, use: , install current development version directly GitHub using devtools:","code":"install.packages(\"tidyllm\") # Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") }  # Install TidyLLM from GitHub devtools::install_github(\"edubruell/tidyllm\")"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"setting-up-api-keys-or-ollama","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Setting up API Keys or ollama","title":"Get Started","text":"using tidyllm, set API keys services plan use. ’s set different providers: Claude models can get API key Anthropic Console: OpenAI can obtain API key signing OpenAI set : Google Gemini can setup API key Google AI Studio Mistral can set API key Mistral console page set groq (confused grok) can setup API keys Groq Console: Perplexity can get API key API settings: Alternatively, persistent storage, add keys .Renviron file: , run usethis::edit_r_environ(), add line API key file, example: want work local large lange models via ollama need install official project website. Ollama sets local large language model server can use run open-source models devices.","code":"Sys.setenv(ANTHROPIC_API_KEY = \"YOUR-ANTHROPIC-API-KEY\") Sys.setenv(OPENAI_API_KEY = \"YOUR-OPENAI-API-KEY\") Sys.setenv(GOOGLE_API_KEY = 'YOUR-GOOGLE-API-KEY') Sys.setenv(MISTRAL_API_KEY = \"YOUR-MISTRAL-API-KEY\") Sys.setenv(GROQ_API_KEY = \"YOUR-GROQ-API-KEY\") Sys.setenv(PERPLEXITY_API_KEY = \"YOUR-PERPLEXITY-API-KEY\") ANTHROPIC_API_KEY=\"YOUR-ANTHROPIC-API-KEY\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"basic-usage","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Basic Usage","title":"Get Started","text":"Let’s start simple example using tidyllm interact different language models: tidyllm built around message-centric interface design, interactions work message history, created functions like llm_message() modified API-functions like chat(). API-functions always work combination verbs providers: Verbs (e.g., chat(), embed(), send_batch()) define type action want perform. Providers (e.g., openai(), claude(), ollama()) specify API service handle action. Alternatively also provider-specific functions like openai_chat() claude_chat() work background main verbs can also call directly. documentation provider-specific functions offers comprehensive overview full range actions input types supported API.","code":"library(tidyllm)  # Start a conversation with Claude conversation <- llm_message(\"What is the capital of France?\") |>   chat(claude())  #Standard way that llm_messages are printed conversation ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## What is the capital of France? ## -------------------------------------------------------------- ## assistant: ## The capital of France is Paris. ## -------------------------------------------------------------- # Continue the conversation with ChatGPT conversation <- conversation |>   llm_message(\"What's a famous landmark in this city?\") |>   chat(openai)  get_reply(conversation) ## [1] \"A famous landmark in Paris is the Eiffel Tower.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"sending-images-to-models","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Sending Images to Models","title":"Get Started","text":"tidyllm also supports sending images multimodal models. Let’s send picture : let ChatGPT guess picture made:","code":"# Describe an image using a llava model on ollama image_description <- llm_message(\"Describe this picture? Can you guess where it was made?\",                                  .imagefile = \"picture.jpeg\") |>   chat(openai(.model = \"gpt-4o\"))  # Get the last reply get_reply(image_description) ## [1] \"The picture shows a beautiful landscape with a lake, mountains, and a town nestled below. The sun is shining brightly, casting a serene glow over the water. The area appears lush and green, with agricultural fields visible. \\n\\nThis type of scenery is reminiscent of northern Italy, particularly around Lake Garda, which features similar large mountains, picturesque water, and charming towns.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"adding-pdfs-to-messages","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Adding PDFs to messages","title":"Get Started","text":"llm_message() function also supports extracting text PDFs including message. allows easily provide context PDF document interacting AI assistant. use feature, need pdftools package installed. already installed, can install : include text PDF prompt, simply pass file path .pdf argument chat function: package automatically extract text PDF include prompt sent API. text wrapped <pdf> tags clearly indicate content PDF:","code":"install.packages(\"pdftools\") llm_message(\"Please summarize the key points from the provided PDF document.\",       .pdf = \"die_verwandlung.pdf\") |>      chat(openai(.model = \"gpt-4o-mini\")) ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## Please summarize the key points from the provided PDF ## document. ##  -> Attached Media Files:  die_verwandlung.pdf  ## -------------------------------------------------------------- ## assistant: ## Here are the key points from the provided PDF document 'Die ## Verwandlung' by Franz Kafka: ##  ## 1. The story centers around Gregor Samsa, who wakes up one ## morning to find that he has been transformed into a giant ## insect-like creature. ##  ## 2. Gregor's transformation causes distress and disruption ## for his family. They struggle to come to terms with the ## situation and how to deal with Gregor in his new state. ##  ## 3. Gregor's family, especially his sister Grete, initially ## tries to care for him, but eventually decides they need ## to get rid of him. They lock him in his room and discuss ## finding a way to remove him. ##  ## 4. Gregor becomes increasingly isolated and neglected by ## his family. He becomes weaker and less mobile due to his ## injuries and lack of proper care. ##  ## 5. Eventually, Gregor dies, and his family is relieved. ## They then begin to make plans to move to a smaller, more ## affordable apartment and start looking for new jobs and ## opportunities. ## -------------------------------------------------------------- Please summarize the key points from the provided PDF document.  <pdf filename=\"example_document.pdf\"> Extracted text from the PDF file... <\/pdf>"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"sending-r-outputs-to-language-models","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Sending R Outputs to Language Models","title":"Get Started","text":"can automatically include R code outputs prompts. llm_message() optional argument .f can specify (anonymous) function, run console output captured appended message run . addition can use .capture_plot send last plot pane model.  Now can send plot data summary language model:","code":"library(tidyverse)  # Create a plot for the mtcars example data ggplot(mtcars, aes(wt, mpg)) +   geom_point() +   geom_smooth(method = \"lm\", formula = 'y ~ x') +   labs(x=\"Weight\",y=\"Miles per gallon\") library(tidyverse) llm_message(\"Analyze this plot and data summary:\",                    .capture_plot = TRUE, #Send the plot pane to a model                   .f = ~{summary(mtcars)}) |> #Run summary(data) and send the output   chat(claude()) ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## Analyze this plot and data summary: ##  -> Attached Media Files:  file1568f6c1b4565.png, RConsole.txt  ## -------------------------------------------------------------- ## assistant: ## Based on the plot and data summary provided, here's an ## analysis: ##  ## 1. Relationship between Weight and MPG: ## The scatter plot shows a clear negative correlation between ## weight (wt) and miles per gallon (mpg). As the weight of the ## car increases, the fuel efficiency (mpg) decreases. ##  ## 2. Linear Trend: ## The blue line in the plot represents a linear regression ## fit. The downward slope confirms the negative relationship ## between weight and mpg. ##  ## 3. Data Distribution: ## - The weight of cars in the dataset ranges from 1.513 to ## 5.424 (likely in thousands of pounds). ## - The mpg values range from 10.40 to 33.90. ##  ## 4. Variability: ## There's some scatter around the regression line, indicating ## that while weight is a strong predictor of mpg, other ## factors also influence fuel efficiency. ##  ## 5. Other Variables: ## While not shown in the plot, the summary statistics provide ## information on other variables: ## - Cylinder count (cyl) ranges from 4 to 8, with a median of ## 6. ## - Horsepower (hp) ranges from 52 to 335, with a mean of ## 146.7. ## - Transmission type (am) is binary (0 or 1), likely ## indicating automatic vs. manual. ##  ## 6. Model Fit: ## The grey shaded area around the regression line represents ## the confidence interval. It widens at the extremes of the ## weight range, indicating less certainty in predictions for ## very light or very heavy vehicles. ##  ## 7. Outliers: ## There are a few potential outliers, particularly at the ## lower and higher ends of the weight spectrum, that deviate ## from the general trend. ##  ## In conclusion, this analysis strongly suggests that ## weight is a significant factor in determining a car's fuel ## efficiency, with heavier cars generally having lower mpg. ## However, the presence of scatter in the data indicates that ## other factors (possibly related to engine characteristics, ## transmission type, or aerodynamics) also play a role in ## determining fuel efficiency. ## --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"getting-replies-from-the-api","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Getting replies from the API","title":"Get Started","text":"Retrieve assistant reply text message history get_reply(). Specify index choose assistant message get: default get_reply() gets last assistant message. Alternatively can also use last_reply() shortcut latest response. can convert text (without attachments) tibble as_tibble(): can use get_metadata() function retrieve metadata models token usage assistant replies: default collects metadata whole message history, can also set .index get metadata specific reply. Alternatively can print metadata standard print method .meta-argument print() via tidyllm_print_metadata option. list column api_specific contains special metadata available APIs (like citations models search grounding perplexity() special information model loading local APIs).","code":"conversation <- llm_message(\"Imagine a German adress.\") |>      chat(groq()) |>      llm_message(\"Imagine another address\") |>      chat(claude())  conversation ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## Imagine a German adress. ## -------------------------------------------------------------- ## assistant: ## Let's imagine a German address: ##  ## Herr Müller ## Musterstraße 12 ## 53111 Bonn ## -------------------------------------------------------------- ## user: ## Imagine another address ## -------------------------------------------------------------- ## assistant: ## Let's imagine another German address: ##  ## Frau Schmidt ## Fichtenweg 78 ## 42103 Wuppertal ## -------------------------------------------------------------- #Getting the first reply conversation |> get_reply(1) ## [1] \"Let's imagine a German address: \\n\\nHerr Müller\\nMusterstraße 12\\n53111 Bonn\" #By default it gets the last reply conversation |> get_reply() ## [1] \"Let's imagine another German address:\\n\\nFrau Schmidt\\nFichtenweg 78\\n42103 Wuppertal\" conversation |> as_tibble() ## # A tibble: 5 × 2 ##   role      content                                                              ##   <chr>     <chr>                                                                ## 1 system    \"You are a helpful assistant\"                                        ## 2 user      \"Imagine a German adress.\"                                           ## 3 assistant \"Let's imagine a German address: \\n\\nHerr Müller\\nMusterstraße 12\\n… ## 4 user      \"Imagine another address\"                                            ## 5 assistant \"Let's imagine another German address:\\n\\nFrau Schmidt\\nFichtenweg … conversation |> get_metadata() ## # A tibble: 2 × 6 ##   model  timestamp           prompt_tokens completion_tokens ##   <chr>  <dttm>                      <dbl>             <dbl> ## 1 groq-… 2024-11-08 14:25:43            20                45 ## 2 claud… 2024-11-08 14:26:02            80                40 ## # ℹ 2 more variables: total_tokens <dbl>, ## #   api_specific <list>"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"working-with-structured-model-outputs","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Working with structured model outputs","title":"Get Started","text":"make model responses easy interpret integrate workflow, tidyllm supports defining schemas ensure models reply structured outputs JSON (JavaScript Object Notation) following specifications. JSON standard format organizing data simple key-value pairs, human-readable machine-friendly. Currently, openai(), gemini() ollama() API provider functions tidyllm support schema enforcement .json_schema argument. ensures replies conform pre-defined consistent data formatting. create schemas, can use tidyllm_schema() function, translates data format specifications JSON-schema format API requires. helper function standardizes data layout ensuring flat (non-nested) JSON structures defined data types. ’s define schema: name: name identifier schema. default name \"tidyllm_schema\". \"character\" \"string\": Text fields. \"factor(...)\": Enumerations allowable values, like factor(Germany, France). \"logical\": TRUE FALSE \"numeric\": Numeric fields. \"type[]\": Lists given type, \"character[]\". Alternatively, can use set field functions offer additional flexibility readability defining fields. functions allow provide descriptions field, giving models context fields assistant reply filled: field_chr(): text fields. field_dbl(): numeric fields. field_lgl(): boolean fields. field_fct(): enumerations (factors), .levels argument specify valid values. functions accept .description (adding descriptions) .vector = TRUE (defining lists). ellmer package installed, can also use ellmer type definitions directly tidyllm_schema(). example, can pass objects like ellmer::type_string() ellmer::type_object() field definitions. Moreover, can supply entire ellmer::type_object() schema directly .json_schema argument API calls. ’s example schema defining address format: model responded JSON format, organizing data key-value pairs like specified. can convert JSON output R list easier handling get_reply_data(): Ollama, OpenAI Google Gemini API work schema provided tidyllm_schema(). API providers like groq(), mistral() support structured outputs simpler JSON mode, accessible .json argument provider functions. Since APIs currently support native schema enforcement, ’ll need prompt model follow specified format directly messages. Although get_reply_data() can help extract structured data responses set .json=TRUE API provider functions, model may always adhere strictly specified structure. APIs support native schema enforcement, tidyllm integrate full schema functionality . Alternatively, also supports flexible schema objects ellmer package latest development version.","code":"address_schema <- tidyllm_schema(   name = \"AddressSchema\",   street = \"character\",   houseNumber = \"numeric\",   postcode = field_chr(\"A postal code for a city\"),   city = \"character\",   region = \"character\",   country = field_fct(\"A European Country\", .levels=(\"Germany\",\"France\")) ) address <- llm_message(\"Imagine an address in JSON format that matches the schema.\") |>         chat(openai(),.json_schema = address_schema) address ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## Imagine an address in JSON format that matches the schema. ## -------------------------------------------------------------- ## assistant: ## {\"street\":\"Hauptstraße\",\"houseNumber\":123,\"postcode\":\"10115\",\"city\":\"Berlin\",\"region\":\"Berlin\",\"country\":\"Germany\"} ## -------------------------------------------------------------- address |> get_reply_data() |> str() ## List of 6 ##  $ street     : chr \"Hauptstraße\" ##  $ houseNumber: int 123 ##  $ postcode   : chr \"10115\" ##  $ city       : chr \"Berlin\" ##  $ region     : chr \"Berlin\" ##  $ country    : chr \"Germany\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"api-parameters","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"API parameters","title":"Get Started","text":"Different API functions support different model parameters like, deterministic response via parameters like temperature. can set parameters via arguments verbs provider functions. Please read API-documentation documentation model functions specific examples. Common arguments, temperature, can specified directly main verbs like chat(). example, chat() supports arguments .model,.temperature, .json_schema (see full list chat() documentation) directly. Provider-specific arguments—.ollama_server ollama() .fileid gemini() set directly provider function: argument provided chat() provider, value specified chat() takes precedence. instance: common argument set chat supported provider chat() raise error. example, sending .json_schema provider support raise error:","code":"temp_example <- llm_message(\"Explain how temperature parameters work  in large language models and why temperature 0 gives you deterministic outputs  in one sentence.\")      #per default it is non-zero   temp_example |> chat(ollama,.temperature=0) ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## Explain how temperature parameters work ## in large language models and why temperature 0 gives you ## deterministic ## outputs in one sentence. ## -------------------------------------------------------------- ## assistant: ## In large language models, temperature parameters control ## the randomness of generated text by scaling the output ## probabilities, with higher temperatures introducing more ## uncertainty and lower temperatures favoring more likely ## outcomes; specifically, setting temperature to 0 effectively ## eliminates all randomness, resulting in deterministic ## outputs because it sets the probability of each token to its ## maximum likelihood value. ## -------------------------------------------------------------- #Retrying with .temperature=0   temp_example |> chat(ollama,.temperature=0) ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## Explain how temperature parameters work ## in large language models and why temperature 0 gives you ## deterministic ## outputs in one sentence. ## -------------------------------------------------------------- ## assistant: ## In large language models, temperature parameters control ## the randomness of generated text by scaling the output ## probabilities, with higher temperatures introducing more ## uncertainty and lower temperatures favoring more likely ## outcomes; specifically, setting temperature to 0 effectively ## eliminates all randomness, resulting in deterministic ## outputs because it sets the probability of each token to its ## maximum likelihood value. ## -------------------------------------------------------------- conversation <- llm_message(\"Hello\") |>    chat(ollama(.ollama_server = \"http://localhost:11434\"),         .temperature = 0) #This uses GPT-4o conversation <- llm_message(\"Hello\") |>    chat(openai(.model=\"gpt-4o-mini\"),        .model=\"gpt-4o\") address <- llm_message(\"Imagine an address in JSON format that matches the schema.\") |>         chat(groq(),.json_schema = address_schema)"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"embeddings","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Embeddings","title":"Get Started","text":"Embedding models tidyllm transform textual inputs vector representations, capturing semantic information can enhance similarity comparisons, clustering, retrieval tasks. can generate embeddings embed()-function. functions return semantic vector representation either message message history , typically application, entry character vector: output tibble two columns, text input embedding list column contains vector semantic embeddings input.","code":"c(\"What is the meaning of life?\",   \"How much wood would a woodchuck chuck?\",   \"How does the brain work?\") |>   embed(ollama) ## # A tibble: 3 × 2 ##   input                                  embeddings  ##   <chr>                                  <list>      ## 1 What is the meaning of life?           <dbl [384]> ## 2 How much wood would a woodchuck chuck? <dbl [384]> ## 3 How does the brain work?               <dbl [384]>"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"batch-requests","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Batch requests","title":"Get Started","text":"Anthropic, OpenAI Mistral offer batch request options around 50% cheaper standard single-interaction APIs. Batch processing allows submit multiple message histories , processed together model providers servers, usually within 24-hour period. tidyllm, can use send_batch() function submit batch requests either API. ’s example send batch request Claude’s batch API: send_batch() function returns list message histories input, marked attribute contains batch-id Claude API well unique names list element can used stitch together messages replies, ready. provide named list messages, tidyllm use names identifiers batch, names unique. Tip: Saving batch requests file allows persist across R sessions, making easier manage large jobs access results later. sending batch request, can check status check_batch(). example: status output shows details number successful, errored, expired, canceled requests batch, well current status. can also see batch requests list_batches() (batches dashboard API provider). processing batch completed can fetch results fetch_batch(): output list message histories, now updated new assistant replies. can process responses tidyllm’s standard tools. launching large batch operation, ’s good practice run test requests review outputs standard chat() function. approach helps confirm prompt settings model configurations produce desired responses, minimizing potential errors resource waste.","code":"#Create a message batch and save it to disk to fetch it later glue(\"Write a poem about {x}\", x=c(\"cats\",\"dogs\",\"hamsters\")) |>   purrr::map(llm_message) |>   send_batch(claude()) |>   saveRDS(\"claude_batch.rds\") #Check the status of the batch readRDS(\"claude_batch.rds\") |>    check_batch(claude()) ## # A tibble: 1 × 8 ##   batch_id          status created_at          expires_at          req_succeeded ##   <chr>             <chr>  <dttm>              <dttm>                      <dbl> ## 1 msgbatch_02A1B2C… ended  2024-11-01 10:30:00 2024-11-02 10:30:00             3 ## # ℹ 3 more variables: req_errored <dbl>, req_expired <dbl>, req_canceled <dbl> conversations <- readRDS(\"claude_batch.rds\") |>   fetch_batch(claude())  poems <- map_chr(conversations, get_reply)"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"streaming-back-responses-experimental","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Streaming back responses (Experimental)","title":"Get Started","text":"chat() supports real-time streaming reply tokens console model works .stream=TRUE argument chat(), api providers. feature offers slightly better feedback model behavior real-time, ’s particularly useful data-analysis workflows. Metadata collected streaming mode. consider feature experimental recommend using non-streaming responses production tasks. Note error handling streaming callbacks varies API differs quality time. Metadata currently supported streaming responses.","code":"llm_message(\"Write a lengthy magazine advertisement for an R package called tidyllm\") |>   chat(claude,.stream=TRUE)"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"choosing-the-right-model-and-api","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Choosing the Right Model and API","title":"Get Started","text":"tidyllm supports multiple APIs, offering distinct large language models varying strengths. choice model API use often depends specific task, cost considerations, data privacy concerns. OpenAI API: Models OpenAI API, particularly GPT-4o model, extremely versatile perform well across wide range tasks, including text generation, code completion, multimodal analysis. addition o1-reasoning models offer good performance set specific task (relatively high price). also azure_openai() provider function prefer use OpenAI API Microsoft Azure. Anthropic API: Claude known generating thoughtful, nuanced responses, making ideal tasks require human-like reasoning, summarization creative writing. However, can sometimes verbose necessary, lacks direct JSON support, requires additional prompting validation ensure structured output. Google Gemini API: Google Gemini great long-context tasks — can handle million tokens! addition, can use .grounding_threshold-parameter gemini_chat() function ground responses based Google searches. lower threshold Gemini relies search instead internal knowledge: Moreover, Gemini API able upload wide range media files use prompts models functions like gemini_upload_file(). Using , Gemini models can used process video audio together messages. example summarizing speech: Mistral API (EU-based): Mistral offers lighter-weight, open-source models developed hosted EU, making particularly appealing data protection (e.g., GDPR compliance) concern. models may powerful GPT-4o Claude Sonnet, Mistral offers good performance standard text generation tasks. Groq API (Fast): Groq offers unique advantage custom AI accelerator hardware, get fastest output available API. delivers high performance low costs, especially tasks require fast execution. hosts many strong open-source models, like lamma3:70b. also groq_transcribe() function available allows transcribe audio files Whipser-Large model Groq API. Perplexity API (Search Citations): Perplexity combines current finetuned Llama models real-time web search capabilities. allows --date information retrieval integration responses. answers contain links citations can accesed via api_specific column get_metadata() Ollama (Local Models): data privacy priority, running open-source models like gemma2::9B locally via ollama gives full control model execution data. However, trade-local models require significant computational resources, often quite powerful large API-providers. ollama blog regularly posts new models advantages can download via ollama_download_model(). OpenAI-compatible Local Models: Besides ollama, many solutions run local models mostly compatible OpenAI API like llama.cpp, vllm many . use API can set base url api .api_url well path model-endpoint .api_path argument openai() provider function. Set .compatible=TRUE skip api-key checks rate-limit tracking. Compatibility local models solutions may vary depending specific API’s implementation, full functionality guaranteed. Ideally can save complicated configurations like object:","code":"llm_message(\"What is tidyllm and who maintains this package?\") |>   gemini_chat(.grounding_threshold = 0.3) #Upload a file for use with gemini upload_info <- gemini_upload_file(\"example.mp3\")  #Make the file available during a Gemini API call llm_message(\"Summarize this speech\") |>   chat(gemini(.fileid = upload_info$name))    #Delte the file from the Google servers after you are done gemini_delete_file(upload_info$name) my_provider <- openai(.model=\"llama3.2:90b\",           .api_url=\"http://localhost:11434\",           .compatible = TRUE,           .api_path = \"/v1/chat/custom/\"           )  llm_message(\"Hi there\") |>   chat(my_provider)"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"setting-a-default-provider","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Setting a Default Provider","title":"Get Started","text":"can also specify default provider options:","code":"# Set default providers #chat provider options(tidyllm_chat_default = openai(.model = \"gpt-4o\")) #embedding provider options(tidyllm_embed_default = ollama(.model = \"all-minilm\")) #send batch provider options(tidyllm_sbatch_default = claude(.temperature=0)) #check batch provider options(tidyllm_cbatch_default = claude()) #fetch batch provider options(tidyllm_fbatch_default = claude()) #List batches provider options(tidyllm_lbatch_default = claude())  # Now you can use chat() or embed() without explicitly specifying a provider conversation <- llm_message(\"Hello, what is the weather today?\") |>    chat()  embeddings <- c(\"What is AI?\", \"Define machine learning.\") |>    embed()  # Now you can use batch functions without explicitly specifying a provider batch_messages <- list(   llm_message(\"Write a poem about the sea.\"),   llm_message(\"Summarize the theory of relativity.\"),   llm_message(\"Invent a name for a new genre of music.\") )  # Send batch using default for send_batch() batch_results <- batch_messages |> send_batch()  # Check batch status using default for check_batch() status <- check_batch(batch_results)  # Fetch completed results using default for fetch_batch() completed_results <- fetch_batch(batch_results)  # List all batches using default for list_batches() all_batches <- list_batches()"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"a-common-classification-task","dir":"Articles","previous_headings":"","what":"A Common Classification Task","title":"Classifying Texts with tidyllm","text":"Imagine ’ve just collected thousands survey responses people describe jobs words. responses detailed, others vague, ’s plenty variation . Now, need categorize standardized occupation codes, like SOC classification system Bureau Labor Statistics. Manually sorting response take days, weeks, inconsistencies coders almost guaranteed. instance, dataset might look something like : 7,000 rows occupation descriptions, ranging “Librarian” “Making sure everything runs” goal classify messy responses one 22 2-digit occupation codes: article, take structured approach tackle classification task efficiently. ’s step--step workflow kind task: Pick Sample:Start filtering dataset retain distinct occupation descriptions. , randomly select sample distinct responses work . Initial Classification: Use simple prompt categorize responses occupation codes. Manual Correction: Review correct classifications create reliable ground truth. Training/Test Split: split ground truth dataset using rsample training test sets. Experimentation: Test different prompts, models, parameters training set, comparing one-shot multi-shot approaches. Model Evaluation: Use yardstick find best-performing combination training data. Testing: Apply best-performing model test set evaluate well performs unseen occupation descriptions. Full Classification: Use validated model setup classify entire dataset efficiently.","code":"library(tidyllm) library(tidyverse) library(glue) occ_data <- read_rds(\"occupation_data.rds\") occ_data ## # A tibble: 7,000 × 2 ##    respondent occupation_open                   ##         <int> <chr>                             ##  1     100019 Ops oversight and strategy        ##  2     100266 Coordinating operations           ##  3     100453 Making sure everything runs       ##  4     100532 Building and demolition           ##  5     100736 Help lawyers with cases           ##  6     100910 I sell mechanical parts           ##  7     101202 Librarian                         ##  8     101325 Operations planning and execution ##  9     101329 Bookkeeper                        ## 10     101367 Kitchen staff                     ## # ℹ 6,990 more rows occ_codes <- read_rds(\"occ_codes_2digits.rds\") |>   print(n=Inf) ## # A tibble: 22 × 2 ##     occ2 occ_title                                                  ##    <dbl> <chr>                                                      ##  1    11 Management Occupations                                     ##  2    13 Business and Financial Operations Occupations              ##  3    15 Computer and Mathematical Occupations                      ##  4    17 Architecture and Engineering Occupations                   ##  5    19 Life, Physical, and Social Science Occupations             ##  6    21 Community and Social Service Occupations                   ##  7    23 Legal Occupations                                          ##  8    25 Educational Instruction and Library Occupations            ##  9    27 Arts, Design, Entertainment, Sports, and Media Occupations ## 10    29 Healthcare Practitioners and Technical Occupations         ## 11    31 Healthcare Support Occupations                             ## 12    33 Protective Service Occupations                             ## 13    35 Food Preparation and Serving Related Occupations           ## 14    37 Building and Grounds Cleaning and Maintenance Occupations  ## 15    39 Personal Care and Service Occupations                      ## 16    41 Sales and Related Occupations                              ## 17    43 Office and Administrative Support Occupations              ## 18    45 Farming, Fishing, and Forestry Occupations                 ## 19    47 Construction and Extraction Occupations                    ## 20    49 Installation, Maintenance, and Repair Occupations          ## 21    51 Production Occupations                                     ## 22    53 Transportation and Material Moving Occupations"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"classifying-a-sub-sample","dir":"Articles","previous_headings":"","what":"Classifying a Sub-Sample","title":"Classifying Texts with tidyllm","text":"start ensuring classify distinct responses. eliminates duplicates ensures efficient reliable classification process: help us focus variations across distinct occupations, avoiding repeated classification efforts identical responses. Next, divide distinct occupations sub-sample manual classification remaining portion used later. use initial_split() function rsample package, splitting 10% data smaller test set manual correction model training: splitting data, now smaller sub-sample 422 observations work initial classification stage.","code":"# Pick only distinct occupations from the dataset distinct_occupations <- occ_data |>    distinct(occupation = occupation_open)  print(distinct_occupations, n = 5) ## # A tibble: 2,209 × 1 ##   occupation                  ##   <chr>                       ## 1 Ops oversight and strategy  ## 2 Coordinating operations     ## 3 Making sure everything runs ## 4 Building and demolition     ## 5 Help lawyers with cases     ## # ℹ 2,204 more rows #Set a seed for reproducability set.seed(123)  # Split the distinct occupations into a sub-sample (10%) and the rest (90%) library(rsample) occ_split <- initial_split(distinct_occupations, prop = 0.8)  # Retrieve the sub-sample and the remaining data rest_of_data <- training(occ_split) sub_sample <- testing(occ_split)  print(sub_sample, n = 5)  ## # A tibble: 442 × 1 ##   occupation                      ##   <chr>                           ## 1 Making sure everything runs     ## 2 Bartender                       ## 3 Post-secondary health education ## 4 Food servin                     ## 5 Exectutive assistant            ## # ℹ 437 more rows"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"a-simple-sequential-classifier","dir":"Articles","previous_headings":"Classifying a Sub-Sample","what":"A simple sequential classifier","title":"Classifying Texts with tidyllm","text":"classify sub-sample occupation descriptions, first generate prompts specific classification task occupation. prompt directs model classify occupation based numerical code, ensuring output aligns coding system. use glue() create prompts occupation description prompt {occupation} purrr::map() pass prompt llm_message(). output list messages ready submission large language model. typical message list classification tasks looks like : classify initial subset, need manually verified later, beneficial use reliable commercial model, output typically requires fewer adjustments. , choose Claude-3.5-Sonnet, setting .temperature 0 ensure deterministic (non-random) responses. Initially, test prompt standard sequential chat(claude()) approach. preliminary step allows us verify model interprets prompt accurately produces output required format running full batch classification tasks. end, create tibble containing occupation descriptions sub-sample alongside prepared classification tasks. randomly select 10 rows tibble. selected occupation passed sequentially classify_sequential() function passes classification task chat(claude()), fetches reply get_reply() converts text output model numeric code parse_number(). Finally, results joined occ_codes table map numeric occupation codes corresponding occupation labels, providing interpretable output: output demonstrates model accurately interprets prompts, assigning occupation description suitable category. Another check classify entire set messages classification_tasks using Anthropic’s Claude-3.5-Sonnet model, look token-usage cost typical message get_metdadata(): Assuming messages initial batch roughly similar, can expect 442 messages need around 150,000 prompt tokens 2200 completion tokens. time writing, price one million input tokens Claude 3$, one million output tokens cost 15$. Prices using batch request half. Classifying subsample batch-processing costs roughly 22 cents.","code":"prompts <- glue('       Classify this occupation response from a survey: {occupation}              Pick one of the following numerical codes from this list.        Respond only with the code!       11 = Management Occupations                                           13 = Business and Financial Operations Occupations                    15 = Computer and Mathematical Occupations                            17 = Architecture and Engineering Occupations                         19 = Life, Physical, and Social Science Occupations                   21 = Community and Social Service Occupations                         23 = Legal Occupations                                                25 = Educational Instruction and Library Occupations                  27 = Arts, Design, Entertainment, Sports, and Media Occupations       29 = Healthcare Practitioners and Technical Occupations               31 = Healthcare Support Occupations                                   33 = Protective Service Occupations                                   35 = Food Preparation and Serving Related Occupations                 37 = Building and Grounds Cleaning and Maintenance Occupations        39 = Personal Care and Service Occupations                            41 = Sales and Related Occupations                                    43 = Office and Administrative Support Occupations                    45 = Farming, Fishing, and Forestry Occupations                       47 = Construction and Extraction Occupations                          49 = Installation, Maintenance, and Repair Occupations                51 = Production Occupations                                           53 = Transportation and Material Moving Occupations       99 = Missing Occupation (No clear occupation)', occupation = sub_sample$occupation)  classification_tasks <- map(prompts,llm_message) classification_tasks[[1]] ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## Classify this occupation response from a survey: Making sure ## everything runs ##  ## Pick one of the following numerical codes from this list. ## Respond only with the code! ## 11 = Management Occupations ## 13 = Business and Financial Operations Occupations ## 15 = Computer and Mathematical Occupations ## 17 = Architecture and Engineering Occupations ## 19 = Life, Physical, and Social Science Occupations ## 21 = Community and Social Service Occupations ## 23 = Legal Occupations ## 25 = Educational Instruction and Library Occupations ## 27 = Arts, Design, Entertainment, Sports, and Media ## Occupations ## 29 = Healthcare Practitioners and Technical Occupations ## 31 = Healthcare Support Occupations ## 33 = Protective Service Occupations ## 35 = Food Preparation and Serving Related Occupations ## 37 = Building and Grounds Cleaning and Maintenance ## Occupations ## 39 = Personal Care and Service Occupations ## 41 = Sales and Related Occupations ## 43 = Office and Administrative Support Occupations ## 45 = Farming, Fishing, and Forestry Occupations ## 47 = Construction and Extraction Occupations ## 49 = Installation, Maintenance, and Repair Occupations ## 51 = Production Occupations ## 53 = Transportation and Material Moving Occupations ## 99 = Missing Occupation (No clear occupation) ## -------------------------------------------------------------- classify_sequential <- function(occupation_open,message){     raw_code <- message |>         chat(claude(.temperature = 0)) |>         get_reply() |>         parse_number()          tibble(occupation_open=occupation_open, occ2=raw_code) }  tibble(occupation_open = sub_sample$occupation, message = classification_tasks) %>%   slice_sample(n=10) |>   pmap_dfr(classify_sequential) |>   left_join(occ_codes, by=\"occ2\") ## # A tibble: 10 × 3 ##    occupation_open                  occ2 occ_title                               ##    <chr>                           <dbl> <chr>                                   ##  1 Fine Carpentry                     47 Construction and Extraction Occupations ##  2 Eyeglass makin'                    51 Production Occupations                  ##  3 Layin' down shingles               47 Construction and Extraction Occupations ##  4 Handle construction budgets        13 Business and Financial Operations Occu… ##  5 Bodyshop guy                       49 Installation, Maintenance, and Repair … ##  6 Hair dresser                       39 Personal Care and Service Occupations   ##  7 Sort and deliver mail              43 Office and Administrative Support Occu… ##  8 Ops oversight                      11 Management Occupations                  ##  9 Oversee all cleaning operations    11 Management Occupations                  ## 10 Systems administrator              15 Computer and Mathematical Occupations classification_tasks[[1]] |>   chat(claude(.temperature = 0)) |>   get_metadata() ## # A tibble: 1 × 5 ##   model                   timestamp prompt_tokens completion_tokens total_tokens ##   <chr>                   <lgl>             <dbl>             <dbl>        <dbl> ## 1 claude-3-5-sonnet-2024… NA                  349                 5          354"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"using-a-claude-batch-request","dir":"Articles","previous_headings":"Classifying a Sub-Sample","what":"Using a claude batch request","title":"Classifying Texts with tidyllm","text":"Batch-processing offers significant cost advantage sequential classification. Anthropic charges half price per token batch requests compared single requests, making efficient choice large-scale tasks. batch requests return results instantly, typically processed within 24 hours, often much faster. Message Batch limited either 10,000 requests 32 MB size, whichever reached first. initiate batch request, use send_claude_batch() function (can course also use general verb send_batch(claude()). batch submission function returns list classification tasks uploaded, marked attribute contains batch-id Claude API well unique names list element can used stitch together messages replies, batch ready. supply named list messages, names used identifiers (ensuring unique submission). convenient access later, save batch output locally, allowing us resume processing batch even closing R session: can check status batch : now see batch created status ended. 442 requests succeeded. Batch results can downloaded within 29 days creation. can see available batches API list_claude_batches() Anthropic console. download results batch use fetch_claude_batch() function: get list messages assistant replies fetch_claude_batch() pass map_chr(get_reply) get assistant replies message character vector. parse numeric code merge occupation titles. ensure output correct, can export results Excel write_xlsx writexl package manually fix miss-classifications. manual review classifier’s output showed highly promising results. 443 classifications, 9 needed corrections, indicating error rate just 2% claude()-based classifier initial prompt. issues arose correctly identifying unclear responses missing, “Doin’ numbers” “Barster” (potential mix Barrister Barkeeper). point stop proceed classify entire data send_claude_batch(), given strong performance manual validation unseen data scaling data cost less 1.50$. However, illustrate general principle find good LLM-based classifiers less sure, now use initial ground truth built base experiment different models prompts. example, try determine simpler alternatives small local models perform just well.","code":"classification_tasks |>   send_claude_batch(.temperature = 0) |>   write_rds(\"sub_sample_batch.rds\") read_rds(\"sub_sample_batch.rds\") |>   check_claude_batch() ## # A tibble: 1 × 8 ##   batch_id          status created_at          expires_at          req_succeeded ##   <chr>             <chr>  <dttm>              <dttm>                      <dbl> ## 1 msgbatch_015JWDH… ended  2024-10-31 16:33:35 2024-11-01 16:33:35           442 ## # ℹ 3 more variables: req_errored <dbl>, req_expired <dbl>, req_canceled <dbl> occ2_codes <- read_rds(\"sub_sample_batch.rds\")) |>   fetch_claude_batch() |>   map_chr(get_reply) |>   parse_number()  tibble(occupation_open = sub_sample$occupation,        occ2 = occ2_codes) |>   left_join(occ_codes, by=\"occ2\") |>   writexl::write_xlsx(\"ground_truth_excel.xlsx\")"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"optimizing-and-testing-classifiers","dir":"Articles","previous_headings":"","what":"Optimizing and Testing Classifiers","title":"Classifying Texts with tidyllm","text":"Next, present general approach optimize test different LLM-based classifiers. First, split dataset training test sets using rsample ensure can experiment different prompts setups training data evaluate final model performance unseen data.","code":"ground_truth <- readxl:::read_xlsx(\"ground_truth_corrected.xlsx\")  # Split the ground-truth into training and testing sets set.seed(123) gt_split <- initial_split(ground_truth, prop = 0.7)  # Retrieve training and testing data train_data <- training(gt_split) test_data  <- testing(gt_split)  print(train_data,n=5) ## # A tibble: 309 × 3 ##   occupation_open              occ2 occ_title                                    ##   <chr>                       <dbl> <chr>                                        ## 1 Computer network technician    15 Computer and Mathematical Occupations        ## 2 Educational support            25 Educational Instruction and Library Occupat… ## 3 Fine Carpentry                 47 Construction and Extraction Occupations      ## 4 Keep things organized          43 Office and Administrative Support Occupatio… ## 5 Group fitness instructor       39 Personal Care and Service Occupations        ## # ℹ 304 more rows"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"writing-a-flexible-classifier-function","dir":"Articles","previous_headings":"Optimizing and Testing Classifiers","what":"Writing a flexible classifier function","title":"Classifying Texts with tidyllm","text":"test different prompts models systematically need create flexible classifier function can handle different prompts models can work api-function including local models support batch requests. function can take open occupation question, ground-truth occ2 prompt prompt_id, provider model arguements allowing us test occupation classification across grid prompts models.","code":"# External numerical code list for reusability numerical_code_list <- c('       11 = Management Occupations                                           13 = Business and Financial Operations Occupations                    15 = Computer and Mathematical Occupations                            ... abreviated ...       51 = Production Occupations                                           53 = Transportation and Material Moving Occupations       99 = Missing Occupation')  # Classification function that accepts prompt, api_function, and model  # as well as the ground truth to pass through as arguments classify_occupation_grid <- function(occupation,                                 occ2,                                 prompt,                                 prompt_id,                                 provider,                                 model){   # Output what the model is currently doing to the console   glue(\"Classifying: {model} - {prompt_id} - {occupation}\\n\") |> cat(\"\\n\")      # List of valid codes as strings   valid_codes <- as.character(occ_codes$occ2)      # Initialize classification_raw   classification <- tryCatch({     # Get the assistant's reply using the dynamically provided API function and model     assistant_reply <- llm_message(prompt) |>       chat(provider(.model = model, .temperature = 0)) |>       get_reply() |>       str_squish()          # Validate the assistant's reply     if (assistant_reply %in% valid_codes) {       as.integer(assistant_reply)     } else {       98L  # Return 98 for invalid responses     }   }, error = function(e){     97L  # Return 97 in case of an error (e.g., API failure)   })      # Return a tibble containing the original occupation description and classification result   tibble(     occupation_open = occupation,     occ2_predict    = classification,     occ2_truth      = occ2,     model           = model,     prompt_id       = prompt_id   ) }"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"defining-the-prompt-and-model-grid","dir":"Articles","previous_headings":"Optimizing and Testing Classifiers","what":"Defining the Prompt and Model Grid","title":"Classifying Texts with tidyllm","text":"define set prompts models want test. allow us apply classifier across different configurations compare results. ’s prompts models set : Prompts: Prompt 1: detailed prompt, asking model classify occupations warning make guesses. Prompt 2: Explicitly ask handle invalid responses returning special code (99) input resemble valid occupation. Prompt 3: shorter, concise version test whether model performs similarly less detailed instructions. Models: Llama3.2:3B: opensource large language model just 3 billion parameters fast, run locally via ollama() Gemma2:9B: Another candidate model, performs well classification tasks, double size Lama3.2 therefore somewhat slower. set grid combining prompts models. expand_grid() function tidyverse useful tool create every possible combination prompts models, use evaluate classifier: run classification across entire grid, use pmap_dfr() purrr package, allows us iterate multiple arguments simultaneously. combination occupation response, prompt, model passed classify_occupation() function, results concatenated single tibble: ⚠️ Note: Running extensive classification grid like , especially large datasets slow models, can take significant amount time. Therefore, ’s often reasonable save intermediate results periodically, don’t lose progress something goes wrong (e.g., crash network issue). combining pwalk() save_rds(), can run combination grid independently store results incrementally. run grid get insights well models prompts work train data. can experiment different models parameters much want see works.","code":"prompts <- tibble(prompt =           c( #Original prompt             'Classify this occupation response from a survey: {occupation}             Pick one of the following numerical codes from this list.              Respond only with the code!             {numerical_code_list}',             #Explicit instruction to avoid classifying something wrong            'Classify this occupation response from a survey: {occupation}             Pick one of the following numerical codes from this list.              Respond only with the code!             {numerical_code_list}                         If this does not look like a valid occupation response reply with              just 99            ',            #Shorter prompt            'Classify this occupation: {occupation}.             Respond only with one of the following codes:             {numerical_code_list}'          ),          prompt_id = 1:3)   grid <- expand_grid(train_data,                      prompts,                      model = c(\"llama3.2\", \"gemma2\")) |>   arrange(model) %>% # Arrange by model so ollama does not reload them often   rename(occupation = occupation_open) |>   rowwise() |>  # Glue together prompts and occupation row-by-row   mutate(prompt = glue(prompt)) |>   ungroup() |> # Ungroup after the rowwise operation   select(model,occupation,occ2,prompt_id,prompt)  nrow(grid) ## [1] 1854 grid_results <- grid |>   pmap_dfr(classify_occupation_grid,            provider = ollama) ## Classifying: gemma2 - 1 - Computer network technician ## Classifying: gemma2 - 2 - Computer network technician ## Classifying: gemma2 - 3 - Computer network technician ## Classifying: ..."},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"accuracy-estimates","dir":"Articles","previous_headings":"Optimizing and Testing Classifiers","what":"Accuracy estimates","title":"Classifying Texts with tidyllm","text":"create overview prediction accuracy overview use yardstick package. functions yardstick, also need encode ground truth model predictions factors: First just just calculate plot model accuracy:  main insights classification experiment: Gemma2 consistently outperforms llama3.2, even gemma2’s top performance Prompt 1 (79.1) falls far short Claude Sonnet’s 98% accuracy initial run. simplified prompt clearly introduces challenges, especially smaller models like llama3.2, shows particularly poor performance (low 20.2% Prompt 3). suggests models might generalize well slightly varied less explicit prompts, whereas Claude able handle variations far greater ease. Let’s look confusion matrix top gemma2 performance see whether specific occupation category causes problems : lot miss-classifications management occupations office services, missing occupations classified well, classified occupations. nothing seems like immediate easy fix. Yet, tricks try go beyond simple one-shot classification. common idea use multi-shot classifications (give model classification examples prompt, even guide output letting complete conversations answers right way). function df_llm_message() let’s easily built message histories put words mouth model. ways can help increase accuracy. popular one first let model reason detail best candidates classification build chain thought gives final answer.","code":"library(yardstick) ##  ## Attaching package: 'yardstick' ## The following object is masked from 'package:readr': ##  ##     spec  gr_factors <- grid_results |>   mutate(across(starts_with(\"occ2_\"),                 ~factor(.x,                         levels=c(occ_codes$occ2,97, 98,99),                         labels=c(occ_codes$occ_title,\"APIFAIL\",\"INVALID\",\"MISSING\")                 ))) accuracy <- gr_factors  |>   group_by(prompt_id, model) |>   accuracy(truth = occ2_truth, estimate = occ2_predict)  accuracy |>   ggplot(aes(x = as.factor(prompt_id), y = .estimate, fill = model)) +   geom_bar(stat = \"identity\", position = \"dodge\") +   labs(title = \"Accuracy by Prompt and Model\", x = \"Prompt ID\", y = \"Accuracy\", fill=\"\") +   theme_bw(22) +   scale_y_continuous(labels = scales::label_percent(),limits = c(0,1)) +   scale_fill_brewer(palette = \"Set1\") conf_mat <- gr_factors |>   filter(prompt_id == 1, model == \"gemma2\") |>   conf_mat(truth = occ2_truth, estimate = occ2_predict)  # Autoplot the confusion matrix autoplot(conf_mat, type = \"heatmap\") +   scale_fill_gradient(low = \"white\", high = \"#E41A1C\") +   ggtitle(\"Confusion Matrix for Model gemma2, Prompt 1\") +   theme_bw(22) +   theme(axis.text.x = element_text(angle = 45, hjust = 1)) ## Scale for fill is already present. ## Adding another scale for fill, which will replace the existing scale."},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"multistep-chain-of-thought-prompting","dir":"Articles","previous_headings":"Optimizing and Testing Classifiers","what":"Multistep Chain-of-Thought Prompting","title":"Classifying Texts with tidyllm","text":"need major change classification function, send two messages model. first one elicits reasoning step, second one asks final code, based answer first message. easily doable message-chaining abilities tidyllm modified chain thought prompting function reasoning output step: Note increases compute time gemma2 massively now produces multiple output . run produces roughly page reasoning, second question generates number, original prompting strategy just gave us one number. run-times increase strongly. Note: test complicated prompting strategies like realtime, another feature tidyllm comes handy. can stream back responses just like online chatbot interfaces first test run .stream-argument provider functions chat() immediately see whether made strange errors prompting. Therefore, pass stream arguement trough classify_occupation_cot() first test function prompting single inputs scale whole data. Let’s run function gemma2: work? turns case ! accuracy even worse without reasoning step! One interesting thing though model right classification one candidates reasoning step 94% cases. probably, still way get better final result gemma2: However, specific case already clear favorite final choice without even needing testing. already know 98% test performance Claude Sonnet 3.5, since manually checked . check prompt models test data needed. can just run entire sample claude_send_batch()","code":"classify_occupation_cot <- function(occupation,                                     occ2,                                     provider,                                     model,                                     stream = FALSE){   # Output what the model is currently doing to the console   glue(\"Classifying with CoT: {model} - {occupation}\\n\") |> cat(\"\\n\")      # Step 1: Ask the model to think through the problem   prompt_reasoning <- glue('     Think about which of the following occupation codes would best describe this occupation description from a survey respondent: \"{occupation}\"          {numerical_code_list}          Explain your reasoning for the 3 top candidate codes step by step. Then evaluate which seems best.   ')      reasoning_response <- tryCatch({     conversation <<- llm_message(prompt_reasoning) |>       chat(provider(.model = model, .temperature = 0, .stream=stream))          conversation |>       get_reply()   }, error = function(e){     conversation <<- llm_message(\"Please classify this occupation response: {occupation}\")     \"Error in reasoning step.\"   })      # Step 2: Ask the model to provide the final answer   prompt_final <- glue('     Based on your reasoning, which code do you pick? Answer only with a numerical code!    ')      final_response <- tryCatch({     conversation |>     llm_message(prompt_final) |>       chat(provider(.model = model, .temperature = 0, .stream=stream)) |>       get_reply() |>       str_squish()   }, error = function(e){     \"97\"   })      # Validate the model's final response   valid_codes <- as.character(occ_codes$occ2)      classification <- if (final_response %in% valid_codes) {     as.integer(final_response)   } else {     98L  # Return 98 for invalid responses   }      # Return a tibble containing the original occupation description and classification result   tibble(     occupation_open = occupation,     occ2_predict    = classification,     occ2_truth      = occ2,     model           = glue(\"{model}_cot\"),     reasoning       = reasoning_response,     final_response  = final_response   ) } results_cot <- grid |>   filter(model==\"gemma2\", prompt_id ==1) |>   select(-prompt,-prompt_id) |>   pmap_dfr(classify_occupation_cot,api_function = ollama, stream=FALSE) ## Classifying with CoT: gemma2 - 1 - Computer network technician ## Classifying with CoT: gemma2 - 2 - Educational support ## Classifying with CoT: gemma2 - 3 - Fine Carpentry ## Classifying with CoT: ... results_cot |>   mutate(across(starts_with(\"occ2_\"),                 ~factor(.x,                         levels=c(occ_codes$occ2,97, 98,99),                         labels=c(occ_codes$occ_title,\"APIFAIL\",\"INVALID\",\"MISSING\")                 )))  |>   accuracy(truth = occ2_truth, estimate = occ2_predict) ## # A tibble: 1 × 3 ##   .metric  .estimator .estimate ##   <chr>    <chr>          <dbl> ## 1 accuracy multiclass     0.754 results_cot |>   rowwise() |>   mutate(reasoning_classifications = str_extract_all(reasoning,\"\\\\d{2}\"),          reasoning_classifications = list(map_int(reasoning_classifications, as.integer)),          right_in_reasoning = occ2_truth %in% reasoning_classifications          ) |>   ungroup() |>   count(right_in_reasoning) |>   mutate(pct=n/sum(n)) ## # A tibble: 2 × 3 ##   right_in_reasoning     n    pct ##   <lgl>              <int>  <dbl> ## 1 FALSE                 19 0.0615 ## 2 TRUE                 290 0.939"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Classifying Texts with tidyllm","text":"article, ’ve demonstrated tidyllm can effectively used tackle complex text classification tasks. walking typical process step--step, sample selection prompt design model testing optimization, ’ve highlighted flexibility package dealing real-world data.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_embed.html","id":"semantic-search-in-economics-paper-abstracts","dir":"Articles","previous_headings":"","what":"Semantic Search in Economics Paper Abstracts","title":"Embedding Models in tidyllm","text":"demonstrate embeddings action, ’ll implement semantic search dataset 22,960 economics paper abstracts published 2010 2024. Instead relying keyword matching, ’ll use embeddings find papers similar topics based underlying meaning. Let’s start loading exploring data: dataset ready, ’ll now use tidyllm generate embeddings perform semantic search find papers similar target_abstract Paper commuting expansion high-speed rail Heuerman Schmieder (2018). task, ’ll use mxbai-embed-large model. model, developed Mixedbread.ai, achieves state---art performance among efficiently sized models outperforms closed-source models like OpenAI’s text-embedding-ada-002. Ollama installed, can download using ollama_download_model(\"mxbai-embed-large\"). important choose embedding model carefully upfront, model produces unique numerical representations text interchangeable models. Alternatively, embedding APIs also available mistral(), gemini(), openai() (well azure_openai()).","code":"library(tidyverse) library(tidyllm) library(here)  abstracts <- read_rds(\"abstracts_data.rds\") #The structure of our file: print(abstracts,width = 60) ## # A tibble: 22,960 × 8 ##    year  journal  authors volume firstpage lastpage abstract ##    <chr> <chr>    <chr>   <chr>  <chr>     <chr>    <chr>    ##  1 2024  Journal… Bauer,… 22     2075      2107     This pa… ##  2 2019  The Rev… Karlan… 86     1704      1746     We use … ##  3 2022  Journal… Corset… 20     513       548      We stud… ##  4 2018  The Rev… Anagol… 85     1971      2004     We stud… ##  5 2024  America… Thores… 16     447       79       This pa… ##  6 2024  Journal… Ren, Y… 238    NA        NA       The rap… ##  7 2013  The Rev… Adhvar… 95     725       740      A key p… ##  8 2022  Econome… Brooks… 90     2187      2214     If expe… ##  9 2011  Health … Fletch… 20     553       570      We exam… ## 10 2010  Journal… Rohwed… 24     119       38       Early r… ## # ℹ 22,950 more rows ## # ℹ 1 more variable: pdf_link <chr>  target_abstract <-  \"We use the expansion of the high-speed rail network in Germany  as a natural experiment to examine the causal effect of reductions in commuting time  betweenregions on the commuting decisions of workers and their choices regarding  where tolive and where to work. We exploit three key features in this setting:i) investmentin high-speed rail has, in some cases dramatically, reduced travel times between regions,ii) several small towns were connected to the high-speed rail network onlyfor political reasons, and iii) high-speed trains have left the transportation of goodsunaffected. Combining novel information on train schedules and the opening ofhigh-speed rail stations with panel data on all workers in Germany, we show that a reduction in travel time by one percent raises the number of commuters betweenregions by 0.25 percent. This effect is mainly driven by workers changing jobs to smaller cities while keeping their place of residence in larger ones. Our findings support the notion that benefits from infrastructure investments accrue in particular to peripheral regions, which gain access to a large pool of qualified workers with a preference for urban life. We find that the introduction of high-speed trains led to a modal shift towards rail transportation in particular on medium distances between 150 and 400 kilometers.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_embed.html","id":"step-1-computing-embeddings-for-one-abstract","dir":"Articles","previous_headings":"Semantic Search in Economics Paper Abstracts","what":"Step 1: Computing Embeddings for one abstract","title":"Embedding Models in tidyllm","text":"compute embedding target abstract use embed() function ollama() provider-function: embed() function returns tibble two columns: - input: original text provided embedding. - embeddings: list column containing numerical vector representation (embedding) input text. case single input embeddings column contains 1,024-dimenstional vector input:","code":"target_tbl <- target_abstract |>   embed(ollama,.model=\"mxbai-embed-large:latest\")  target_tbl str(target_tbl) ## # A tibble: 1 × 2 ##   input                                           embeddings ##   <chr>                                           <list>     ## 1 We use the expansion of the high-speed rail ne… <dbl> str(target_tbl$embeddings) ## List of 1 ##  $ : num [1:1024] -0.838 0.669 0.202 -0.686 -0.985 ..."},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_embed.html","id":"step-2-computing-embeddings-for-the-entire-abstract-corpus","dir":"Articles","previous_headings":"Semantic Search in Economics Paper Abstracts","what":"Step 2: Computing Embeddings for the entire abstract corpus","title":"Embedding Models in tidyllm","text":"working large corpus like 22,960 abstracts, embedding entries single pass using embed() impractical often leads errors. commercial APIs, typically strict limits number inputs allowed per request (usually caps 50 100 inputs). local APIs, resource constraints memory processing power impose similar restrictions. efficiently handle , batch data, processing manageable number abstracts time. generate_abstract_embeddings() function takes vector abstracts input divides manageable batches 200. batch, uses embed() function compute embeddings via ollama(). Progress logged console keep track batch completion provide clear view process. Since long-running processes prone interruptions, network timeouts unexpected system errors, saves results disk .rds files (consider arrow::write_parquet() database really big workloads). MacBook Pro M1 Pro processor, function completes embedding entire dataset approximately 25 minutes writes 207 MB data disk. time may vary depending system specifications batch size. compare multiple target abstracts entire collection, course need embed . function finished, need load computed embeddings:","code":"#Our batches embedding function generate_abstract_embeddings <- function(abstracts){      #Preapre abstract batches   embedding_batches <- tibble(abstract = abstracts) |>     group_by(batch = floor(1:n() / 200)+1) |>     group_split()      #Work with batches of 200 abstracts   n_batches <- length(embedding_batches)   glue(\"Processing {n_batches} batches of 200 abstracts\") |> cat(\"\\n\")      #Embed the batches via ollama mxbai-embed-large    embedding_batches %>%     walk(~{       batch_number <- pull(.x,batch) |> unique()        glue(\"Generate Text Embeddings for Abstract Batch: {batch_number}/{n_batches}\") |> cat(\"\\n\")              emb_matrix <- .x$abstract %>%         embed(ollama,.model=\"mxbai-embed-large:latest\") |>         write_rds(here(\"embedded_asbtracts\",paste0(batch_number,\".rds\")))     }) }  #Run the function over all abstracts abstracts |>   pull(abstract) |>   generate_abstract_embeddings() ## Processing 115 batches of 200 abstracts ## Generate Text Embeddings for Abstract Batch: 1/115 ## Generate Text Embeddings for Abstract Batch: 2/115  ## Generate Text Embeddings for Abstract Batch: 3/115 ## ... embedded_asbtracts <- here(\"embedded_asbtracts\") |>   dir() |>   map_dfr(~read_rds(here(\"embedded_asbtracts\",.x)))"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_embed.html","id":"step-3-performing-the-semantic-search","dir":"Articles","previous_headings":"Semantic Search in Economics Paper Abstracts","what":"Step 3: Performing the Semantic Search","title":"Embedding Models in tidyllm","text":"embeddings precomputed, can now perform semantic search find abstracts similar target. search use cosine similarity compare embedding vectors. Cosine similarity measures similarity two vectors calculating cosine angle . ranges -1 (opposite directions) 1 (identical directions). formula : cosine_similarity(𝐚,𝐛)=∑=1naibi∑=1nai2⋅∑=1nbi2 \\text{cosine_similarity}(\\mathbf{}, \\mathbf{b}) = \\frac{\\sum_{=1}^n a_i b_i}{\\sqrt{\\sum_{=1}^n a_i^2} \\cdot \\sqrt{\\sum_{=1}^n b_i^2}} : ∑aibi\\sum a_i b_i dot product. ∑ai2\\sqrt{\\sum a_i^2} ∑bi2\\sqrt{\\sum b_i^2} magnitudes vectors. Vectors embedding space represent semantic meanings texts. space: Cosine similarity focuses direction rather magnitude. Two vectors pointing similar directions (small angle) cosine similarity close 1. Vectors 90 degrees (orthogonal, semantic overlap) cosine similarity 0. Vectors pointing opposite directions (large angle, entirely dissimilar) cosine similarity -1. model use embedding vector represents point high-dimensional space 1,024 dimensions. Even though dimensions spatially interpretable like 2D 3D, underlying principle still holds: cosine similarity measures much two vectors “lean” direction. two vectors cosine similarity 0.25, means angle relatively small, implying share 25% directional alignment. compute cosine similarity two vectors express simple function: apply function compute cosine similarity abstracts corpus target paper find 10 similar abstracts: top ten similar articles corpus based search : Top 10 Similar Abstracts Unsurprisingly, top result target abstract , presented slight formatting differences. However, remaining top results strongly align target’s thematic focus economic social impacts transportation (specifically high speed reail) infrastructure. second result discusses economic effects German High-Speed Rail (HSR), third fourth focus Japan’s Shinkansen China’s HSR network, respectively. findings highlight model’s capability identify semantically rich connections across diverse contexts, illustrating ability capture complex thematic overlaps.","code":"cosine_similarity <- function(a, b) {     sum(a * b) / (sqrt(sum(a^2)) * sqrt(sum(b^2))) } top10_similar <- embedded_asbtracts %>%   mutate(cosine_sim = map_dbl(embeddings,                                ~cosine_similarity(.x,                                                   target_tbl$embeddings[[1]])))|>   arrange(desc(cosine_sim)) |>   slice(1:10) |>    rename(abstract=input) |>   left_join(abstracts |>               select(year,authors,journal,abstract), by=\"abstract\") |>   select(-embeddings)"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_embed.html","id":"outlook-clustering-and-beyond","dir":"Articles","previous_headings":"Semantic Search in Economics Paper Abstracts","what":"Outlook: Clustering and Beyond","title":"Embedding Models in tidyllm","text":"article focused semantic search, embeddings open door wide range advanced analytical techniques. potential use-cases embeddings: Clustering Topic Discovery: Embeddings can leveraged unsupervised learning methods like K-means hierarchical clustering automatically group text topics themes. approach particularly beneficial exploratory research, enabling users uncover hidden patterns structures within large corpora without predefined labels. Dimensionality Reduction Visualization: High-dimensional embedding vectors often need simplification human interpretation. Techniques like Principal Component Analysis (PCA), t-SNE, UMAP allow us project embeddings lower-dimensional spaces (e.g., 2D 3D). visualizations can reveal clusters, trends, outliers, providing insights glance. Retrieval augmented generation (RAG): Embeddings play crucial role retrieval-augmented generation, technique enhances large language models (LLMs). workflow, embedding search retrieves semantically similar documents corpus, appended LLM’s prompt. process helps model generate contextually rich accurate responses, especially knowledge-intensive tasks. Supporting Workflows Qualitative Research: Embeddings can also transform workflows qualitative research, discussed paper Kugler et al. (2023). integration Natural Language Processing (NLP) tools enables researchers automate coding steps traditionally require manual effort. Specifically, embeddings can assist categorizing text data according predefined themes, making research workflows efficient transparent. models bring significant advantages, challenges remain. study highlights --shelf language models often struggle discern implicit references closely related topics effectively human researchers. However, modern embedding models ones used study might help deal challenges.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_video.html","id":"sending-an-example-video-to-gemini","dir":"Articles","previous_headings":"","what":"Sending an example video to Gemini","title":"Video and Audio Data with the Gemini API","text":"segment PC mp4-file can simply use gemini_upload_file() make available gemini() requests. use gemini_upload_file(), returns tibble containing detailed metadata uploaded file. output includes several key columns: name (unique identifier file server), display_name (original file name), mime_type (file’s media type, e.g., video/mp4), size_bytes (file size bytes), create_time (timestamp indicating file uploaded), uri (URL pointing file’s location Google’s servers) state (indicating current processing status file). range metadata, primarily need name, can use .fileid argument gemini(): file uploaded can reuse different requests. Note Gemini also supports tidyllm_schema(), allowing get structured responses file types can use Gemini: gemini_list_files() creates tibble overview files uploaded Gemni. can use check files currently available use Gemini. example, can now ask gemini() audio file uploaded earlier : completed working file, can delete gemini_delete_file()","code":"library(tidyverse) library(tidyllm) upload_info <- gemini_upload_file(\"the_thinking_machine_1960s.mp4\") upload_info str(upload_info) ## # A tibble: 1 × 7 ##   name   display_name mime_type size_bytes create_time uri   ##   <chr>  <chr>        <chr>          <dbl> <chr>       <chr> ## 1 files… the_thinkin… video/mp4    6520447 2024-11-18… http… ## # ℹ 1 more variable: state <chr> ## tibble [1 × 7] (S3: tbl_df/tbl/data.frame) ##  $ name        : chr \"files/wutpmcv9jve9\" ##  $ display_name: chr \"the_thinking_machine_1960s.mp4\" ##  $ mime_type   : chr \"video/mp4\" ##  $ size_bytes  : num 6520447 ##  $ create_time : chr \"2024-11-18T09:40:29.603980Z\" ##  $ uri         : chr \"https://generativelanguage.googleapis.com/v1beta/files/wutpmcv9jve9\" ##  $ state       : chr \"PROCESSING\" llm_message(\"Give me a detailed summary of this video\") |>   chat(gemini(.fileid = upload_info$name)) ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## Give me a 200 word summary of this video ## -------------------------------------------------------------- ## assistant: ## Here is a summary of the video. ##  ## This 1961 black and white Paramount News clip explores ## the burgeoning field of artificial intelligence. The host ## interviews Professor Jerome B. Wiesner, director of MIT's ## Research Laboratory of Electronics. Wiesner discusses the ## capabilities of computers, noting that while their abilities ## were previously limited, he suspects they'll be able to ## \"think\" within a few years. The segment shows a computer ## playing checkers against a human opponent. Other experts, ## including Oliver Selfridge and Claude Shannon, offer their ## perspectives on whether machines can truly think and the ## potential implications for the future, particularly in ## language translation. One expert predicts that within ## 10–15 years, machines will be performing tasks previously ## considered the realm of human intelligence. The film ends ## by showing a computer that translates Russian to English. ## Despite this success, one expert notes that computers will ## not replace translators of poetry and novels. ## -------------------------------------------------------------- structured_request <- llm_message(\"Extract some details about this video\") |>   chat(gemini(.fileid = upload_info$name),        .json_schema = tidyllm_schema(name=\"videoschema\",                                      Persons = \"character[]\",                                      ScientistQuotes = \"character[]\",                                      Topics = \"character[]\",                                      runtime = \"numeric\"))  structured_request |>   get_reply_data() ## $Persons ## [1] \"Professor Jerome B. Wiesner\" \"Oliver G. Selfridge\"         ## [3] \"Claude Shannon\"              ##  ## $ScientistQuotes ## [1] \"Well, that's a very hard question to answer. If you'd asked me that question just a few years ago I'd have said it was very far fetched. And today I just have to admit I don't really know. I suspect that if you'd come back in four or five years I'll say sure they really do think.\"                                                                                                                                               ## [2] \"I'm convinced that machines can and will think. I don't mean that machines will behave like men. I don't think for a very long time we're going to have a difficult problem distinguishing a man from a robot. And I don't think my daughter will ever marry a computer. But I think the computers will be doing the things that men do when we say they're thinking. I am convinced that machines can and will think in our lifetime.\" ## [3] \"I confidently expect that within a matter of 10 or 15 years, something will emerge from the laboratories which is not too far from the robot of science fiction fame\"                                                                                                                                                                                                                                                                   ##  ## $Topics ## [1] \"Artificial Intelligence\"        \"Computer\"                       ## [3] \"Thinking Machine\"               \"Machine Translation\"            ## [5] \"Russian to English Translation\" \"Cold War\"                       ##  ## $runtime ## [1] 172 gemini_files <- gemini_list_files() gemini_files ## # A tibble: 3 × 10 ##   name  display_name mime_type size_bytes create_time update_time ##   <chr> <chr>        <chr>          <dbl> <chr>       <chr>       ## 1 file… example.mp3  audio/mp3    2458836 2024-11-15… 2024-11-15… ## 2 file… the_thinkin… video/mp4    6520447 2024-11-18… 2024-11-18… ## 3 file… example_akt… applicat…     193680 2024-11-17… 2024-11-17… ## # ℹ 4 more variables: expiration_time <chr>, sha256_hash <chr>, ## #   uri <chr>, state <chr> llm_message(\"What's in this audio file. Describe it in 100 words.\") |>   gemini_chat(.fileid=gemini_files$name[1]) ## Message History: ## system: ## You are a helpful assistant ## -------------------------------------------------------------- ## user: ## What's in this audio file. Describe it in 100 words. ## -------------------------------------------------------------- ## assistant: ## This is an audio recording of an interview with Robert ## Bosch. He discusses his early life, his career path from ## apprentice to founder of his renowned company, and his ## philosophy on business. Bosch recounts his decision to ## become a precision mechanic, his travels to America and ## England, and the development of his famous magneto ignition ## system. He emphasizes his principles of high-quality work, ## fair treatment of employees, and the importance of trust and ## reliability in business dealings. The interview concludes ## with well wishes. ## -------------------------------------------------------------- gemini_files$name[1] |>   gemini_delete_file() ## File files/nzq2zw9u30y8 has been successfully deleted."},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_video.html","id":"supported-file-types","dir":"Articles","previous_headings":"","what":"Supported file types","title":"Video and Audio Data with the Gemini API","text":"Google Gemini API supports wide range file formats, enabling seamless multimodal workflows. documents, handles PDFs, plain text, HTML, CSS, Markdown, CSV-tables, XML, RTF. Uploading documents gemini_upload_file() useful, since standard llm_message() function .pdf argument designed extract textual content PDFs. contrast, Google Gemini API enhances supporting multimodal PDFs, including contain images image-(scanned documents). allows users work textual visual data interactions. example, can upload old scan paper contains extractable text ask gemini extract references: Image formats gemini_upload_file() include PNG, JPEG, WEBP, HEIC, HEIF, supported video formats cover MP4, MPEG, MOV, AVI, FLV, MPG, WEBM, WMV, 3GPP. audio, API works WAV, MP3, AIFF, AAC, OGG Vorbis, FLAC. Code files JavaScript Python also supported.","code":"neal1995 <- gemini_upload_file(\"1995_Neal_Industry_Specific.pdf\")              bib <- llm_message(\"Extract all the references from this paper in the specified format\") |>                      chat(gemini(.fileid=neal1995$name),                  .json_schema = tidyllm_schema(name=\"references\",                                                APAreferences =\"character[]\")             )  references <- bib |> get_reply_data() references[1:5] ## [1] \"Addison, John, and Portugal, Pedro. \\\"Job Displacement, Relative Wage Changes, and Duration of Unemployment.\\\" *Journal of Labor Economics* 7 (July 1989): 281–302.\" ## [2] \"Altonji, Joseph, and Shakotko, Robert. \\\"Do Wages Rise with Seniority?\\\" *Review of Economic Studies* 54 (July 1987): 437–59.\"                                       ## [3] \"Becker, Gary. *Human Capital*. New York: Columbia University Press, 1975.\"                                                                                           ## [4] \"Carrington, William. \\\"Wage Losses for Displaced Workers: Is It Really the Firm That Matters?\\\" *Journal of Human Resources* 28 (Summer 1993): 435–62.\"              ## [5] \"Carrington, William, and Zaman, Asad. \\\"Interindustry Variation in the Costs of Job Displacement.\\\" *Journal of Labor Economics* 12 (April 1994): 243–76.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_video.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Video and Audio Data with the Gemini API","text":"ability work diverse media types Gemini API opens wide range applications across scientific disciplines. linguistics humanities research, text, audio, video can used analyze language patterns, conduct speech recognition, study historical footage. Social scientists can process interview recordings, video observations, might interesting application across whole range fields. integrating text, audio, video, images, code unified workflow, using Gemini API tidyllm offers many possibilities available standard multimodal tools llm_message().","code":""},{"path":"https://edubruell.github.io/tidyllm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Eduard Brüll. Author, maintainer. Jia Zhang. Contributor.","code":""},{"path":"https://edubruell.github.io/tidyllm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Brüll E (2025). tidyllm: Tidy Integration Large Language Models. R package version 0.3.2, https://edubruell.github.io/tidyllm/.","code":"@Manual{,   title = {tidyllm: Tidy Integration of Large Language Models},   author = {Eduard Brüll},   year = {2025},   note = {R package version 0.3.2},   url = {https://edubruell.github.io/tidyllm/}, }"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"tidyllm-","dir":"","previous_headings":"","what":"Tidy Integration of Large Language Models","title":"Tidy Integration of Large Language Models","text":"tidyllm R package designed access various large language model APIs, including Anthropic Claude, OpenAI,Google Gemini, Perplexity,Groq, Mistral, local models via Ollama OpenAI-compatible APIs. Built simplicity functionality, helps generate text, analyze media, integrate model feedback data workflows ease.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Tidy Integration of Large Language Models","text":"Multiple Model Support: Seamlessly switch various model providers using best offer. Media Handling: Extract process text PDFs capture console outputs messaging. Upload imagefiles last plotpane multimodal models. Gemini API even video audio inputs supported. Interactive Messaging History: Manage ongoing conversation models, maintaining structured history messages media interactions, automatically formatted API Batch processing: Efficiently handle large workloads Anthropic, OpenAI Mistral batch processing APIs, reducing costs 50%. Tidy Workflow: Use R’s functional programming features side-effect-free, pipeline-oriented operation style.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tidy Integration of Large Language Models","text":"install tidyllm CRAN, use: development version GitHub:","code":"install.packages(\"tidyllm\") # Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") } devtools::install_github(\"edubruell/tidyllm\")"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"basic-example","dir":"","previous_headings":"","what":"Basic Example","title":"Tidy Integration of Large Language Models","text":"’s quick example using tidyllm describe image using Claude model follow local open-source models: examples advanced usage, check Get Started vignette. Please note: use tidyllm, need either installation ollama active API key one supported providers (e.g., Claude, ChatGPT). See Get Started vignette setup instructions.","code":"library(\"tidyllm\")  # Describe an image with  claude conversation <- llm_message(\"Describe this image\",                                .imagefile = here(\"image.png\")) |>   chat(claude())  # Use the description to query further with groq conversation |>   llm_message(\"Based on the previous description,   what could the research in the figure be about?\") |>   chat(ollama(.model = \"gemma2\"))"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"interface-change-in-last-release","dir":"","previous_headings":"","what":"Interface-change in last release","title":"Tidy Integration of Large Language Models","text":"last CRAN release tidyllm, introduces major interface change provide intuitive user experience. Previously, provider-specific functions like claude(), openai(), others directly used chat-based workflows. specified API-provider performed chat-interaction. Now, functions primarily serve provider configuration general verbs like chat(),embed() send_batch(). combination general verb provider always route requests provider-specific function like openai_chat(). Read Changelog package vignette information. backward compatibility, old use functions like openai() claude() directly chat requests still works now issues deprecation warnings. recommended either use verb-based interface: use verbose provider-specific functions directly:","code":"llm_message(\"Hallo\") |> chat(openai(.model=\"gpt-4o\")) llm_message(\"Hallo\") |> openai_chat(.model=\"gpt-4o\")"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn More","title":"Tidy Integration of Large Language Models","text":"detailed instructions advanced features, see: Get Started tidyllm Changelog Documentation Classifying Texts tidyllm Structured Question Answering PDFs Using Embedding Models Semantic Search Video Audio Data Gemini API","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"similar-packages","dir":"","previous_headings":"","what":"Similar packages","title":"Tidy Integration of Large Language Models","text":"similar R packages working LLMs: ellmer especially great asynchronous workflows, chatbots Shiny advanced tool-calling capabilities. schema functions offer robust support complex structured data extraction, making great choice applications require highly interactive structured LLM interactions. ellmer’s feature set overlaps tidyllm areas, interface design philosophy different. rollama specifically designed support Ollama API, enabling seamless interaction local LLM models. key strength rollama lies specialized Ollama API functionalities, copy create currently available tidyllm. features make rollama particularly suited workflows requiring model management deployment within Ollama ecosystem.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Tidy Integration of Large Language Models","text":"welcome contributions! Feel free open issues submit pull requests GitHub.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Tidy Integration of Large Language Models","text":"project licensed MIT License - see LICENSE file details.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":null,"dir":"Reference","previous_headings":"","what":"Large Language Model Message Class — LLMMessage","title":"Large Language Model Message Class — LLMMessage","text":"LLMMessage S7 class managing conversation history intended use large language models (LLMs). Please use llm_message()create modify LLMMessage objects.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage(message_history = list(), system_prompt = character(0))"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"message_history list containing messages. message named list keys like role, content, media, etc. system_prompt character string representing default system prompt used conversation.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Large Language Model Message Class — LLMMessage","text":"LLMMessage class includes following features: Stores message history structured format. Supports attaching media metadata messages. Provides generics like add_message(), has_image(), remove_message() interaction. Enables API-specific formatting to_api_format() generic. message_history: list containing messages. message named list keys like role, content, media, etc. system_prompt: character string representing default system prompt used conversation.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Azure OpenAI Endpoint Provider Function — azure_openai","title":"Azure OpenAI Endpoint Provider Function — azure_openai","text":"azure_openai() function acts interface interacting Azure OpenAI API main tidyllm verbs.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Azure OpenAI Endpoint Provider Function — azure_openai","text":"","code":"azure_openai(..., .called_from = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Azure OpenAI Endpoint Provider Function — azure_openai","text":"... Parameters passed Azure OpenAI API specific function, model configuration, input text, API-specific options. .called_from internal argument specifies action (e.g., chat) function invoked . argument automatically managed modified user.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Azure OpenAI Endpoint Provider Function — azure_openai","text":"result requested action, depending specific function invoked (currently, updated LLMMessage object azure_openai_chat()).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Azure OpenAI Endpoint Provider Function — azure_openai","text":"azure_openai() currently routes messages azure_openai_chat() used chat(). send_batch(). dynamically routes requests OpenAI-specific functions like azure_openai_chat() azure_openai_embedding() based context call.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai_chat","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai_chat","text":"function sends message history Azure OpenAI Chat Completions API returns assistant's reply. function work progress fully tested","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai_chat","text":"","code":"azure_openai_chat(   .llm,   .endpoint_url = Sys.getenv(\"AZURE_ENDPOINT_URL\"),   .deployment = \"gpt-4o-mini\",   .api_version = \"2024-08-01-preview\",   .max_completion_tokens = NULL,   .frequency_penalty = NULL,   .logit_bias = NULL,   .logprobs = FALSE,   .top_logprobs = NULL,   .presence_penalty = NULL,   .seed = NULL,   .stop = NULL,   .stream = FALSE,   .temperature = NULL,   .top_p = NULL,   .timeout = 60,   .verbose = FALSE,   .json_schema = NULL,   .dry_run = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai_chat","text":".llm LLMMessage object containing conversation history. .endpoint_url Base URL API (default:  Sys.getenv(\"AZURE_ENDPOINT_URL\")). .deployment identifier model deployed (default: \"gpt-4o-mini\"). .api_version version API deployed (default: \"2024-10-01-preview\") .max_completion_tokens upper bound number tokens can generated completion, including visible output tokens reasoning tokens. .frequency_penalty Number -2.0 2.0. Positive values penalize new tokens based existing frequency text far. .logit_bias named list modifying likelihood specified tokens appearing completion. .logprobs Whether return log probabilities output tokens (default: FALSE). .top_logprobs integer 0 20 specifying number likely tokens return token position. .presence_penalty Number -2.0 2.0. Positive values penalize new tokens based whether appear text far. .seed specified, system make best effort sample deterministically. .stop 4 sequences API stop generating tokens. .stream set TRUE, answer streamed console comes (default: FALSE). .temperature sampling temperature use, 0 2. Higher values make output random. .top_p alternative sampling temperature, called nucleus sampling. .timeout Request timeout seconds (default: 60). .verbose additional information shown API call (default: FALSE). .json_schema JSON schema object R list enforce output structure (defined precedence JSON mode). .dry_run TRUE, perform dry run return request object (default: FALSE). .max_tries Maximum retries perform request","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai_chat","text":"new LLMMessage object containing original messages plus assistant's response.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai_chat","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage msg <- llm_message(\"What is R programming?\") result <- azure_openai_chat(msg)  # With custom parameters result2 <- azure_openai_chat(msg,                   .deployment = \"gpt-4o-mini\",                  .temperature = 0.7,                   .max_tokens = 1000) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using OpenAI API on Azure — azure_openai_embedding","title":"Generate Embeddings Using OpenAI API on Azure — azure_openai_embedding","text":"Generate Embeddings Using OpenAI API Azure","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using OpenAI API on Azure — azure_openai_embedding","text":"","code":"azure_openai_embedding(   .input,   .deployment = \"text-embedding-3-small\",   .endpoint_url = Sys.getenv(\"AZURE_ENDPOINT_URL\"),   .api_version = \"2023-05-15\",   .truncate = TRUE,   .timeout = 120,   .dry_run = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using OpenAI API on Azure — azure_openai_embedding","text":".input character vector texts embed LLMMesssageobject .deployment embedding model identifier (default: \"text-embedding-3-small\"). .endpoint_url Base URL API (default:  Sys.getenv(\"AZURE_ENDPOINT_URL\")). .api_version API-Version othe Azure OpenAI API used (default: \"2023-05-15\") .truncate Whether truncate inputs fit model's context length (default: TRUE). .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object. .max_tries Maximum retry attempts requests (default: 3).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using OpenAI API on Azure — azure_openai_embedding","text":"tibble two columns: input embeddings. input column contains texts sent embed, embeddings column list column row contains embedding vector sent input.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/cancel_openai_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel an In-Progress OpenAI Batch — cancel_openai_batch","title":"Cancel an In-Progress OpenAI Batch — cancel_openai_batch","text":"function cancels -progress batch created OpenAI API. batch moved \"cancelling\" state , eventually, \"cancelled.\"","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/cancel_openai_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel an In-Progress OpenAI Batch — cancel_openai_batch","text":"","code":"cancel_openai_batch(.batch_id, .dry_run = FALSE, .max_tries = 3, .timeout = 60)"},{"path":"https://edubruell.github.io/tidyllm/reference/cancel_openai_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel an In-Progress OpenAI Batch — cancel_openai_batch","text":".batch_id Character; unique identifier batch cancel. .dry_run Logical; TRUE, returns constructed request without executing (default: FALSE). .max_tries Integer; maximum number retries request fails (default: 3). .timeout Integer; request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/cancel_openai_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cancel an In-Progress OpenAI Batch — cancel_openai_batch","text":"list containing response OpenAI API cancellation status.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a Language Model — chat","title":"Chat with a Language Model — chat","text":"chat() function sends message language model via specified provider returns response. routes provided LLMMessage object appropriate provider-specific chat function, allowing specification common arguments applicable across different providers.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a Language Model — chat","text":"","code":"chat(   .llm,   .provider = getOption(\"tidyllm_chat_default\"),   .dry_run = NULL,   .stream = NULL,   .temperature = NULL,   .timeout = NULL,   .top_p = NULL,   .max_tries = NULL,   .model = NULL,   .verbose = NULL,   .json_schema = NULL,   .tools = NULL,   .seed = NULL,   .stop = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a Language Model — chat","text":".llm LLMMessage object containing message conversation history send language model. .provider function function call specifying language model provider additional parameters. call provider function like openai(), claude(), etc. can also set default provider function via tidyllm_chat_default option. .dry_run Logical; TRUE, simulates request without sending provider. Useful testing. .stream Logical; TRUE, streams response provider real-time. .temperature Numeric; controls randomness model's output (0 = deterministic). .timeout Numeric; maximum time (seconds) wait response. .top_p Numeric; nucleus sampling parameter, limits sampling top cumulative probability p. .max_tries Integer; maximum number retries failed requests. .model Character; model identifier use (e.g., \"gpt-4\"). .verbose Logical; TRUE, prints additional information request response. .json_schema List; JSON schema object R list enforce output structure .seed Integer; sets random seed reproducibility. .stop Character vector; specifies sequences model stop generating tokens. .frequency_penalty Numeric; adjusts likelihood repeating tokens (positive values decrease repetition). .presence_penalty Numeric; adjusts likelihood introducing new tokens (positive values encourage novelty).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a Language Model — chat","text":"updated LLMMessage object containing response language model.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Chat with a Language Model — chat","text":"chat() function provides unified interface interacting different language model providers. Common arguments .temperature, .model, .stream supported providers can passed directly chat(). provider support particular argument, error raised. Advanced provider-specific configurations can accessed via provider functions.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a Language Model — chat","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage with OpenAI provider llm_message(\"Hello World\") |>    chat(ollama(.ollama_server = \"https://my-ollama-server.de\"),.model=\"mixtral\")        chat(mistral,.model=\"mixtral\")  # Use streaming with Claude provider llm_message(\"Tell me a story\") |>    chat(claude(),.stream=TRUE) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":null,"dir":"Reference","previous_headings":"","what":"Alias for the OpenAI Provider Function — chatgpt","title":"Alias for the OpenAI Provider Function — chatgpt","text":"chatgpt function alias openai() provider function. provides convenient way interact OpenAI API tasks sending chat messages, generating embeddings, handling batch operations using tidyllm verbs like chat(), embed(), send_batch().","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Alias for the OpenAI Provider Function — chatgpt","text":"","code":"chatgpt(..., .called_from = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Alias for the OpenAI Provider Function — chatgpt","text":"... Parameters passed appropriate OpenAI-specific function, model configuration, input text, API-specific options. .called_from internal argument specifies context (e.g., chat, embed, send_batch) function invoked. automatically managed modified user.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Alias for the OpenAI Provider Function — chatgpt","text":"result requested action, depending specific function invoked (e.g., updated LLMMessage object chat(), matrix embed()).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_azure_openai_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Batch Processing Status for Azure OpenAI Batch API — check_azure_openai_batch","title":"Check Batch Processing Status for Azure OpenAI Batch API — check_azure_openai_batch","text":"function retrieves processing status details specified Azure OpenAI batch ID Azure OpenAI Batch API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_azure_openai_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Batch Processing Status for Azure OpenAI Batch API — check_azure_openai_batch","text":"","code":"check_azure_openai_batch(   .llms = NULL,   .endpoint_url = Sys.getenv(\"AZURE_ENDPOINT_URL\"),   .batch_id = NULL,   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/check_azure_openai_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Batch Processing Status for Azure OpenAI Batch API — check_azure_openai_batch","text":".llms list LLMMessage objects. .endpoint_url Base URL API (default:  Sys.getenv(\"AZURE_ENDPOINT_URL\")). .batch_id manually set batch ID. .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .max_tries Maximum retries perform request (default: 3). .timeout Integer specifying request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_azure_openai_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Batch Processing Status for Azure OpenAI Batch API — check_azure_openai_batch","text":"tibble information status batch processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Batch Processing Status — check_batch","title":"Check Batch Processing Status — check_batch","text":"function retrieves processing status details specified batchid list LLMMessage objects batch attribute. routes input appropriate provider-specific batch API function.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Batch Processing Status — check_batch","text":"","code":"check_batch(   .llms,   .provider = getOption(\"tidyllm_cbatch_default\"),   .dry_run = NULL,   .max_tries = NULL,   .timeout = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/check_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Batch Processing Status — check_batch","text":".llms list LLMMessage objects character vector batch ID. .provider function function call specifying language model provider additional parameters. call provider function like openai(), claude(), etc. can also set default provider function via tidyllm_cbatch_default option. .dry_run Logical; TRUE, returns prepared request object without executing .max_tries Maximum retries perform request .timeout Integer specifying request timeout seconds","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Batch Processing Status — check_batch","text":"tibble information status batch processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_claude_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Batch Processing Status for Claude API — check_claude_batch","title":"Check Batch Processing Status for Claude API — check_claude_batch","text":"function retrieves processing status details specified Claude batch ID Claude API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_claude_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Batch Processing Status for Claude API — check_claude_batch","text":"","code":"check_claude_batch(   .llms = NULL,   .batch_id = NULL,   .api_url = \"https://api.anthropic.com/\",   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/check_claude_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Batch Processing Status for Claude API — check_claude_batch","text":".llms list LLMMessage objects .batch_id manually set batchid .api_url Character; base URL Claude API (default: \"https://api.anthropic.com/\"). .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .max_tries Maximum retries peform request .timeout Integer specifying request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_claude_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Batch Processing Status for Claude API — check_claude_batch","text":"tibble information status batch processing","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_mistral_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Batch Processing Status for Mistral Batch API — check_mistral_batch","title":"Check Batch Processing Status for Mistral Batch API — check_mistral_batch","text":"function retrieves processing status details specified Mistral batch ID Mistral Batch API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_mistral_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Batch Processing Status for Mistral Batch API — check_mistral_batch","text":"","code":"check_mistral_batch(   .llms = NULL,   .batch_id = NULL,   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/check_mistral_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Batch Processing Status for Mistral Batch API — check_mistral_batch","text":".llms list LLMMessage objects. .batch_id manually set batch ID. .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .max_tries Maximum retries perform request (default: 3). .timeout Integer specifying request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_mistral_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Batch Processing Status for Mistral Batch API — check_mistral_batch","text":"tibble information status batch processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_openai_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","title":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","text":"function retrieves processing status details specified OpenAI batch ID OpenAI Batch API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_openai_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","text":"","code":"check_openai_batch(   .llms = NULL,   .batch_id = NULL,   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/check_openai_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","text":".llms list LLMMessage objects. .batch_id manually set batch ID. .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .max_tries Maximum retries perform request (default: 3). .timeout Integer specifying request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_openai_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","text":"tibble information status batch processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":null,"dir":"Reference","previous_headings":"","what":"Provider Function for Claude models on the Anthropic API — claude","title":"Provider Function for Claude models on the Anthropic API — claude","text":"claude() function acts interface interacting Anthropic API main tidyllm verbs chat(), embed(), send_batch(). dynamically routes requests Claude-specific functions like claude_chat() send_claude_batch() based context call.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Function for Claude models on the Anthropic API — claude","text":"","code":"claude(..., .called_from = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Function for Claude models on the Anthropic API — claude","text":"... Parameters passed appropriate OpenAI-specific function, model configuration, input text, API-specific options. .called_from internal argument specifies action (e.g., chat, send_batch) function invoked . argument automatically managed modified user.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Provider Function for Claude models on the Anthropic API — claude","text":"result requested action, depending specific function invoked (e.g., updated LLMMessage object chat(), matrix embed()).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Interact with Claude AI models via the Anthropic API — claude_chat","title":"Interact with Claude AI models via the Anthropic API — claude_chat","text":"Interact Claude AI models via Anthropic API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interact with Claude AI models via the Anthropic API — claude_chat","text":"","code":"claude_chat(   .llm,   .model = \"claude-3-5-sonnet-20241022\",   .max_tokens = 1024,   .temperature = NULL,   .top_k = NULL,   .top_p = NULL,   .metadata = NULL,   .stop_sequences = NULL,   .tools = NULL,   .api_url = \"https://api.anthropic.com/\",   .verbose = FALSE,   .max_tries = 3,   .timeout = 60,   .stream = FALSE,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/claude_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interact with Claude AI models via the Anthropic API — claude_chat","text":".llm LLMMessage object containing conversation history system prompt. .model Character string specifying Claude model version (default: \"claude-3-5-sonnet-20241022\"). .max_tokens Integer specifying maximum number tokens response (default: 1024). .temperature Numeric 0 1 controlling response randomness. .top_k Integer controlling diversity limiting top K tokens. .top_p Numeric 0 1 nucleus sampling. .metadata List additional metadata include request. .stop_sequences Character vector sequences halt response generation. .tools List additional tools functions model can use. .api_url Base URL Anthropic API (default: \"https://api.anthropic.com/\"). .verbose Logical; TRUE, displays additional information API call (default: FALSE). .max_tries Maximum retries peform request .timeout Integer specifying request timeout seconds (default: 60). .stream Logical; TRUE, streams response piece piece (default: FALSE). .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interact with Claude AI models via the Anthropic API — claude_chat","text":"new LLMMessage object containing original messages plus Claude's response.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interact with Claude AI models via the Anthropic API — claude_chat","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage msg <- llm_message(\"What is R programming?\") result <- claude_chat(msg)  # With custom parameters result2 <- claude_chat(msg,                   .temperature = 0.7,                   .max_tokens = 1000) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/claude_list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Models from the Anthropic Claude API — claude_list_models","title":"List Available Models from the Anthropic Claude API — claude_list_models","text":"List Available Models Anthropic Claude API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude_list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Models from the Anthropic Claude API — claude_list_models","text":"","code":"claude_list_models(   .api_url = \"https://api.anthropic.com\",   .timeout = 60,   .max_tries = 3,   .dry_run = FALSE,   .verbose = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/claude_list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Models from the Anthropic Claude API — claude_list_models","text":".api_url Base URL API (default: \"https://api.anthropic.com\"). .timeout Request timeout seconds (default: 60). .max_tries Maximum number retries API request (default: 3). .dry_run Logical; TRUE, returns prepared request object without executing . .verbose Logical; TRUE, prints additional information request.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude_list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Models from the Anthropic Claude API — claude_list_models","text":"tibble containing model information (columns include type,id, display_name, created_at), NULL models found.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a Data Frame to an LLMMessage Object — df_llm_message","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"function converts data frame LLMMessage object representing conversation history. data frame must specific columns (role content), row representing message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"","code":"df_llm_message(.df)"},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":".df data frame least two rows columns role content. role column contain \"user\", \"assistant\", \"system\". content column contain corresponding message text.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"LLMMessage object representing structured conversation.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/embed.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate text embeddings — embed","title":"Generate text embeddings — embed","text":"embed() function allows embed text via specified provider. routes input appropriate provider-specific embedding function.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/embed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate text embeddings — embed","text":"","code":"embed(   .input,   .provider = getOption(\"tidyllm_embed_default\"),   .model = NULL,   .truncate = NULL,   .timeout = NULL,   .dry_run = NULL,   .max_tries = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/embed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate text embeddings — embed","text":".input character vector texts embed LLMMessage object .provider function function call specifying language model provider additional parameters. call provider function like openai(), ollama(), etc. can also set default provider function via tidyllm_embed_default option. .model embedding model use .truncate Whether truncate inputs fit model's context length .timeout Timeout API request seconds .dry_run TRUE, perform dry run return request object. .max_tries Maximum retry attempts requests","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/embed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate text embeddings — embed","text":"tibble two columns: input embeddings. input column contains texts sent embed, embeddings column list column row contains embedding vector sent input.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/embed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate text embeddings — embed","text":"","code":"if (FALSE) { # \\dontrun{ c(\"What is the meaning of life, the universe and everything?\",  \"How much wood would a woodchuck chuck?\",  \"How does the brain work?\") |>  embed(gemini)  } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_azure_openai_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch Results for an Azure OpenAI Batch — fetch_azure_openai_batch","title":"Fetch Results for an Azure OpenAI Batch — fetch_azure_openai_batch","text":"function retrieves results completed Azure OpenAI batch updates provided list LLMMessage objects responses. aligns response original request using custom_ids generated send_azure_openai_batch().","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_azure_openai_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch Results for an Azure OpenAI Batch — fetch_azure_openai_batch","text":"","code":"fetch_azure_openai_batch(   .llms,   .endpoint_url = Sys.getenv(\"AZURE_ENDPOINT_URL\"),   .batch_id = NULL,   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_azure_openai_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch Results for an Azure OpenAI Batch — fetch_azure_openai_batch","text":".llms list LLMMessage objects part batch. .endpoint_url Base URL API (default:  Sys.getenv(\"AZURE_ENDPOINT_URL\")). .batch_id Character; unique identifier batch. default NULL function attempt use batch_id attribute .llms. .dry_run Logical; TRUE, returns constructed request without executing (default: FALSE). .max_tries Integer; maximum number retries request fails (default: 3). .timeout Integer; request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_azure_openai_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch Results for an Azure OpenAI Batch — fetch_azure_openai_batch","text":"list updated LLMMessage objects, assistant's response added successful.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch Results from a Batch API — fetch_batch","title":"Fetch Results from a Batch API — fetch_batch","text":"function retrieves results completed  batch updates provided list LLMMessage objects responses. aligns response original request using custom_ids generated send_batch().","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch Results from a Batch API — fetch_batch","text":"","code":"fetch_batch(   .llms,   .provider = getOption(\"tidyllm_fbatch_default\"),   .dry_run = NULL,   .max_tries = NULL,   .timeout = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch Results from a Batch API — fetch_batch","text":".llms list LLMMessage objects containing conversation histories. .provider function function call specifying language model provider additional parameters. call provider function like openai(), claude(), etc. can also set default provider function via tidyllm_fbatch_default option. .dry_run Logical; TRUE, returns constructed request without executing .max_tries Integer; maximum number retries request fails .timeout Integer; request timeout seconds","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch Results from a Batch API — fetch_batch","text":"list updated LLMMessage objects, assistant's response added successful.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fetch Results from a Batch API — fetch_batch","text":"function routes input appropriate provider-specific batch API function.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_claude_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch Results for a Claude Batch — fetch_claude_batch","title":"Fetch Results for a Claude Batch — fetch_claude_batch","text":"function retrieves results completed Claude batch updates provided list LLMMessage objects responses. aligns response original request using custom_ids generated send_claude_batch().","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_claude_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch Results for a Claude Batch — fetch_claude_batch","text":"","code":"fetch_claude_batch(   .llms,   .batch_id = NULL,   .api_url = \"https://api.anthropic.com/\",   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_claude_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch Results for a Claude Batch — fetch_claude_batch","text":".llms list LLMMessage objects part batch. list names (custom IDs) set send_claude_batch() ensure correct alignment. .batch_id Character; unique identifier batch. default NULL function attempt use batch_id attribute .llms. .api_url Character; base URL Claude API (default: \"https://api.anthropic.com/\"). .dry_run Logical; TRUE, returns constructed request without executing (default: FALSE). .max_tries Integer; maximum number retries request fails (default: 3). .timeout Integer; request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_claude_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch Results for a Claude Batch — fetch_claude_batch","text":"list updated LLMMessage objects, assistant's response added successful.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_mistral_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch Results for an Mistral Batch — fetch_mistral_batch","title":"Fetch Results for an Mistral Batch — fetch_mistral_batch","text":"function retrieves results completed Mistral batch updates provided list LLMMessage objects responses. aligns response original request using custom_ids generated send_mistral_batch().","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_mistral_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch Results for an Mistral Batch — fetch_mistral_batch","text":"","code":"fetch_mistral_batch(   .llms,   .batch_id = NULL,   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_mistral_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch Results for an Mistral Batch — fetch_mistral_batch","text":".llms list LLMMessage objects part batch. .batch_id Character; unique identifier batch. default NULL function attempt use batch_id attribute .llms. .dry_run Logical; TRUE, returns constructed request without executing (default: FALSE). .max_tries Integer; maximum number retries request fails (default: 3). .timeout Integer; request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_mistral_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch Results for an Mistral Batch — fetch_mistral_batch","text":"list updated LLMMessage objects, assistant's response added successful.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_openai_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch Results for an OpenAI Batch — fetch_openai_batch","title":"Fetch Results for an OpenAI Batch — fetch_openai_batch","text":"function retrieves results completed OpenAI batch updates provided list LLMMessage objects responses. aligns response original request using custom_ids generated send_openai_batch().","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_openai_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch Results for an OpenAI Batch — fetch_openai_batch","text":"","code":"fetch_openai_batch(   .llms,   .batch_id = NULL,   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_openai_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch Results for an OpenAI Batch — fetch_openai_batch","text":".llms list LLMMessage objects part batch. .batch_id Character; unique identifier batch. default NULL function attempt use batch_id attribute .llms. .dry_run Logical; TRUE, returns constructed request without executing (default: FALSE). .max_tries Integer; maximum number retries request fails (default: 3). .timeout Integer; request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_openai_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch Results for an OpenAI Batch — fetch_openai_batch","text":"list updated LLMMessage objects, assistant's response added successful.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/field_chr.html","id":null,"dir":"Reference","previous_headings":"","what":"Define Field Descriptors for JSON Schema — field_chr","title":"Define Field Descriptors for JSON Schema — field_chr","text":"functions create field descriptors used tidyllm_schema() define JSON schema fields. support character, factor, numeric, logical types.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/field_chr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define Field Descriptors for JSON Schema — field_chr","text":"","code":"field_chr(.description = character(0), .vector = FALSE)  field_fct(.description = character(0), .levels, .vector = FALSE)  field_dbl(.description = character(0), .vector = FALSE)  field_lgl(.description = character(0), .vector = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/field_chr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define Field Descriptors for JSON Schema — field_chr","text":".description character string describing field (optional). .vector logical value indicating field vector (default: FALSE). .levels character vector specifying allowable values (field_fct() ).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/field_chr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define Field Descriptors for JSON Schema — field_chr","text":"S7 tidyllm_field object representing field descriptor.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/field_chr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define Field Descriptors for JSON Schema — field_chr","text":"","code":"field_chr(\"A common street name\") #> <tidyllm::tidyllm_field> #>  @ type       : chr \"string\" #>  @ description: chr \"A common street name\" #>  @ enum       : chr(0)  #>  @ vector     : logi FALSE field_fct(\"State abbreviation\", .levels = c(\"CA\", \"TX\", \"Other\")) #> <tidyllm::tidyllm_field> #>  @ type       : chr \"string\" #>  @ description: chr \"State abbreviation\" #>  @ enum       : chr [1:3] \"CA\" \"TX\" \"Other\" #>  @ vector     : logi FALSE field_dbl(\"House number\") #> <tidyllm::tidyllm_field> #>  @ type       : chr \"number\" #>  @ description: chr \"House number\" #>  @ enum       : chr(0)  #>  @ vector     : logi FALSE field_lgl(\"Is residential\") #> <tidyllm::tidyllm_field> #>  @ type       : chr \"boolean\" #>  @ description: chr \"Is residential\" #>  @ enum       : chr(0)  #>  @ vector     : logi FALSE field_dbl(\"A list of appartment numbers at the address\",.vector=TRUE ) #> <tidyllm::tidyllm_field> #>  @ type       : chr \"number\" #>  @ description: chr \"A list of appartment numbers at the address\" #>  @ enum       : chr(0)  #>  @ vector     : logi TRUE"},{"path":"https://edubruell.github.io/tidyllm/reference/gemini.html","id":null,"dir":"Reference","previous_headings":"","what":"Google Gemini Provider Function — gemini","title":"Google Gemini Provider Function — gemini","text":"gemini() function acts provider interface interacting Google Gemini API tidyllm's main verbs chat() embed(). dynamically routes requests Gemini-specific functions like gemini_chat() gemini_embedding() based context call.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Google Gemini Provider Function — gemini","text":"","code":"gemini(..., .called_from = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/gemini.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Google Gemini Provider Function — gemini","text":"... Parameters passed appropriate Gemini-specific function, model configuration, input text, API-specific options. .called_from internal argument specifying action (e.g., chat, embed) function invoked . argument automatically managed tidyllm verbs modified user.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Google Gemini Provider Function — gemini","text":"result requested action, depending specific function invoked (e.g., updated LLMMessage object chat()).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Google Gemini Provider Function — gemini","text":"functions, gemini_upload_file() gemini_delete_file(), specific Gemini general verb counterparts.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLMMessage to Gemini API — gemini_chat","title":"Send LLMMessage to Gemini API — gemini_chat","text":"Send LLMMessage Gemini API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLMMessage to Gemini API — gemini_chat","text":"","code":"gemini_chat(   .llm,   .model = \"gemini-2.0-flash\",   .fileid = NULL,   .temperature = NULL,   .max_output_tokens = NULL,   .top_p = NULL,   .top_k = NULL,   .grounding_threshold = NULL,   .presence_penalty = NULL,   .frequency_penalty = NULL,   .stop_sequences = NULL,   .safety_settings = NULL,   .json_schema = NULL,   .timeout = 120,   .dry_run = FALSE,   .max_tries = 3,   .verbose = FALSE,   .stream = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLMMessage to Gemini API — gemini_chat","text":".llm existing LLMMessage object initial text prompt. .model model identifier (default: \"gemini-1.5-flash\"). .fileid Optional vector file IDs uploaded via gemini_upload_file() (default: NULL). .temperature Controls randomness generation (default: NULL, range: 0.0-2.0). .max_output_tokens Maximum tokens response (default: NULL). .top_p Controls nucleus sampling (default: NULL, range: 0.0-1.0). .top_k Controls diversity token selection (default: NULL, range: 0 ). .grounding_threshold grounding threshold 0 1. lower grounding thresholds  Gemini use Google search relevant information answering.  (default: NULL). .presence_penalty Penalizes new tokens (default: NULL, range: -2.0 2.0). .frequency_penalty Penalizes frequent tokens (default: NULL, range: -2.0 2.0). .stop_sequences Optional character sequences stop generation (default: NULL, 5). .safety_settings list safety settings (default: NULL). .json_schema schema enforce output structure .timeout connection time (default: 120 seconds). .dry_run TRUE, perform dry run return request object. .max_tries Maximum retries perform request (default: 3). .verbose additional information shown API call. .stream response streamed (default: FALSE).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLMMessage to Gemini API — gemini_chat","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_delete_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a File from Gemini API — gemini_delete_file","title":"Delete a File from Gemini API — gemini_delete_file","text":"Deletes specific file Gemini API using file ID.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_delete_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a File from Gemini API — gemini_delete_file","text":"","code":"gemini_delete_file(.file_name)"},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_delete_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a File from Gemini API — gemini_delete_file","text":".file_name file ID (e.g., \"files/abc-123\") delete.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_delete_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Delete a File from Gemini API — gemini_delete_file","text":"Invisibly returns NULL. Prints confirmation message upon successful deletion.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using the Google Gemini API — gemini_embedding","title":"Generate Embeddings Using the Google Gemini API — gemini_embedding","text":"Generate Embeddings Using Google Gemini API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using the Google Gemini API — gemini_embedding","text":"","code":"gemini_embedding(   .input,   .model = \"text-embedding-004\",   .truncate = TRUE,   .timeout = 120,   .dry_run = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using the Google Gemini API — gemini_embedding","text":".input character vector texts embed LLMMessage object .model embedding model identifier (default: \"text-embedding-3-small\"). .truncate Whether truncate inputs fit model's context length (default: TRUE). .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object. .max_tries Maximum retry attempts requests (default: 3).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using the Google Gemini API — gemini_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_file_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Metadata for a File from Gemini API — gemini_file_metadata","title":"Retrieve Metadata for a File from Gemini API — gemini_file_metadata","text":"Retrieves metadata specific file uploaded Gemini API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_file_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Metadata for a File from Gemini API — gemini_file_metadata","text":"","code":"gemini_file_metadata(.file_name)"},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_file_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Metadata for a File from Gemini API — gemini_file_metadata","text":".file_name file ID (e.g., \"files/abc-123\") retrieve metadata .","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_file_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Metadata for a File from Gemini API — gemini_file_metadata","text":"tibble containing metadata fields name, display name, MIME type, size, URI.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_list_files.html","id":null,"dir":"Reference","previous_headings":"","what":"List Files in Gemini API — gemini_list_files","title":"List Files in Gemini API — gemini_list_files","text":"Lists metadata files uploaded Gemini API, supporting pagination.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_list_files.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Files in Gemini API — gemini_list_files","text":"","code":"gemini_list_files(.page_size = 10, .page_token = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_list_files.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Files in Gemini API — gemini_list_files","text":".page_size maximum number files return per page (default: 10, maximum: 100). .page_token token fetching next page results (default: NULL).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_list_files.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Files in Gemini API — gemini_list_files","text":"tibble containing metadata file, including fields name, display name, MIME type, URI.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_upload_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a File to Gemini API — gemini_upload_file","title":"Upload a File to Gemini API — gemini_upload_file","text":"Uploads file Gemini API returns metadata tibble.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_upload_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a File to Gemini API — gemini_upload_file","text":"","code":"gemini_upload_file(.file_path)"},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_upload_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a File to Gemini API — gemini_upload_file","text":".file_path local file path file upload.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/gemini_upload_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a File to Gemini API — gemini_upload_file","text":"tibble containing metadata uploaded file, including name, URI, MIME type.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_logprobs.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Log Probabilities from Assistant Replies — get_logprobs","title":"Retrieve Log Probabilities from Assistant Replies — get_logprobs","text":"Extracts token log probabilities assistant replies within LLMMessage object. row represents token log probability top alternative tokens.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_logprobs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Log Probabilities from Assistant Replies — get_logprobs","text":"","code":"get_logprobs(.llm, .index = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_logprobs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Log Probabilities from Assistant Replies — get_logprobs","text":".llm LLMMessage object containing message history. .index positive integer specifying assistant reply's log probabilities extract. NULL (default), log probabilities replies returned.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_logprobs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Log Probabilities from Assistant Replies — get_logprobs","text":"tibble containing log probabilities specified assistant reply replies.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_logprobs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Log Probabilities from Assistant Replies — get_logprobs","text":"empty tibble output logprobs requested. Currently works openai_chat() Columns include: reply_index: index assistant reply message history. token: generated token. logprob: log probability generated token. bytes: byte-level encoding token. top_logprobs: list column containing top alternative tokens log probabilities.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/get_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Metadata from Assistant Replies — get_metadata","title":"Retrieve Metadata from Assistant Replies — get_metadata","text":"Retrieves metadata assistant replies within LLMMessage object. returns metadata tibble.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Metadata from Assistant Replies — get_metadata","text":"","code":"get_metadata(.llm, .index = NULL)  last_metadata(.llm)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Metadata from Assistant Replies — get_metadata","text":".llm LLMMessage object containing message history. .index positive integer specifying assistant reply's metadata extract. NULL (default), metadata replies returned.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Metadata from Assistant Replies — get_metadata","text":"tibble containing metadata specified assistant reply replies.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_metadata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Metadata from Assistant Replies — get_metadata","text":"Metadata columns may include: model: model used generating reply. timestamp: time reply generated. prompt_tokens: number tokens input prompt. completion_tokens: number tokens assistant's reply. total_tokens: total number tokens (prompt + completion). api_specific: list column API-specific metadata. convenience, last_metadata() provided retrieve metadata last message.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Assistant Reply as Text — get_reply","title":"Retrieve Assistant Reply as Text — get_reply","text":"Extracts plain text content assistant's reply LLMMessage object. Use get_reply_data() structured replies JSON format.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Assistant Reply as Text — get_reply","text":"","code":"get_reply(.llm, .index = NULL)  last_reply(.llm)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Assistant Reply as Text — get_reply","text":".llm LLMMessage object containing message history. .index positive integer indicating index assistant reply retrieve. Defaults NULL, retrieves last reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Assistant Reply as Text — get_reply","text":"Returns character string containing assistant's reply, NA_character_ reply exists.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Assistant Reply as Text — get_reply","text":"function core utility retrieving assistant replies index. convenience, last_reply() provided wrapper retrieve latest assistant reply.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Assistant Reply as Structured Data — get_reply_data","title":"Retrieve Assistant Reply as Structured Data — get_reply_data","text":"Parses assistant's reply JSON returns corresponding structured data. reply marked JSON, attempts extract parse JSON content text.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Assistant Reply as Structured Data — get_reply_data","text":"","code":"get_reply_data(.llm, .index = NULL)  last_reply_data(.llm)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Assistant Reply as Structured Data — get_reply_data","text":".llm LLMMessage object containing message history. .index positive integer indicating index assistant reply retrieve. Defaults NULL, retrieves last reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Assistant Reply as Structured Data — get_reply_data","text":"Returns parsed data assistant's reply, NULL parsing fails.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Assistant Reply as Structured Data — get_reply_data","text":"convenience, last_reply_data() provided wrapper retrieve latest assistant reply's data.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a User Message by Index — get_user_message","title":"Retrieve a User Message by Index — get_user_message","text":"Extracts content user's message LLMMessage object specific index.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a User Message by Index — get_user_message","text":"","code":"get_user_message(.llm, .index = NULL)  last_user_message(.llm)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a User Message by Index — get_user_message","text":".llm LLMMessage object. .index positive integer indicating user message retrieve. Defaults NULL, retrieves last message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a User Message by Index — get_user_message","text":"Returns content user's message specified index. messages found, returns NA_character_.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve a User Message by Index — get_user_message","text":"convenience, last_user_message() provided wrapper retrieve latest user message without specifying index.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Groq API Provider Function — groq","title":"Groq API Provider Function — groq","text":"groq() function acts interface interacting Groq API tidyllm's main verbs. Currently, Groq supports groq_chat() chat-based interactions groq_transcribe() transcription tasks.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Groq API Provider Function — groq","text":"","code":"groq(..., .called_from = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Groq API Provider Function — groq","text":"... Parameters passed Groq-specific function, model configuration, input text, API-specific options. .called_from internal argument specifies action (e.g., chat) function invoked . argument automatically managed modified user.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Groq API Provider Function — groq","text":"result requested action, depending specific function invoked (currently, updated LLMMessage object groq_chat()).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Groq API Provider Function — groq","text":"Since groq_transcribe() unique Groq general verb counterpart, groq() currently routes messages groq_chat() used verbs like chat().","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLM Messages to the Groq Chat API — groq_chat","title":"Send LLM Messages to the Groq Chat API — groq_chat","text":"function sends message history Groq Chat API returns assistant's reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLM Messages to the Groq Chat API — groq_chat","text":"","code":"groq_chat(   .llm,   .model = \"deepseek-r1-distill-llama-70b\",   .max_tokens = 1024,   .temperature = NULL,   .top_p = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL,   .stop = NULL,   .seed = NULL,   .tools = NULL,   .tool_choice = NULL,   .api_url = \"https://api.groq.com/\",   .json = FALSE,   .timeout = 60,   .verbose = FALSE,   .stream = FALSE,   .dry_run = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/groq_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLM Messages to the Groq Chat API — groq_chat","text":".llm LLMMessage object containing conversation history. .model identifier model use (default: \"llama-3.2-11b-vision-preview\"). .max_tokens maximum number tokens can generated response (default: 1024). .temperature Controls randomness model's response. Values 0 2 allowed, higher values increase randomness (optional). .top_p Nucleus sampling parameter controls proportion probability mass considered. Values 0 1 allowed (optional). .frequency_penalty Number -2.0 2.0. Positive values penalize repeated tokens, reducing likelihood repetition (optional). .presence_penalty Number -2.0 2.0. Positive values encourage new topics penalizing tokens appeared far (optional). .stop One sequences API stop generating tokens. Can string list strings (optional). .seed integer deterministic sampling. specified, attempts return result repeated requests identical parameters (optional). .tools Either single TOOL object list TOOL objects representing available functions tool calls (optional). .tool_choice character string specifying tool-calling behavior; valid values \"none\", \"auto\", \"required\" (optional). .api_url Base URL Groq API (default: \"https://api.groq.com/\"). .json Whether response structured JSON (default: FALSE). .timeout Request timeout seconds (default: 60). .verbose TRUE, displays additional information API call, including rate limit details (default: FALSE). .stream Logical; TRUE, streams response piece piece (default: FALSE). .dry_run TRUE, performs dry run returns constructed request object without executing (default: FALSE). .max_tries Maximum retries peform request","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLM Messages to the Groq Chat API — groq_chat","text":"new LLMMessage object containing original messages plus assistant's response.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Send LLM Messages to the Groq Chat API — groq_chat","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage msg <- llm_message(\"What is Groq?\") result <- groq_chat(msg)  # With custom parameters result2 <- groq_chat(msg,                 .model = \"llama-3.2-vision\",                .temperature = 0.5,                 .max_tokens = 512) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/groq_list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Models from the Groq API — groq_list_models","title":"List Available Models from the Groq API — groq_list_models","text":"List Available Models Groq API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Models from the Groq API — groq_list_models","text":"","code":"groq_list_models(   .api_url = \"https://api.groq.com\",   .timeout = 60,   .max_tries = 3,   .dry_run = FALSE,   .verbose = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/groq_list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Models from the Groq API — groq_list_models","text":".api_url Base URL API (default: \"https://api.groq.com\"). .timeout Request timeout seconds (default: 60). .max_tries Maximum number retries API request (default: 3). .dry_run Logical; TRUE, returns prepared request object without executing . .verbose Logical; TRUE, prints additional information request.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Models from the Groq API — groq_list_models","text":"tibble containing model information (columns include id, created, owned_by, context_window), NULL models found.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":null,"dir":"Reference","previous_headings":"","what":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":"function reads audio file sends Groq transcription API transcription.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":"","code":"groq_transcribe(   .audio_file,   .model = \"whisper-large-v3\",   .language = NULL,   .prompt = NULL,   .temperature = 0,   .api_url = \"https://api.groq.com/openai/v1/audio/transcriptions\",   .dry_run = FALSE,   .verbose = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":".audio_file path audio file (required). Supported formats include flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm. .model model use transcription (default: \"whisper-large-v3\"). .language language input audio, ISO-639-1 format (optional). .prompt prompt guide transcription style. match audio language (optional). .temperature Sampling temperature, 0 1, higher values producing randomness (default: 0). .api_url Base URL API (default: \"https://api.groq.com/openai/v1/audio/transcriptions\"). .dry_run Logical; TRUE, performs dry run returns request object without making API call (default: FALSE). .verbose Logical; TRUE, rate limiting info displayed API request (default: FALSE). .max_tries Maximum retries peform request","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":"character vector containing transcription.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage groq_transcribe(.audio_file = \"example.mp3\") } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/list_azure_openai_batches.html","id":null,"dir":"Reference","previous_headings":"","what":"List Azure OpenAI Batch Requests — list_azure_openai_batches","title":"List Azure OpenAI Batch Requests — list_azure_openai_batches","text":"Retrieves batch request details Azure OpenAI Batch API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_azure_openai_batches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Azure OpenAI Batch Requests — list_azure_openai_batches","text":"","code":"list_azure_openai_batches(   .endpoint_url = Sys.getenv(\"AZURE_ENDPOINT_URL\"),   .limit = 20,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/list_azure_openai_batches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Azure OpenAI Batch Requests — list_azure_openai_batches","text":".endpoint_url Base URL API (default:  Sys.getenv(\"AZURE_ENDPOINT_URL\")). .limit Maximum number batches retrieve (default: 20). .max_tries Maximum retry attempts requests (default: 3). .timeout Request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_azure_openai_batches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Azure OpenAI Batch Requests — list_azure_openai_batches","text":"tibble batch details: batch ID, status, creation time, expiration time, request counts (total, completed, failed).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_batches.html","id":null,"dir":"Reference","previous_headings":"","what":"List all Batch Requests on a Batch API — list_batches","title":"List all Batch Requests on a Batch API — list_batches","text":"List Batch Requests Batch API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_batches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List all Batch Requests on a Batch API — list_batches","text":"","code":"list_batches(.provider = getOption(\"tidyllm_lbatch_default\"))"},{"path":"https://edubruell.github.io/tidyllm/reference/list_batches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List all Batch Requests on a Batch API — list_batches","text":".provider function function call specifying language model provider additional parameters. call provider function like openai(), claude(), etc. can also set default provider function via tidyllm_lbatch_default option.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_batches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List all Batch Requests on a Batch API — list_batches","text":"tibble information status batch processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_claude_batches.html","id":null,"dir":"Reference","previous_headings":"","what":"List Claude Batch Requests — list_claude_batches","title":"List Claude Batch Requests — list_claude_batches","text":"Retrieves batch request details Claude API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_claude_batches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Claude Batch Requests — list_claude_batches","text":"","code":"list_claude_batches(   .api_url = \"https://api.anthropic.com/\",   .limit = 20,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/list_claude_batches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Claude Batch Requests — list_claude_batches","text":".api_url Base URL Claude API (default: \"https://api.anthropic.com/\"). .limit Maximum number batches retrieve (default: 20). .max_tries Maximum retry attempts requests (default: 3). .timeout Request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_claude_batches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Claude Batch Requests — list_claude_batches","text":"tibble batch details: batch ID, status, creation time, expiration time, request counts (succeeded, errored, expired, canceled).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_mistral_batches.html","id":null,"dir":"Reference","previous_headings":"","what":"List Mistral Batch Requests — list_mistral_batches","title":"List Mistral Batch Requests — list_mistral_batches","text":"Retrieves batch request details OpenAI Batch API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_mistral_batches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Mistral Batch Requests — list_mistral_batches","text":"","code":"list_mistral_batches(   .limit = 100,   .max_tries = 3,   .timeout = 60,   .status = NULL,   .created_after = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/list_mistral_batches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Mistral Batch Requests — list_mistral_batches","text":".limit Maximum number batches retrieve (default: 20). .max_tries Maximum retry attempts requests (default: 3). .timeout Request timeout seconds (default: 60). .status Filter status. (default: NULL) .created_after created string specifiying date-time  (default: NULL)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_mistral_batches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Mistral Batch Requests — list_mistral_batches","text":"tibble batch details batches fitting request","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Models for a Provider — list_models","title":"List Available Models for a Provider — list_models","text":"list_models() function retrieves available models specified provider.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Models for a Provider — list_models","text":"","code":"list_models(.provider = getOption(\"tidyllm_lmodels_default\"), ...)"},{"path":"https://edubruell.github.io/tidyllm/reference/list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Models for a Provider — list_models","text":".provider function function call specifying provider additional parameters. can also set default provider via tidyllm_lmodels_default option. ... Additional arguments passed provider-specific list_models function.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Models for a Provider — list_models","text":"tibble containing model information.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_openai_batches.html","id":null,"dir":"Reference","previous_headings":"","what":"List OpenAI Batch Requests — list_openai_batches","title":"List OpenAI Batch Requests — list_openai_batches","text":"Retrieves batch request details OpenAI Batch API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_openai_batches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List OpenAI Batch Requests — list_openai_batches","text":"","code":"list_openai_batches(.limit = 20, .max_tries = 3, .timeout = 60)"},{"path":"https://edubruell.github.io/tidyllm/reference/list_openai_batches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List OpenAI Batch Requests — list_openai_batches","text":".limit Maximum number batches retrieve (default: 20). .max_tries Maximum retry attempts requests (default: 3). .timeout Request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_openai_batches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List OpenAI Batch Requests — list_openai_batches","text":"tibble batch details: batch ID, status, creation time, expiration time, request counts (total, completed, failed).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Create or Update Large Language Model Message Object — llm_message","title":"Create or Update Large Language Model Message Object — llm_message","text":"function creates new LLMMessage object updates existing one. supports adding text prompts various media types, images, PDFs, text files, plots.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create or Update Large Language Model Message Object — llm_message","text":"","code":"llm_message(   .llm = NULL,   .prompt = NULL,   .role = \"user\",   .system_prompt = \"You are a helpful assistant\",   .imagefile = NULL,   .pdf = NULL,   .textfile = NULL,   .capture_plot = FALSE,   .f = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create or Update Large Language Model Message Object — llm_message","text":".llm existing LLMMessage object initial text prompt. .prompt Text prompt add message history. .role role message sender, typically \"user\" \"assistant\". .system_prompt Default system prompt new LLMMessage needs created. .imagefile Path image file attached (optional). .pdf Path PDF file attached (optional). Can character vector length one (file path), list filename, start_page, end_page. .textfile Path text file read attached (optional). .capture_plot Boolean indicate whether plot captured attached image (optional). .f R function object coercible function via rlang::as_function, whose output captured attached (optional).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create or Update Large Language Model Message Object — llm_message","text":"Returns updated new LLMMessage object.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":null,"dir":"Reference","previous_headings":"","what":"Mistral Provider Function — mistral","title":"Mistral Provider Function — mistral","text":"mistral() function acts interface interacting Mistral API main tidyllm verbs chat() embed(). dynamically routes requests Mistral-specific functions like mistral_chat() mistral_embedding() based context call.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mistral Provider Function — mistral","text":"","code":"mistral(..., .called_from = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mistral Provider Function — mistral","text":"... Parameters passed appropriate Mistral-specific function, model configuration, input text, API-specific options. .called_from internal argument specifies action (e.g., chat, embed, send_batch) function invoked . argument automatically managed modified user.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mistral Provider Function — mistral","text":"result requested action, depending specific function invoked (e.g., updated LLMMessage object chat(), matrix embed()).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLMMessage to Mistral API — mistral_chat","title":"Send LLMMessage to Mistral API — mistral_chat","text":"Send LLMMessage Mistral API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLMMessage to Mistral API — mistral_chat","text":"","code":"mistral_chat(   .llm,   .model = \"mistral-large-latest\",   .stream = FALSE,   .seed = NULL,   .json = FALSE,   .temperature = 0.7,   .top_p = 1,   .stop = NULL,   .safe_prompt = FALSE,   .timeout = 120,   .max_tries = 3,   .max_tokens = 1024,   .min_tokens = NULL,   .dry_run = FALSE,   .verbose = FALSE,   .tools = NULL,   .tool_choice = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLMMessage to Mistral API — mistral_chat","text":".llm LLMMessage object. .model model identifier use (default: \"mistral-large-latest\"). .stream Whether stream back partial progress console. (default: FALSE). .seed seed use random sampling. set, different calls generate deterministic results (optional). .json Whether output JSON mode(default: FALSE). .temperature Sampling temperature use, 0.0 1.5. Higher values make output random, lower values make focused deterministic (default: 0.7). .top_p Nucleus sampling parameter, 0.0 1.0. model considers tokens top_p probability mass (default: 1). .stop Stop generation token detected, one tokens detected providing list (optional). .safe_prompt Whether inject safety prompt conversations (default: FALSE). .timeout connection time seconds (default: 120). .max_tries Maximum retries peform request .max_tokens maximum number tokens generate completion. Must >= 0 (default: 1024). .min_tokens minimum number tokens generate completion. Must >= 0 (optional). .dry_run TRUE, perform dry run return request object (default: FALSE). .verbose additional information shown API call? (default: FALSE) .tools Either single TOOL object list TOOL objects representing available functions tool calls. .tool_choice character string specifying tool-calling behavior; valid values \"none\", \"auto\", \"required\".","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLMMessage to Mistral API — mistral_chat","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using Mistral API — mistral_embedding","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":"Generate Embeddings Using Mistral API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":"","code":"mistral_embedding(   .input,   .model = \"mistral-embed\",   .timeout = 120,   .max_tries = 3,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":".input character vector texts embed LLMMessage object .model embedding model identifier (default: \"mistral-embed\"). .timeout Timeout API request seconds (default: 120). .max_tries Maximum retries peform request .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Models from the Mistral API — mistral_list_models","title":"List Available Models from the Mistral API — mistral_list_models","text":"List Available Models Mistral API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Models from the Mistral API — mistral_list_models","text":"","code":"mistral_list_models(   .api_url = \"https://api.mistral.ai\",   .timeout = 60,   .max_tries = 3,   .dry_run = FALSE,   .verbose = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Models from the Mistral API — mistral_list_models","text":".api_url Base URL API (default: \"https://api.mistral.ai\"). .timeout Request timeout seconds (default: 60). .max_tries Maximum number retries API request (default: 3). .dry_run Logical; TRUE, returns prepared request object without executing . .verbose Logical; TRUE, prints additional information request.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Models from the Mistral API — mistral_list_models","text":"tibble containing model information (columns include id created), NULL models found.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Ollama API Provider Function — ollama","title":"Ollama API Provider Function — ollama","text":"ollama() function acts interface interacting local AI models via Ollama API. integrates seamlessly main tidyllm verbs chat() embed().","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ollama API Provider Function — ollama","text":"","code":"ollama(..., .called_from = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ollama API Provider Function — ollama","text":"... Parameters passed appropriate Ollama-specific function, model configuration, input text, API-specific options. .called_from internal argument specifying verb (e.g., chat, embed) function invoked . argument automatically managed tidyllm set user.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ollama API Provider Function — ollama","text":"result requested action: chat(): updated LLMMessage object containing model's response. embed(): matrix column corresponds embedding.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ollama API Provider Function — ollama","text":"functionalities, like ollama_download_model() ollama_list_models() unique Ollama API general verb counterpart. functions can accessed directly. Supported Verbs: chat(): Sends message Ollama model retrieves model's response. embed(): Generates embeddings input texts using Ollama model. send_batch(): Behaves different send_batch() verbs since immediately processes answers","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Interact with local AI models via the Ollama API — ollama_chat","title":"Interact with local AI models via the Ollama API — ollama_chat","text":"Interact local AI models via Ollama API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interact with local AI models via the Ollama API — ollama_chat","text":"","code":"ollama_chat(   .llm,   .model = \"gemma2\",   .stream = FALSE,   .seed = NULL,   .json_schema = NULL,   .temperature = NULL,   .num_ctx = 2048,   .num_predict = NULL,   .top_k = NULL,   .top_p = NULL,   .min_p = NULL,   .mirostat = NULL,   .mirostat_eta = NULL,   .mirostat_tau = NULL,   .repeat_last_n = NULL,   .repeat_penalty = NULL,   .tfs_z = NULL,   .stop = NULL,   .ollama_server = \"http://localhost:11434\",   .timeout = 120,   .keep_alive = NULL,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interact with local AI models via the Ollama API — ollama_chat","text":".llm LLMMessage object containing conversation history system prompt. .model Character string specifying Ollama model use (default: \"gemma2\") .stream Logical; whether stream response (default: FALSE) .seed Integer; seed reproducible generation (default: NULL) .json_schema JSON schema object R list enforce output structure (default: NULL) .temperature Float 0-2; controls randomness responses (default: NULL) .num_ctx Integer; sets context window size (default: 2048) .num_predict Integer; maximum number tokens predict (default: NULL) .top_k Integer; controls diversity limiting top tokens considered (default: NULL) .top_p Float 0-1; nucleus sampling threshold (default: NULL) .min_p Float 0-1; minimum probability threshold (default: NULL) .mirostat Integer (0,1,2); enables Mirostat sampling algorithm (default: NULL) .mirostat_eta Float; Mirostat learning rate (default: NULL) .mirostat_tau Float; Mirostat target entropy (default: NULL) .repeat_last_n Integer; tokens look back repetition (default: NULL) .repeat_penalty Float; penalty repeated tokens (default: NULL) .tfs_z Float; tail free sampling parameter (default: NULL) .stop Character; custom stop sequence(s) (default: NULL) .ollama_server String; Ollama API endpoint (default: \"http://localhost:11434\") .timeout Integer; API request timeout seconds (default: 120) .keep_alive Character; long ollama model kept memory request (default: NULL - 5 Minutes) .dry_run Logical; TRUE, returns request object without execution (default: FALSE)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interact with local AI models via the Ollama API — ollama_chat","text":"new LLMMessage object containing original messages plus model's response","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_chat.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Interact with local AI models via the Ollama API — ollama_chat","text":"function provides extensive control generation process various parameters: Temperature (0-2): Higher values increase creativity, lower values make responses focused Top-k/Top-p: Control diversity generated text Mirostat: Advanced sampling algorithm maintaining consistent complexity Repeat penalties: Prevent repetitive text Context window: Control much previous conversation considered","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interact with local AI models via the Ollama API — ollama_chat","text":"","code":"if (FALSE) { # \\dontrun{ llm_message(\"user\", \"Hello, how are you?\") response <- ollama_chat(llm, .model = \"gemma2\", .temperature = 0.7)  # With custom parameters response <- ollama_chat(   llm,   .model = \"llama2\",   .temperature = 0.8,   .top_p = 0.9,   .num_ctx = 4096 ) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_delete_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a model from the Ollama API — ollama_delete_model","title":"Delete a model from the Ollama API — ollama_delete_model","text":"function sends DELETE request Ollama API remove specified model.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_delete_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a model from the Ollama API — ollama_delete_model","text":"","code":"ollama_delete_model(.model, .ollama_server = \"http://localhost:11434\")"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_delete_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a model from the Ollama API — ollama_delete_model","text":".model name model delete. .ollama_server base URL Ollama API (default \"http://localhost:11434\").","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a model from the Ollama API — ollama_download_model","title":"Download a model from the Ollama API — ollama_download_model","text":"function sends request Ollama API download specified model Ollama's large online library models.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a model from the Ollama API — ollama_download_model","text":"","code":"ollama_download_model(.model, .ollama_server = \"http://localhost:11434\")"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a model from the Ollama API — ollama_download_model","text":".model name model download. .ollama_server base URL Ollama API (default \"http://localhost:11434\").","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using Ollama API — ollama_embedding","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"Generate Embeddings Using Ollama API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"","code":"ollama_embedding(   .input,   .model = \"all-minilm\",   .truncate = TRUE,   .ollama_server = \"http://localhost:11434\",   .timeout = 120,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":".input Aa charachter vector texts embed LLMMessage object .model embedding model identifier (default: \"-minilm\"). .truncate Whether truncate inputs fit model's context length (default: TRUE). .ollama_server URL Ollama server used (default: \"http://localhost:11434\"). .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve and return model information from the Ollama API — ollama_list_models","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"function connects Ollama API retrieves information available models, returning tibble.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"","code":"ollama_list_models(.ollama_server = \"http://localhost:11434\")"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":".ollama_server URL ollama server used","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"tibble containing model information, NULL models found.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai.html","id":null,"dir":"Reference","previous_headings":"","what":"OpenAI Provider Function — openai","title":"OpenAI Provider Function — openai","text":"openai() function acts interface interacting OpenAI API main tidyllm verbs chat(), embed(), send_batch(). dynamically routes requests OpenAI-specific functions like openai_chat() openai_embedding() based context call.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"OpenAI Provider Function — openai","text":"","code":"openai(..., .called_from = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"OpenAI Provider Function — openai","text":"... Parameters passed appropriate OpenAI-specific function, model configuration, input text, API-specific options. .called_from internal argument specifies action (e.g., chat, embed, send_batch) function invoked . argument automatically managed modified user.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"OpenAI Provider Function — openai","text":"result requested action, depending specific function invoked (e.g., updated LLMMessage object chat(), matrix embed()).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLM Messages to the OpenAI Chat Completions API — openai_chat","title":"Send LLM Messages to the OpenAI Chat Completions API — openai_chat","text":"function sends message history OpenAI Chat Completions API returns assistant's reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLM Messages to the OpenAI Chat Completions API — openai_chat","text":"","code":"openai_chat(   .llm,   .model = \"gpt-4o\",   .max_completion_tokens = NULL,   .reasoning_effort = NULL,   .frequency_penalty = NULL,   .logit_bias = NULL,   .presence_penalty = NULL,   .seed = NULL,   .stop = NULL,   .stream = FALSE,   .temperature = NULL,   .top_p = NULL,   .api_url = \"https://api.openai.com/\",   .timeout = 60,   .verbose = FALSE,   .json_schema = NULL,   .max_tries = 3,   .dry_run = FALSE,   .compatible = FALSE,   .api_path = \"/v1/chat/completions\",   .logprobs = NULL,   .top_logprobs = NULL,   .tools = NULL,   .tool_choice = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/openai_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLM Messages to the OpenAI Chat Completions API — openai_chat","text":".llm LLMMessage object containing conversation history. .model identifier model use (default: \"gpt-4o\"). .max_completion_tokens upper bound number tokens can generated completion, including visible output tokens reasoning tokens. .reasoning_effort long reasoning models reason (can either \"low\",\"medium\" \"high\") .frequency_penalty Number -2.0 2.0. Positive values penalize new tokens based existing frequency text far. .logit_bias named list modifying likelihood specified tokens appearing completion. .presence_penalty Number -2.0 2.0. Positive values penalize new tokens based whether appear text far. .seed specified, system make best effort sample deterministically. .stop 4 sequences API stop generating tokens. .stream set TRUE, answer streamed console comes (default: FALSE). .temperature sampling temperature use, 0 2. Higher values make output random. .top_p alternative sampling temperature, called nucleus sampling. .api_url Base URL API (default: \"https://api.openai.com/\"). .timeout Request timeout seconds (default: 60). .verbose additional information shown API call (default: FALSE). .json_schema JSON schema object provided tidyllm schema ellmer schemata. .max_tries Maximum retries perform request .dry_run TRUE, perform dry run return request object (default: FALSE). .compatible TRUE, skip API rate-limit checks OpenAI compatible APIs (default: FALSE). .api_path path relative base .api_url API (default: \"/v1/chat/completions\"). .logprobs TRUE, get log probabilities output token (default: NULL). .top_logprobs specified, get top N log probabilities output token (0-5, default: NULL). .tools Either single TOOL object list TOOL objects representing available functions tool calls. .tool_choice character string specifying tool-calling behavior; valid values \"none\", \"auto\", \"required\".","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLM Messages to the OpenAI Chat Completions API — openai_chat","text":"new LLMMessage object containing original messages plus assistant's response.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using OpenAI API — openai_embedding","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":"Generate Embeddings Using OpenAI API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":"","code":"openai_embedding(   .input,   .model = \"text-embedding-3-small\",   .truncate = TRUE,   .timeout = 120,   .dry_run = FALSE,   .max_tries = 3,   .verbose = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":".input existing LLMMessage object (character vector texts embed) .model embedding model identifier (default: \"text-embedding-3-small\"). .truncate Whether truncate inputs fit model's context length (default: TRUE). .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object. .max_tries Maximum retry attempts requests (default: 3). .verbose information current ratelimits printed? (default: FALSE)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":"tibble two columns: input embeddings. input column contains texts sent embed, embeddings column list column row contains embedding vector sent input.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Models from the OpenAI API — openai_list_models","title":"List Available Models from the OpenAI API — openai_list_models","text":"List Available Models OpenAI API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Models from the OpenAI API — openai_list_models","text":"","code":"openai_list_models(   .api_url = \"https://api.openai.com\",   .timeout = 60,   .max_tries = 3,   .dry_run = FALSE,   .verbose = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/openai_list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Models from the OpenAI API — openai_list_models","text":".api_url Base URL API (default: \"https://api.openai.com\"). .timeout Request timeout seconds (default: 60). .max_tries Maximum number retries API request (default: 3). .dry_run Logical; TRUE, returns prepared request object without executing . .verbose Logical; TRUE, prints additional information request.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Available Models from the OpenAI API — openai_list_models","text":"tibble containing model information (columns include id, created, owned_by), NULL models found.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch Process PDF into LLM Messages — pdf_page_batch","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"function processes PDF file page page. page, extracts text converts page image. creates list LLMMessage objects text image multimodal processing. Users can specify range pages process provide custom function generate prompts page.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"","code":"pdf_page_batch(   .pdf,   .general_prompt,   .system_prompt = \"You are a helpful assistant\",   .page_range = NULL,   .prompt_fn = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":".pdf Path PDF file. .general_prompt default prompt applied page .prompt_fn provided. .system_prompt Optional system prompt initialize LLMMessage (default \"helpful assistant\"). .page_range vector two integers specifying start end pages process. NULL, pages processed. .prompt_fn optional custom function generates prompt page. function takes page text input returns string. NULL, .general_prompt used pages.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"list LLMMessage objects, containing text image page.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perplexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Perplexity Provider Function — perplexity","title":"Perplexity Provider Function — perplexity","text":"perplexity() function acts provider interface interacting Perplexity API tidyllm's chat() verb. dynamically routes requests Perplxeity-specific function. moment perplexity_chat()","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perplexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perplexity Provider Function — perplexity","text":"","code":"perplexity(..., .called_from = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/perplexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perplexity Provider Function — perplexity","text":"... Parameters passed appropriate Perplexity-specific function, model configuration, input text, API-specific options. .called_from internal argument specifying action (e.g., chat, embed) function invoked . argument automatically managed tidyllm verbs modified user.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perplexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perplexity Provider Function — perplexity","text":"result requested action, depending specific function invoked (e.g., updated LLMMessage object chat()).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perplexity_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLM Messages to the Perplexity Chat API — perplexity_chat","title":"Send LLM Messages to the Perplexity Chat API — perplexity_chat","text":"function sends message history Perplexity Chat API returns assistant's reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perplexity_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLM Messages to the Perplexity Chat API — perplexity_chat","text":"","code":"perplexity_chat(   .llm,   .model = \"sonar\",   .max_tokens = 1024,   .temperature = NULL,   .top_p = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL,   .stop = NULL,   .search_domain_filter = NULL,   .return_images = FALSE,   .search_recency_filter = NULL,   .api_url = \"https://api.perplexity.ai/\",   .json = FALSE,   .timeout = 60,   .verbose = FALSE,   .stream = FALSE,   .dry_run = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/perplexity_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLM Messages to the Perplexity Chat API — perplexity_chat","text":".llm LLMMessage object containing conversation history. .model identifier model use (default: \"sonar\"). .max_tokens maximum number tokens can generated response (default: 1024). .temperature Controls randomness model's response. Values 0 (exclusive) 2 (exclusive) allowed, higher values increase randomness (optional). .top_p Nucleus sampling parameter controls proportion probability mass considered. Values 0 (exclusive) 1 (exclusive) allowed (optional). .frequency_penalty Number greater 0. Values > 1.0 penalize repeated tokens, reducing likelihood repetition (optional). .presence_penalty Number -2.0 2.0. Positive values encourage new topics penalizing tokens appeared far (optional). .stop One sequences API stop generating tokens. Can string list strings (optional). .search_domain_filter vector domains limit exclude search results. exclusion, prefix domains \"-\" (optional, currently closed beta). .return_images Logical; TRUE, enables returning images model's response (default: FALSE, currently closed beta). .search_recency_filter Limits search results specific time interval (e.g., \"month\", \"week\", \"day\", \"hour\"). applies online models (optional). .api_url Base URL Perplexity API (default: \"https://api.perplexity.ai/\"). .json Whether response structured JSON (default: FALSE). .timeout Request timeout seconds (default: 60). .verbose TRUE, displays additional information API call, including rate limit details (default: FALSE). .stream Logical; TRUE, streams response piece piece (default: FALSE). .dry_run TRUE, performs dry run returns constructed request object without executing (default: FALSE). .max_tries Maximum retries perform request (default: 3).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perplexity_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLM Messages to the Perplexity Chat API — perplexity_chat","text":"new LLMMessage object containing original messages plus assistant's response.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the current rate limit information for all or a specific API — rate_limit_info","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"function retrieves rate limit details specified API, APIs stored .tidyllm_rate_limit_env API specified.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"","code":"rate_limit_info(.api_name = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":".api_name (Optional) name API whose rate limit info want get provided, rate limit info APIs environment returned","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"tibble containing rate limit information.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_azure_openai_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a Batch of Messages to Azure OpenAI Batch API — send_azure_openai_batch","title":"Send a Batch of Messages to Azure OpenAI Batch API — send_azure_openai_batch","text":"function creates submits batch messages Azure OpenAI Batch API asynchronous processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_azure_openai_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a Batch of Messages to Azure OpenAI Batch API — send_azure_openai_batch","text":"","code":"send_azure_openai_batch(   .llms,   .deployment = \"gpt-4o-mini\",   .endpoint_url = Sys.getenv(\"AZURE_ENDPOINT_URL\"),   .api_version = \"2024-10-01-preview\",   .max_completion_tokens = NULL,   .frequency_penalty = NULL,   .logit_bias = NULL,   .logprobs = FALSE,   .top_logprobs = NULL,   .presence_penalty = NULL,   .seed = NULL,   .stop = NULL,   .temperature = NULL,   .top_p = NULL,   .dry_run = FALSE,   .overwrite = FALSE,   .max_tries = 3,   .timeout = 60,   .verbose = FALSE,   .json_schema = NULL,   .id_prefix = \"tidyllm_azure_openai_req_\" )"},{"path":"https://edubruell.github.io/tidyllm/reference/send_azure_openai_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a Batch of Messages to Azure OpenAI Batch API — send_azure_openai_batch","text":".llms LLMMessage object containing conversation history. .deployment identifier model deployed (default: \"gpt-4o-mini\"). .endpoint_url Base URL API (default:  Sys.getenv(\"AZURE_ENDPOINT_URL\")). .api_version version API deployed (default: \"2024-10-01-preview\") .max_completion_tokens upper bound number tokens can generated completion, including visible output tokens reasoning tokens. .frequency_penalty Number -2.0 2.0. Positive values penalize new tokens based existing frequency text far. .logit_bias named list modifying likelihood specified tokens appearing completion. .logprobs Whether return log probabilities output tokens (default: FALSE). .top_logprobs integer 0 20 specifying number likely tokens return token position. .presence_penalty Number -2.0 2.0. Positive values penalize new tokens based whether appear text far. .seed specified, system make best effort sample deterministically. .stop 4 sequences API stop generating tokens. .temperature sampling temperature use, 0 2. Higher values make output random. .top_p alternative sampling temperature, called nucleus sampling. .dry_run TRUE, perform dry run return request object (default: FALSE). .overwrite Logical; TRUE, allows overwriting existing batch ID (default: FALSE). .max_tries Maximum number retries perform request (default: 3). .timeout Request timeout seconds (default: 60). .verbose Logical; TRUE, additional info requests printed (default: FALSE). .json_schema JSON schema object R list enforce output structure (default: NULL). .id_prefix Character string specify prefix generating custom IDs names .llms missing (default: \"tidyllm_openai_req_\").","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_azure_openai_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a Batch of Messages to Azure OpenAI Batch API — send_azure_openai_batch","text":"updated named list .llms identifiers align batch responses, including batch_id attribute.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a batch of messages to a batch API — send_batch","title":"Send a batch of messages to a batch API — send_batch","text":"send_batch() function allows send list LLMMessage objects API. routes input appropriate provider-specific batch API function.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a batch of messages to a batch API — send_batch","text":"","code":"send_batch(   .llms,   .provider = getOption(\"tidyllm_sbatch_default\"),   .dry_run = NULL,   .temperature = NULL,   .timeout = NULL,   .top_p = NULL,   .max_tries = NULL,   .model = NULL,   .verbose = NULL,   .json_schema = NULL,   .seed = NULL,   .stop = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL,   .id_prefix = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/send_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a batch of messages to a batch API — send_batch","text":".llms list LLMMessage objects containing conversation histories. .provider function function call specifying language model provider additional parameters. call provider function like openai(), claude(), etc. can also set default provider function via tidyllm_sbatch_default option. .dry_run Logical; TRUE, simulates request without sending provider. Useful testing. .temperature Numeric; controls randomness model's output (0 = deterministic). .timeout Numeric; maximum time (seconds) wait response. .top_p Numeric; nucleus sampling parameter, limits sampling top cumulative probability p. .max_tries Integer; maximum number retries failed requests. .model Character; model identifier use (e.g., \"gpt-4\"). .verbose Logical; TRUE, prints additional information request response. .json_schema List; JSON schema object R list enforce output structure .seed Integer; sets random seed reproducibility. .stop Character vector; specifies sequences model stop generating tokens. .frequency_penalty Numeric; adjusts likelihood repeating tokens (positive values decrease repetition). .presence_penalty Numeric; adjusts likelihood introducing new tokens (positive values encourage novelty). .id_prefix Character string specify prefix generating custom IDs names .llms missing","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a batch of messages to a batch API — send_batch","text":"updated named list .llms identifiers align batch responses, including batch_id attribute.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_claude_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a Batch of Messages to Claude API — send_claude_batch","title":"Send a Batch of Messages to Claude API — send_claude_batch","text":"function creates submits batch messages Claude API asynchronous processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_claude_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a Batch of Messages to Claude API — send_claude_batch","text":"","code":"send_claude_batch(   .llms,   .model = \"claude-3-5-sonnet-20241022\",   .max_tokens = 1024,   .temperature = NULL,   .top_k = NULL,   .top_p = NULL,   .stop_sequences = NULL,   .api_url = \"https://api.anthropic.com/\",   .verbose = FALSE,   .dry_run = FALSE,   .overwrite = FALSE,   .max_tries = 3,   .timeout = 60,   .id_prefix = \"tidyllm_claude_req_\" )"},{"path":"https://edubruell.github.io/tidyllm/reference/send_claude_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a Batch of Messages to Claude API — send_claude_batch","text":".llms list LLMMessage objects containing conversation histories. .model Character string specifying Claude model version (default: \"claude-3-5-sonnet-20241022\"). .max_tokens Integer specifying maximum tokens per response (default: 1024). .temperature Numeric 0 1 controlling response randomness. .top_k Integer diversity limiting top K tokens. .top_p Numeric 0 1 nucleus sampling. .stop_sequences Character vector sequences halt response generation. .api_url Base URL Claude API (default: \"https://api.anthropic.com/\"). .verbose Logical; TRUE, prints message batch ID (default: FALSE). .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .overwrite Logical; TRUE, allows overwriting existing batch ID associated request (default: FALSE). .max_tries Maximum number retries perform request. .timeout Integer specifying request timeout seconds (default: 60). .id_prefix Character string specify prefix generating custom IDs names .llms missing. Defaults \"tidyllm_claude_req_\".","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_claude_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a Batch of Messages to Claude API — send_claude_batch","text":"updated named list .llms identifiers align batch responses, including batch_id attribute.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_mistral_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a Batch of Requests to the Mistral API — send_mistral_batch","title":"Send a Batch of Requests to the Mistral API — send_mistral_batch","text":"Send Batch Requests Mistral API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_mistral_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a Batch of Requests to the Mistral API — send_mistral_batch","text":"","code":"send_mistral_batch(   .llms,   .model = \"mistral-small-latest\",   .endpoint = \"/v1/chat/completions\",   .metadata = NULL,   .temperature = 0.7,   .top_p = 1,   .max_tokens = 1024,   .min_tokens = NULL,   .seed = NULL,   .stop = NULL,   .dry_run = FALSE,   .overwrite = FALSE,   .max_tries = 3,   .timeout = 60,   .id_prefix = \"tidyllm_mistral_req_\" )"},{"path":"https://edubruell.github.io/tidyllm/reference/send_mistral_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a Batch of Requests to the Mistral API — send_mistral_batch","text":".llms list LLMMessage objects containing conversation histories. .model Mistral model version (default: \"mistral-small-latest\"). .endpoint API endpoint (default: \"/v1/chat/completions\"). .metadata Optional metadata batch. .temperature Sampling temperature use, 0.0 1.5. Higher values make output random (default: 0.7). .top_p Nucleus sampling parameter, 0.0 1.0 (default: 1). .max_tokens maximum number tokens generate completion (default: 1024). .min_tokens minimum number tokens generate (optional). .seed Random seed deterministic outputs (optional). .stop Stop generation specific tokens strings (optional). .dry_run Logical; TRUE, returns prepared request without executing (default: FALSE). .overwrite Logical; TRUE, allows overwriting existing custom IDs (default: FALSE). .max_tries Maximum retry attempts requests (default: 3). .timeout Request timeout seconds (default: 60). .id_prefix Prefix generating custom IDs (default: \"tidyllm_mistral_req_\").","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_mistral_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a Batch of Requests to the Mistral API — send_mistral_batch","text":"prepared_llms list batch_id attribute attached.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_ollama_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a Batch of Messages to Ollama API — send_ollama_batch","title":"Send a Batch of Messages to Ollama API — send_ollama_batch","text":"function creates submits batch messages Ollama API Contrary batch functions, functions waits batch finish receives requests. advantage compared sending single messages via chat() Ollama handles large parallel requests quicker many individual chat requests.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_ollama_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a Batch of Messages to Ollama API — send_ollama_batch","text":"","code":"send_ollama_batch(   .llms,   .model = \"gemma2\",   .stream = FALSE,   .seed = NULL,   .json_schema = NULL,   .temperature = NULL,   .num_ctx = 2048,   .num_predict = NULL,   .top_k = NULL,   .top_p = NULL,   .min_p = NULL,   .mirostat = NULL,   .mirostat_eta = NULL,   .mirostat_tau = NULL,   .repeat_last_n = NULL,   .repeat_penalty = NULL,   .tfs_z = NULL,   .stop = NULL,   .ollama_server = \"http://localhost:11434\",   .timeout = 120,   .keep_alive = NULL,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/send_ollama_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a Batch of Messages to Ollama API — send_ollama_batch","text":".llms list LLMMessage objects containing conversation histories. .model Character string specifying Ollama model use (default: \"gemma2\") .stream Logical; whether stream response (default: FALSE) .seed Integer; seed reproducible generation (default: NULL) .json_schema JSON schema object R list enforce output structure (default: NULL) .temperature Float 0-2; controls randomness responses (default: NULL) .num_ctx Integer; sets context window size (default: 2048) .num_predict Integer; maximum number tokens predict (default: NULL) .top_k Integer; controls diversity limiting top tokens considered (default: NULL) .top_p Float 0-1; nucleus sampling threshold (default: NULL) .min_p Float 0-1; minimum probability threshold (default: NULL) .mirostat Integer (0,1,2); enables Mirostat sampling algorithm (default: NULL) .mirostat_eta Float; Mirostat learning rate (default: NULL) .mirostat_tau Float; Mirostat target entropy (default: NULL) .repeat_last_n Integer; tokens look back repetition (default: NULL) .repeat_penalty Float; penalty repeated tokens (default: NULL) .tfs_z Float; tail free sampling parameter (default: NULL) .stop Character; custom stop sequence(s) (default: NULL) .ollama_server String; Ollama API endpoint (default: \"http://localhost:11434\") .timeout Integer; API request timeout seconds (default: 120) .keep_alive Character; long ollama model kept memory request (default: NULL - 5 Minutes) .dry_run Logical; TRUE, returns request object without execution (default: FALSE)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_ollama_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a Batch of Messages to Ollama API — send_ollama_batch","text":"list updated LLMMessage objects, assistant's response added successful.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_ollama_batch.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Send a Batch of Messages to Ollama API — send_ollama_batch","text":"function provides extensive control generation process various parameters: Temperature (0-2): Higher values increase creativity, lower values make responses focused Top-k/Top-p: Control diversity generated text Mirostat: Advanced sampling algorithm maintaining consistent complexity Repeat penalties: Prevent repetitive text Context window: Control much previous conversation considered","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_openai_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","title":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","text":"function creates submits batch messages OpenAI Batch API asynchronous processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_openai_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","text":"","code":"send_openai_batch(   .llms,   .model = \"gpt-4o\",   .max_completion_tokens = NULL,   .reasoning_effort = NULL,   .frequency_penalty = NULL,   .logit_bias = NULL,   .presence_penalty = NULL,   .seed = NULL,   .stop = NULL,   .temperature = NULL,   .top_p = NULL,   .logprobs = NULL,   .top_logprobs = NULL,   .dry_run = FALSE,   .overwrite = FALSE,   .json_schema = NULL,   .max_tries = 3,   .timeout = 60,   .verbose = FALSE,   .id_prefix = \"tidyllm_openai_req_\" )"},{"path":"https://edubruell.github.io/tidyllm/reference/send_openai_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","text":".llms list LLMMessage objects containing conversation histories. .model Character string specifying OpenAI model version (default: \"gpt-4o\"). .max_completion_tokens Integer specifying maximum tokens per response (default: NULL). .reasoning_effort long reasoning models reason (can either \"low\",\"medium\" \"high\") .frequency_penalty Number -2.0 2.0. Positive values penalize new tokens based existing frequency text far. .logit_bias named list modifying likelihood specified tokens appearing completion. .presence_penalty Number -2.0 2.0. Positive values penalize new tokens based whether appear text far. .seed specified, system make best effort sample deterministically. .stop 4 sequences API stop generating tokens. .temperature sampling temperature use, 0 2. Higher values make output random. .top_p alternative sampling temperature, called nucleus sampling. .logprobs TRUE, get log probabilities output token (default: NULL). .top_logprobs specified, get top N log probabilities output token (0-5, default: NULL). .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .overwrite Logical; TRUE, allows overwriting existing batch ID associated request (default: FALSE). .json_schema JSON schema object provided tidyllm_schema ellmer schemata (default: NULL). .max_tries Maximum number retries perform request (default: 3). .timeout Integer specifying request timeout seconds (default: 60). .verbose Logical; TRUE, additional info requests printed (default: FALSE). .id_prefix Character string specify prefix generating custom IDs names .llms missing (default: \"tidyllm_openai_req_\").","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_openai_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","text":"updated named list .llms identifiers align batch responses, including batch_id attribute.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","title":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","text":"tidy interface integrating large language model (LLM) APIs 'Claude', 'Openai', 'Groq','Mistral' local models via 'Ollama' R workflows. package supports text media-based interactions, interactive message history, batch request APIs, tidy, pipeline-oriented interface streamlined integration data workflows. Web services available https://www.anthropic.com, https://openai.com, https://groq.com, https://mistral.ai/ https://ollama.com.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","text":"Maintainer: Eduard Brüll eduard.bruell@zew.de contributors: Jia Zhang [contributor]","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a JSON Schema for Structured Outputs — tidyllm_schema","title":"Create a JSON Schema for Structured Outputs — tidyllm_schema","text":"function creates JSON schema structured outputs, supporting character-based shorthand S7 tidyllm_field objects. also integrates ellmer types like ellmer::type_string() ellmer namespace","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a JSON Schema for Structured Outputs — tidyllm_schema","text":"","code":"tidyllm_schema(name = \"tidyllm_schema\", ...)"},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a JSON Schema for Structured Outputs — tidyllm_schema","text":"name character string specifying schema name (default: \"tidyllm_schema\"). ... Named arguments name represents field, value either character string, tidyllm_field, ellmer type. Supported character shorthand types: \"character\" \"string\" character fields \"logical\" boolean fields \"numeric\" number fields \"factor(...)\" enumerations Use [] indicate vectors, e.g., \"character[]\"","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a JSON Schema for Structured Outputs — tidyllm_schema","text":"list representing JSON schema, suitable use .json_schema LLM API calls.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a JSON Schema for Structured Outputs — tidyllm_schema","text":"","code":"if (FALSE) { # \\dontrun{ # Example using different field types address_schema <- tidyllm_schema(   name = \"AddressSchema\",   Street = field_chr(\"A common street name\"),   house_number = field_dbl(),   City = field_chr(\"Name of a city\"),   State = field_fct(\"State abbreviation\", .levels = c(\"CA\", \"TX\", \"Other\")),   Country = \"string\",   PostalCode = \"string\" )  llm_message(\"Imagine an address\") |> chat(openai, .json_schema = address_schema)  # Example with vector field tidyllm_schema(   plz = field_dbl(.vector = TRUE) ) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_tool.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Tool Definition for tidyllm — tidyllm_tool","title":"Create a Tool Definition for tidyllm — tidyllm_tool","text":"Creates tool definition use Language Model API calls support function calling. function wraps existing R function schema information LLM interaction.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_tool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Tool Definition for tidyllm — tidyllm_tool","text":"","code":"tidyllm_tool(.f, .description = character(0), ...)"},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_tool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Tool Definition for tidyllm — tidyllm_tool","text":".f function wrap tool .description Character string describing tool ... Named arguments providing schema definitions function parameter using tidyllm_fields","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_tool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Tool Definition for tidyllm — tidyllm_tool","text":"TOOL class object can used tidyllm chat() functions","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_tool.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Tool Definition for tidyllm — tidyllm_tool","text":"parameter schema ... correspond parameter wrapped function. required function parameters must corresponding schema definitions.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_tool.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Tool Definition for tidyllm — tidyllm_tool","text":"","code":"get_weather <- function(location){} weather_tool <- tidyllm_tool(   get_weather,   \"Get the current weather in a given location\",   location = field_chr(\"The city and state, e.g., San Francisco, CA\") )"},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"tool-usage-introduced-to-tidyllm-0-3-2","dir":"Changelog","previous_headings":"","what":"Tool usage introduced to tidyllm","title":"Dev-Version 0.3.2","text":"first tool usage system inspired similar system ellmer introduced tidyllm. moment tool use available openai() mistral() graudally extended API providers support : can use tidyllm_tool() function run make functions available large language model. model can run functions current session, needed chat request.","code":"get_current_time <- function(tz, format = \"%Y-%m-%d %H:%M:%S\") {   format(Sys.time(), tz = tz, format = format, usetz = TRUE) }  time_tool <- tidyllm_tool(   .f = get_current_time,   .description = \"Returns the current time in a specified timezone. Use this to determine the current time in any location.\",   tz = field_chr(\"The time zone identifier (e.g., 'Europe/Berlin', 'America/New_York', 'Asia/Tokyo', 'UTC'). Required.\"),   format = field_chr(\"Format string for the time output. Default is '%Y-%m-%d %H:%M:%S'.\") )   llm_message(\"What's the exact time in Stuttgart?\") |>   chat(openai,.tools=time_tool)    #> Message History: #> system: #> You are a helpful assistant #> -------------------------------------------------------------- #> user: #> What's the exact time in Stuttgart? #> -------------------------------------------------------------- #> assistant: #> The current time in Stuttgart (Europe/Berlin timezone) is #> 2025-03-03 09:51:22 CET. #> --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"version-031","dir":"Changelog","previous_headings":"","what":"Version 0.3.1","title":"Version 0.3.1","text":"CRAN release: 2025-02-24 ⚠️ bad bug latest CRAN release fetch_openai_batch() function now fixed latest Github version. CRAN version fetch_openai_batch() function throws errors logprobs turned .","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"changes-compared-to-last-release-0-3-1","dir":"Changelog","previous_headings":"","what":"Changes compared to last release","title":"Version 0.3.1","text":"New schema field functions inspired ellmer schema compatibility ellmer: ellmer installed can now directly use ellmer type objects .json_schema option api-functions. Moreover, tidyllm_schema() now accepts ellmer types field definitions. addition four ellmer-inspired type-definition functionsfield_chr(), field_dbl(), field_lgl() field_fct() added allow set description fields schemata Support logprobs openai_chat() send_openai_batch() new get_logprobs() function: Bugfix OpenAI metadata extraction New ollama_delete_model() function list_models() now verb supporting providers. New synchronous send_ollama_batch() function make use fast parallel request features Ollama. New batch functions Azure Openai (thanks Jia Zhang) New parameters openai() reasoning models supported Default models updated perplexity() gemini() Fixed bug print method LLMMessage","code":"ellmer_adress <-ellmer::type_object(     street = ellmer::type_string(\"A famous street\"),     houseNumber = ellmer::type_number(\"a 3 digit number\"),     postcode = ellmer::type_string(),     city = ellmer::type_string(\"A large city\"),     region = ellmer::type_string(),     country = ellmer::type_enum(values = c(\"Germany\", \"France\"))   )   person_schema <-  tidyllm_schema(                 person_name = \"string\",                 age = field_dbl(\"An age between 25 and 40\"),                 is_employed = field_lgl(\"Employment Status in the last year\")                 occupation = field_fct(.levels=c(\"Lawyer\",\"Butcher\")),                 address = ellmer_adress                 )  address_message <- llm_message(\"imagine an address\") |>   chat(openai,.json_schema = ellmer_adress)    person_message  <- llm_message(\"imagine a person profile\") |>   chat(openai,.json_schema = person_schema) badger_poem <- llm_message(\"Write a haiku about badgers\") |>     chat(openai(.logprobs=TRUE,.top_logprobs=5))   badger_poem |> get_logprobs() #> # A tibble: 19 × 5 #>   reply_index token          logprob bytes     top_logprobs #>          <int> <chr>            <dbl> <list>    <list>       #>  1           1 \"In\"       -0.491      <int [2]> <list [5]>   #>  2           1 \" moon\"    -1.12       <int [5]> <list [5]>   #>  3           1 \"lit\"      -0.00489    <int [3]> <list [5]>   #>  4           1 \" forest\"  -1.18       <int [7]> <list [5]>   #>  5           1 \",\"        -0.00532    <int [1]> <list [5]> list_models(openai) #> # A tibble: 52 × 3 #>    id                                   created             owned_by #>    <chr>                                <chr>               <chr>    #>  1 gpt-4o-mini-audio-preview-2024-12-17 2024-12-13 18:52:00 system   #>  2 gpt-4-turbo-2024-04-09               2024-04-08 18:41:17 system   #>  3 dall-e-3                             2023-10-31 20:46:29 system   #>  4 dall-e-2                             2023-11-01 00:22:57 system"},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"version-030","dir":"Changelog","previous_headings":"","what":"Version 0.3.0","title":"Version 0.3.0","text":"CRAN release: 2024-12-08 tidyllm 0.3.0 represents major milestone tidyllm largest changes compared 0.2.0 :","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"new-verb-based-interface-0-3-0","dir":"Changelog","previous_headings":"","what":"New Verb-Based Interface","title":"Version 0.3.0","text":"Verbs (e.g., chat(), embed(), send_batch()) define type action want perform. Providers (e.g., openai(), claude(), ollama()) arguement verbs specify API handle action take provider-specific arguments verb provider combination routes interaction provider-specific functions like openai_chat() claude_chat() work background. functions can also called directly alternative verbose provider-specific interface.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"old-usage-0-3-0","dir":"Changelog","previous_headings":"New Verb-Based Interface","what":"Old Usage:","title":"Version 0.3.0","text":"","code":"llm_message(\"Hello World\") |>   openai(.model = \"gpt-4o\")"},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"new-usage-0-3-0","dir":"Changelog","previous_headings":"New Verb-Based Interface","what":"New Usage:","title":"Version 0.3.0","text":"","code":"# Recommended Verb-Based Approach llm_message(\"Hello World\") |>   chat(openai(.model = \"gpt-4o\"))    # Or even configuring a provider outside my_ollama <- ollama(.model = \"llama3.2-vision:90B\",        .ollama_server = \"https://ollama.example-server.de\",        .temperature = 0)  llm_message(\"Hello World\") |>   chat(my_ollama)  # Alternative Approach is to use more verbose specific functions: llm_message(\"Hello World\") |>   openai_chat(.model = \"gpt-4o\")"},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"backward-compatibility-0-3-0","dir":"Changelog","previous_headings":"New Verb-Based Interface","what":"Backward Compatibility:","title":"Version 0.3.0","text":"old functions (openai(), claude(), etc.) still work directly supply LLMMessage arguement, issue deprecation warnings used directly chat. Users encouraged transition new interface future-proof workflows.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-3-0","dir":"Changelog","previous_headings":"","what":"Breaking Changes:","title":"Version 0.3.0","text":"output format embedding APIs changed matrix tibble input column list column containing one embedding vector one input per row. R6-based saved LLMMessage objects longer compatible new version. Saved objects earlier versions need re-created","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"other-major-features-0-3-0","dir":"Changelog","previous_headings":"","what":"Other Major Features:","title":"Version 0.3.0","text":"gemini() perplexity() new supported API providers. gemini() brings interesting Video Audio features well search grounding tidyllm. perplexity() also offers well cited search grounded assitant replies Batch-Processing mistral() New Metadata-Extraction function get_reply_metadata() get information token usage, relevant metadata (like sources used grounding)","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-3-0","dir":"Changelog","previous_headings":"","what":"Improvements:","title":"Version 0.3.0","text":"Transitioned R6 S7 main LLMMessage class, improving maintainability, interoperability, future-proofing. Consolidated API-specific functionality dedicated files","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-2-7","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.2.7","text":"Batch API functions Mistral API Search Grounding .grounding_threshold argument added gemini_chat() function allowing use Google searches ground model responses search result Gemini models. example, asking maintainer obscure R package works grounding lead hallucination without: Perplexity additional API provider available perplexity_chat(). neat feature perplexity --date web search detailed citations. Cited sources available api_specific-list column get_metadata() .json_schema support ollama() available Ollama 0.5.0","code":"llm_message(\"What is tidyllm and who maintains this package?\") |>   gemini_chat(.grounding_threshold = 0.3)"},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-2-7","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.2.7","text":"Metadata extraction now handled api-specific methods. get_metadata() returns list column API-specific metadata","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"large-refactor-of-package-internals-0-2-6","dir":"Changelog","previous_headings":"","what":"Large Refactor of package internals","title":"Version 0.2.6","text":"Switch R6 S7 main LLMMessage class Several bug-fixes df_llm_message() API formatting methods now code files API providers Rate-limit header extraction tracking streaming callback generation now methods APIProvider classes api-specific code now api_openai.R,api_gemini.R,etc. files Support as_tibble() S3 Generic LLMMessage Rate limit tracking output verbose mode API-functions moved single function track_rate_limit() Unnecessary .onattach() removed Bugfix callback method Gemini streaming responses (still ideal, works) Embedding functions refactored reduce repeated code API-key check moved API-object method Slight refactoring batch functions (still quite bit potential reduce duplication)","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-2-6","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.2.6","text":"Old R6-based LLMMessage-objects compatible new version anymore! also applies saved objects, like lists batch files.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"minor-features-0-2-6","dir":"Changelog","previous_headings":"","what":"Minor Features","title":"Version 0.2.6","text":"Google Gemini now supports working multiple files one message file upload functionality","code":"here::here(\"local_wip\",\"example.mp3\") |> gemini_upload_file() here::here(\"local_wip\",\"legrille.mp4\") |> gemini_upload_file()  file_tibble <- gemini_list_files()  llm_message(\"What are these two files about?\") |>   gemini_chat(.fileid=file_tibble$name)"},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-2-5","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.2.5","text":"Better embedding functions improved output error handling new documentation. New article using embeddings tidyllm. Support embedding models azure azure_openai_embedding()","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-2-5","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.2.5","text":"output format embed() related API-specific functions changed matrix tibble input column list column containing one embedding vector one input per row.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"refinements-of-the-new-interface-0-2-4","dir":"Changelog","previous_headings":"","what":"Refinements of the new interface","title":"Version 0.2.4","text":"One disadvantage first iteration new interface arguements needed passed provider-specific functions, going provider function. feels, unintuitive, users expect common arguments (e.g., .model, .temperature) set directly main verbs like chat() send_batch().Moreover, provider functions don’t expose arguments autocomplete, making harder users explore options. Therefore, main API verbs now directly accept common arguements, check available arguements API.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"bug-fixes-0-2-4","dir":"Changelog","previous_headings":"","what":"Bug-fixes","title":"Version 0.2.4","text":"New error message setting provider main verbs Missing export main verbs fixed Wrong documentation fixed","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-interface-overhaul-0-2-3","dir":"Changelog","previous_headings":"","what":"Major Interface Overhaul","title":"Version 0.2.3","text":"tidyllm introduced verb-based interface overhaul provide intuitive flexible user experience. Previously, provider-specific functions like claude(), openai(), others directly used chat-based workflows. Now, functions primarily serve provider configuration general verbs like chat().","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"key-changes-0-2-3","dir":"Changelog","previous_headings":"Major Interface Overhaul","what":"Key Changes:","title":"Version 0.2.3","text":"Verbs (e.g., chat(), embed(), send_batch()) define type action want perform. Providers (e.g., openai(), claude(), ollama()) arguement verbs specify API handle action take provider-specific arguments verb provider combination routes interaction provider-specific functions like openai_chat() claude_chat() work background. functions can also called directly alternative verbose provider-specific interface.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"old-usage-0-2-3","dir":"Changelog","previous_headings":"Major Interface Overhaul","what":"Old Usage:","title":"Version 0.2.3","text":"","code":"llm_message(\"Hello World\") |>   openai(.model = \"gpt-4o\")"},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"new-usage-0-2-3","dir":"Changelog","previous_headings":"Major Interface Overhaul","what":"New Usage:","title":"Version 0.2.3","text":"old functions (openai(), claude(), etc.) still work directly supply LLMMessage arguement, issue deprecation warnings used directly chat. Users encouraged transition new interface future-proof workflows.","code":"# Recommended Verb-Based Approach llm_message(\"Hello World\") |>   chat(openai(.model = \"gpt-4o\"))    # Or even configuring a provider outside my_ollama <- ollama(.model = \"llama3.2-vision:90B\",        .ollama_server = \"https://ollama.example-server.de\",        .temperature = 0)  llm_message(\"Hello World\") |>   chat(my_ollama)  # Alternative Approach is to use more verbose specific functions: llm_message(\"Hello World\") |>   openai_chat(.model = \"gpt-4o\")"},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-2-2","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.2.2","text":"Added functions work Google Gemini API, new gemini() main API-function Support file upload workflows Gemini: Brings video audio support tidyllm Google Gemini second API fully support tidyllm_schema() gemini()-requests allow wide range file types can used context messages PDF: application/pdf TXT: text/plain HTML: text/html CSS: text/css Markdown: text/md CSV: text/csv XML: text/xml RTF: text/rtf JavaScript: application/x-javascript, text/javascript Python: application/x-python, text/x-python PNG: image/png JPEG: image/jpeg WEBP: image/webp HEIC: image/heic HEIF: image/heif MP4: video/mp4 MPEG: video/mpeg MOV: video/mov AVI: video/avi FLV: video/x-flv MPG: video/mpg WEBM: video/webm WMV: video/wmv 3GPP: video/3gpp WAV: audio/wav MP3: audio/mp3 AIFF: audio/aiff AAC: audio/aac OGG Vorbis: audio/ogg FLAC: audio/flac","code":"#Upload a file for use with gemini upload_info <- gemini_upload_file(\"example.mp3\")  #Make the file available during a Gemini API call llm_message(\"Summarize this speech\") |>   gemini(.fileid = upload_info$name)    #Delte the file from the Google servers gemini_delete_file(upload_info$name)"},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-2-1","dir":"Changelog","previous_headings":"","what":"Major Features:","title":"Version 0.2.1","text":"Added get_metadata() function retrieve format metadata LLMMessage objects. Enhanced print method LLMMessage support printing metadata, controlled via new tidyllm_print_metadata option new .meta-arguement print method.","code":"conversation <- llm_message(\"Write a short poem about software development\") |>   claude()    #Get metdata on token usage and model as tibble   get_metadata(conversation)  #or print it with the message print(conversation,.meta=TRUE)  #Or allways print it options(tidyllm_print_metadata=TRUE)"},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"bug-fixes-0-2-1","dir":"Changelog","previous_headings":"","what":"Bug-fixes:","title":"Version 0.2.1","text":"Fixed bug send_openai_batch() caused missing .json-arguement passed messages without schema","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"version-020","dir":"Changelog","previous_headings":"","what":"Version 0.2.0","title":"Version 0.2.0","text":"CRAN release: 2024-11-07 New CRAN release. Largest changes compared 0.1.0: Major Features: Batch Request Support: Added support batch requests Anthropic OpenAI APIs, enabling large-scale request handling. Schema Support: Improved structured outputs JSON mode advanced .json_schema handling openai(), enhancing support well-defined JSON responses. Azure OpenAI Integration: Introduced azure_openai() function accessing Azure OpenAI service, full support rate-limiting batch operations tailored Azure’s API structure. Embedding Model Support: Added embedding generation functions OpenAI, Ollama, Mistral APIs, supporting message content media embedding. Mistral API Integration: New mistral() function provides full support Mistral models hosted EU, including rate-limiting streaming capabilities. PDF Batch Processing: Introduced pdf_page_batch() function, processes PDFs page page, allowing users define page-specific prompts detailed analysis. Support OpenAI-compatible APIs: Introduced .compatible argument (flexible url path) openai() allow compatibility third-party OpenAI-compatible APIs. Improvements: API Format Refactoring: Complete refactor to_api_format() reduce code duplication, simplify API format generation, improve maintainability. Improved Error Handling: Enhanced input validation error messaging API-functions functions, making troubleshooting easier. Rate-Limiting Enhancements: Updated rate limiting use httr2::req_retry() addition rate-limit tracking functions tidyllm, using 429 headers wait rate limit resets. Expanded Testing: Added comprehensive tests API functions using httptest2 Breaking Changes: Redesigned Reply Functions: get_reply() split get_reply() text outputs get_reply_data() structured outputs, improving type stability compared earlier function different outputs based .json-arguement. Deprecation chatgpt(): chatgpt() function deprecated favor openai() feature alignment improved consistency. Minor Updates Bug Fixes: Expanded PDF Support llm_message(): Allows extraction specific page ranges PDFs, improving flexibility document handling. New ollama_download_model() function download models Ollama API sequential chat API functions now support streaming","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-11","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.11","text":"Support Anthropic OpenAI batch request API added New .compatible-arguement openai() allow working compatible third party APIs","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-11","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.11","text":"Complete refactor to_api_format(): API format generation now much less code duplication maintainable.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-1-10","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.1.10","text":"get_reply() split two type-stable functions: get_reply() text get_reply_data() structured outputs.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-10","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.10","text":"Rate limiting updated use httr2::req_retry(): Rate limiting now uses right 429 headers come.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-9","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.9","text":"Enhanced Input Validation: API functions now improved input validation, ensuring better alignment API documentation Improved error handling human-readable error messages failed requests API Advanced JSON Mode openai(): openai() function now supports advanced .json_schemas, allowing structured output JSON mode precise responses. Reasoning Models Support: Support O1 reasoning models added, better handling system prompts openai() function. Streaming callback functions refactored: Given streaming callback format Open AI, Mistral Groq nearly identical three now rely callback function.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-1-9","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.1.9","text":"chatgpt() Deprecated: chatgpt() function deprecated favor openai(). Users migrate openai() take advantage new features enhancements.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-9","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.9","text":"Better Error Handling: openai(), ollama(), claude() functions now return informative error messages API calls fail, helping debugging troubleshooting.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-8","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.8","text":"Embedding functions process message histories combine text message content media attachments embedding models. ollama_embedding() generate embeddings using Ollama API. openai_embedding() generate embeddings using OpenAI API. mistral_embedding() generate embeddings using Mistral API.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-8","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.8","text":"PDF Page Support llm_message(): llm_message() function now supports specifying range pages PDF passing list filename, start_page, end_page. allows users extract process specific pages PDF.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-7","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.7","text":"PDF Page Batch Processing: Introduced pdf_page_batch() function, processes PDF files page page, extracting text converting page image, allowing general prompt page-specific prompts. function generates list LLMMessage objects can sent API work batch-API functions tidyllm.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-6","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.6","text":"Support Mistral API: New mistral() function use Mistral Models Le Platforme servers hosted EU, rate-limiting streaming support.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-5","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.5","text":"last_user_message() pulls last message user sent. get_reply() gets assistant reply given index assistant messages. get_user_message() gets user message given index user messages.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-5","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.5","text":"Easier Troubleshooting API-function: API functions now support .dry_run argument, allowing users generate httr2-request easier debugging inspection. API Function Tests: Implemented httptest2-based tests mock responses API functions, covering basic functionality rate-limiting.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-4","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.4","text":"Model Download: Introduced ollama_download_model() function download models Ollama API. supports streaming mode provides live progress bar updates download progress.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-4","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.4","text":"Refactoring llm_message()","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-3","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.3","text":"groq() function now supports images. complete streaming support across API-functions.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-1-3","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.1.3","text":"Groq Models: System prompts longer sent Groq models, since many models Groq support multimodal models Groq disallow .","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-2","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.2","text":"New unit tests llm_message(). Improvements streaming functions.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-1","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.1","text":"JSON Mode: JSON mode now widely supported across API functions, allowing structured outputs APIs support . .json argument now passed API functions, specifying API respond, needed anymore last_reply(). Improved last_reply() Behavior: behavior last_reply() function changed. now automatically handles JSON replies parsing structured data falling back raw text case errors. can still force raw text replies even JSON output using .raw argument.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.1.1","text":"last_reply(): .json argument longer used, JSON replies automatically parsed. Use .raw force raw text replies.","code":""}]

<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Changelog • tidyllm</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Changelog"><meta property="og:image" content="https://edubruell.github.io/tidyllm/logo.png"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyllm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/tidyllm.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/tidyllm_classifiers.html">Classifying Texts with tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm_embed.html">Embedding Models in tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm_video.html">Video and Audio Data with the Gemini API</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm-pdfquestions.html">Structured Question Answering from PDFs</a></li>
  </ul></li>
<li class="active nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/edubruell/tidyllm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-news">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Changelog</h1>
      <small>Source: <a href="https://github.com/edubruell/tidyllm/blob/HEAD/NEWS.md" class="external-link"><code>NEWS.md</code></a></small>
    </div>

    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.3.2" id="dev-version-032">Dev-Version 0.3.2<a class="anchor" aria-label="anchor" href="#dev-version-032"></a></h2>
<div class="section level3">
<h3 id="tool-usage-introduced-to-tidyllm-0-3-2">Tool usage introduced to tidyllm<a class="anchor" aria-label="anchor" href="#tool-usage-introduced-to-tidyllm-0-3-2"></a></h3>
<p>A first tool usage system inspired by a similar system in <code>ellmer</code> has been introduced to tidyllm. At the moment tool use is only available for <code><a href="../reference/claude.html">claude()</a></code>, <code><a href="../reference/openai.html">openai()</a></code>, <code><a href="../reference/mistral.html">mistral()</a></code>, <code><a href="../reference/ollama.html">ollama()</a></code> and <code><a href="../reference/groq.html">groq()</a></code> but will be gradually extended to the other API providers that support it:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">get_current_time</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">tz</span>, <span class="va">format</span> <span class="op">=</span> <span class="st">"%Y-%m-%d %H:%M:%S"</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/format.html" class="external-link">format</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Sys.time.html" class="external-link">Sys.time</a></span><span class="op">(</span><span class="op">)</span>, tz <span class="op">=</span> <span class="va">tz</span>, format <span class="op">=</span> <span class="va">format</span>, usetz <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">time_tool</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tidyllm_tool.html">tidyllm_tool</a></span><span class="op">(</span></span>
<span>  .f <span class="op">=</span> <span class="va">get_current_time</span>,</span>
<span>  .description <span class="op">=</span> <span class="st">"Returns the current time in a specified timezone. Use this to determine the current time in any location."</span>,</span>
<span>  tz <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"The time zone identifier (e.g., 'Europe/Berlin', 'America/New_York', 'Asia/Tokyo', 'UTC'). Required."</span><span class="op">)</span>,</span>
<span>  format <span class="op">=</span> <span class="fu"><a href="../reference/field_chr.html">field_chr</a></span><span class="op">(</span><span class="st">"Format string for the time output. Default is '%Y-%m-%d %H:%M:%S'."</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What's the exact time in Stuttgart?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">openai</span>,.tools<span class="op">=</span><span class="va">time_tool</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co">#&gt; Message History:</span></span>
<span><span class="co">#&gt; system:</span></span>
<span><span class="co">#&gt; You are a helpful assistant</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------</span></span>
<span><span class="co">#&gt; user:</span></span>
<span><span class="co">#&gt; What's the exact time in Stuttgart?</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------</span></span>
<span><span class="co">#&gt; assistant:</span></span>
<span><span class="co">#&gt; The current time in Stuttgart (Europe/Berlin timezone) is</span></span>
<span><span class="co">#&gt; 2025-03-03 09:51:22 CET.</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------  </span></span></code></pre></div>
<p>You can use the <code><a href="../reference/tidyllm_tool.html">tidyllm_tool()</a></code> function to run to make functions available to a large language model. The model can then run these functions in your current session, if they are needed for a chat request.</p>
</div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.3.1" id="version-031">Version 0.3.1<a class="anchor" aria-label="anchor" href="#version-031"></a></h2><p class="text-muted">CRAN release: 2025-02-24</p>
<p>⚠️ There is a bad bug in the latest CRAN release in the <code><a href="../reference/fetch_openai_batch.html">fetch_openai_batch()</a></code> function that is now fixed in the latest Github version. For the CRAN version the <code><a href="../reference/fetch_openai_batch.html">fetch_openai_batch()</a></code> function throws errors if the logprobs are turned off.</p>
<div class="section level3">
<h3 id="changes-compared-to-last-release-0-3-1">Changes compared to last release<a class="anchor" aria-label="anchor" href="#changes-compared-to-last-release-0-3-1"></a></h3>
<ul><li>
<strong>New schema field functions inspired by ellmer and more schema compatibility with ellmer:</strong> If you have ellmer installed you can now directly use ellmer type objects in the <code>.json_schema</code> option of api-functions. Moreover, <code><a href="../reference/tidyllm_schema.html">tidyllm_schema()</a></code> now accepts ellmer types as field definitions. In addition four ellmer-inspired type-definition functions<code><a href="../reference/field_chr.html">field_chr()</a></code>, <code><a href="../reference/field_chr.html">field_dbl()</a></code>, <code><a href="../reference/field_chr.html">field_lgl()</a></code> and <code><a href="../reference/field_chr.html">field_fct()</a></code> were added that allow you to set description fields in schemata</li>
</ul><div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a> ellmer_adress <span class="ot">&lt;-</span>ellmer<span class="sc">::</span><span class="fu">type_object</span>(</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    <span class="at">street =</span> ellmer<span class="sc">::</span><span class="fu">type_string</span>(<span class="st">"A famous street"</span>),</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>    <span class="at">houseNumber =</span> ellmer<span class="sc">::</span><span class="fu">type_number</span>(<span class="st">"a 3 digit number"</span>),</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>    <span class="at">postcode =</span> ellmer<span class="sc">::</span><span class="fu">type_string</span>(),</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>    <span class="at">city =</span> ellmer<span class="sc">::</span><span class="fu">type_string</span>(<span class="st">"A large city"</span>),</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>    <span class="at">region =</span> ellmer<span class="sc">::</span><span class="fu">type_string</span>(),</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>    <span class="at">country =</span> ellmer<span class="sc">::</span><span class="fu">type_enum</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"Germany"</span>, <span class="st">"France"</span>))</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>  ) </span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>person_schema <span class="ot">&lt;-</span>  <span class="fu">tidyllm_schema</span>(</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>                <span class="at">person_name =</span> <span class="st">"string"</span>,</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>                <span class="at">age =</span> <span class="fu">field_dbl</span>(<span class="st">"An age between 25 and 40"</span>),</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>                <span class="at">is_employed =</span> <span class="fu">field_lgl</span>(<span class="st">"Employment Status in the last year"</span>)</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>                <span class="at">occupation =</span> <span class="fu">field_fct</span>(<span class="at">.levels=</span><span class="fu">c</span>(<span class="st">"Lawyer"</span>,<span class="st">"Butcher"</span>)),</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>                <span class="at">address =</span> ellmer_adress</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>                )</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>address_message <span class="ot">&lt;-</span> <span class="fu">llm_message</span>(<span class="st">"imagine an address"</span>) <span class="sc">|&gt;</span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>  <span class="fu">chat</span>(openai,<span class="at">.json_schema =</span> ellmer_adress)</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>  </span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>person_message  <span class="ot">&lt;-</span> <span class="fu">llm_message</span>(<span class="st">"imagine a person profile"</span>) <span class="sc">|&gt;</span></span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>  <span class="fu">chat</span>(openai,<span class="at">.json_schema =</span> person_schema)</span></code></pre></div>
<ul><li>Support for logprobs in <code><a href="../reference/openai_chat.html">openai_chat()</a></code> and <code><a href="../reference/send_openai_batch.html">send_openai_batch()</a></code> and new <code><a href="../reference/get_logprobs.html">get_logprobs()</a></code> function:</li>
</ul><div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">badger_poem</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Write a haiku about badgers"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.logprobs<span class="op">=</span><span class="cn">TRUE</span>,.top_logprobs<span class="op">=</span><span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span> <span class="va">badger_poem</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/get_logprobs.html">get_logprobs</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 19 × 5</span></span>
<span><span class="co">#&gt;   reply_index token          logprob bytes     top_logprobs</span></span>
<span><span class="co">#&gt;          &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;      </span></span>
<span><span class="co">#&gt;  1           1 "In"       -0.491      &lt;int [2]&gt; &lt;list [5]&gt;  </span></span>
<span><span class="co">#&gt;  2           1 " moon"    -1.12       &lt;int [5]&gt; &lt;list [5]&gt;  </span></span>
<span><span class="co">#&gt;  3           1 "lit"      -0.00489    &lt;int [3]&gt; &lt;list [5]&gt;  </span></span>
<span><span class="co">#&gt;  4           1 " forest"  -1.18       &lt;int [7]&gt; &lt;list [5]&gt;  </span></span>
<span><span class="co">#&gt;  5           1 ","        -0.00532    &lt;int [1]&gt; &lt;list [5]&gt;  </span></span></code></pre></div>
<ul><li>Bugfix in OpenAI metadata extraction</li>
<li>New <code><a href="../reference/ollama_delete_model.html">ollama_delete_model()</a></code> function</li>
<li>
<code><a href="../reference/list_models.html">list_models()</a></code> is now a verb supporting most providers.</li>
</ul><div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/list_models.html">list_models</a></span><span class="op">(</span><span class="va">openai</span><span class="op">)</span></span>
<span><span class="co">#&gt; # A tibble: 52 × 3</span></span>
<span><span class="co">#&gt;    id                                   created             owned_by</span></span>
<span><span class="co">#&gt;    &lt;chr&gt;                                &lt;chr&gt;               &lt;chr&gt;   </span></span>
<span><span class="co">#&gt;  1 gpt-4o-mini-audio-preview-2024-12-17 2024-12-13 18:52:00 system  </span></span>
<span><span class="co">#&gt;  2 gpt-4-turbo-2024-04-09               2024-04-08 18:41:17 system  </span></span>
<span><span class="co">#&gt;  3 dall-e-3                             2023-10-31 20:46:29 system  </span></span>
<span><span class="co">#&gt;  4 dall-e-2                             2023-11-01 00:22:57 system  </span></span></code></pre></div>
<ul><li>New synchronous <code><a href="../reference/send_ollama_batch.html">send_ollama_batch()</a></code> function to make use of the fast parallel request features of Ollama.</li>
<li>New batch functions for Azure Openai (thanks <a href="https://github.com/JiaZhang42" class="external-link">Jia Zhang</a>)</li>
<li>New parameters for <code><a href="../reference/openai.html">openai()</a></code> reasoning models supported</li>
<li>Default models updated for <code><a href="../reference/perplexity.html">perplexity()</a></code> and <code><a href="../reference/gemini.html">gemini()</a></code>
</li>
<li>Fixed bug in the print method of <code>LLMMessage</code>
</li>
</ul></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.3.0" id="version-030">Version 0.3.0<a class="anchor" aria-label="anchor" href="#version-030"></a></h2><p class="text-muted">CRAN release: 2024-12-08</p>
<p><strong>tidyllm 0.3.0</strong> represents a major milestone for <strong>tidyllm</strong></p>
<p>The largest changes compared to <strong>0.2.0</strong> are:</p>
<div class="section level3">
<h3 id="new-verb-based-interface-0-3-0">New Verb-Based Interface<a class="anchor" aria-label="anchor" href="#new-verb-based-interface-0-3-0"></a></h3>
<ul><li>
<strong>New Verb-Based Interface</strong>: Users can now use verbs like <code><a href="../reference/chat.html">chat()</a></code>, <code><a href="../reference/embed.html">embed()</a></code>, <code><a href="../reference/send_batch.html">send_batch()</a></code>, <code><a href="../reference/check_batch.html">check_batch()</a></code>, and <code><a href="../reference/fetch_batch.html">fetch_batch()</a></code> to interact with APIs. These functions always work with a combination of verbs and providers:
<ul><li>
<strong>Verbs</strong> (e.g., <code><a href="../reference/chat.html">chat()</a></code>, <code><a href="../reference/embed.html">embed()</a></code>, <code><a href="../reference/send_batch.html">send_batch()</a></code>) define the type of action you want to perform.</li>
<li>
<strong>Providers</strong> (e.g., <code><a href="../reference/openai.html">openai()</a></code>, <code><a href="../reference/claude.html">claude()</a></code>, <code><a href="../reference/ollama.html">ollama()</a></code>) are an arguement of verbs and specify the API to handle the action with and take provider-specific arguments</li>
</ul></li>
</ul><p>Each verb and provider combination routes the interaction to provider-specific functions like <code><a href="../reference/openai_chat.html">openai_chat()</a></code> or <code><a href="../reference/claude_chat.html">claude_chat()</a></code> that do the work in the background. These functions can also be called directly as an alternative more verbose and provider-specific interface.</p>
<div class="section level4">
<h4 id="old-usage-0-3-0">Old Usage:<a class="anchor" aria-label="anchor" href="#old-usage-0-3-0"></a></h4>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello World"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="new-usage-0-3-0">New Usage:<a class="anchor" aria-label="anchor" href="#new-usage-0-3-0"></a></h4>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Recommended Verb-Based Approach</span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello World"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co"># Or even configuring a provider outside</span></span>
<span><span class="va">my_ollama</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ollama.html">ollama</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"llama3.2-vision:90B"</span>,</span>
<span>       .ollama_server <span class="op">=</span> <span class="st">"https://ollama.example-server.de"</span>,</span>
<span>       .temperature <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello World"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">my_ollama</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Alternative Approach is to use more verbose specific functions:</span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello World"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/openai_chat.html">openai_chat</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="backward-compatibility-0-3-0">Backward Compatibility:<a class="anchor" aria-label="anchor" href="#backward-compatibility-0-3-0"></a></h4>
<ul><li>The old functions (<code><a href="../reference/openai.html">openai()</a></code>, <code><a href="../reference/claude.html">claude()</a></code>, etc.) still work if you directly supply an <code>LLMMessage</code> as arguement, but issue deprecation warnings when used directly for chat.</li>
<li>Users are encouraged to transition to the new interface for future-proof workflows.</li>
</ul></div>
</div>
<div class="section level3">
<h3 id="breaking-changes-0-3-0">Breaking Changes:<a class="anchor" aria-label="anchor" href="#breaking-changes-0-3-0"></a></h3>
<ul><li>The output format of embedding APIs was changed from a matrix to a tibble with an input column and a list column containing one embedding vector and one input per row.</li>
<li>
<code>R6</code>-based saved <code>LLMMessage</code> objects are no longer compatible with the new version. Saved objects from earlier versions need to be re-created</li>
</ul></div>
<div class="section level3">
<h3 id="other-major-features-0-3-0">Other Major Features:<a class="anchor" aria-label="anchor" href="#other-major-features-0-3-0"></a></h3>
<ul><li>
<code><a href="../reference/gemini.html">gemini()</a></code> and <code><a href="../reference/perplexity.html">perplexity()</a></code> as new supported API providers. <code><a href="../reference/gemini.html">gemini()</a></code> brings interesting Video and Audio features as well as search grounding to <strong>tidyllm</strong>. <code><a href="../reference/perplexity.html">perplexity()</a></code> also offers well cited search grounded assitant replies</li>
<li>Batch-Processing for <code><a href="../reference/mistral.html">mistral()</a></code>
</li>
<li>New Metadata-Extraction function <code>get_reply_metadata()</code> to get information on token usage, or on other relevant metadata (like sources used for grounding)</li>
</ul></div>
<div class="section level3">
<h3 id="improvements-0-3-0">Improvements:<a class="anchor" aria-label="anchor" href="#improvements-0-3-0"></a></h3>
<ul><li>Refactored Package Internals:
<ul><li>Transitioned from <code>R6</code> to <code>S7</code> for the main <code>LLMMessage</code> class, improving maintainability, interoperability, and future-proofing.</li>
<li>Consolidated all API-specific functionality into dedicated files</li>
</ul></li>
</ul></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.2.7" id="version-027">Version 0.2.7<a class="anchor" aria-label="anchor" href="#version-027"></a></h2>
<div class="section level3">
<h3 id="major-features-0-2-7">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-2-7"></a></h3>
<ul><li>Batch API functions for the Mistral API</li>
<li>Search Grounding with the <code>.grounding_threshold</code> argument added of the <code><a href="../reference/gemini_chat.html">gemini_chat()</a></code> function allowing you to use Google searches to ground model responses to a search result Gemini models. For example, asking about the maintainer of an obscure R package works with grounding but does only lead to a hallucination without:</li>
</ul><div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What is tidyllm and who maintains this package?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/gemini_chat.html">gemini_chat</a></span><span class="op">(</span>.grounding_threshold <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span></span></code></pre></div>
<ul><li>Perplexity as additional API provider available through <code><a href="../reference/perplexity_chat.html">perplexity_chat()</a></code>. The neat feature of perplexity is the up-to-date web search it does with detailed citations. Cited sources are available in the <code>api_specific</code>-list column of <code><a href="../reference/get_metadata.html">get_metadata()</a></code>
</li>
<li>
<code>.json_schema</code> support for <code><a href="../reference/ollama.html">ollama()</a></code> available with Ollama 0.5.0</li>
</ul></div>
<div class="section level3">
<h3 id="improvements-0-2-7">Improvements<a class="anchor" aria-label="anchor" href="#improvements-0-2-7"></a></h3>
<ul><li>Metadata extraction is now handled by api-specific methods. <code><a href="../reference/get_metadata.html">get_metadata()</a></code> returns a list column with API-specific metadata</li>
</ul></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.2.6" id="version-026">Version 0.2.6<a class="anchor" aria-label="anchor" href="#version-026"></a></h2>
<div class="section level3">
<h3 id="large-refactor-of-package-internals-0-2-6">Large Refactor of package internals<a class="anchor" aria-label="anchor" href="#large-refactor-of-package-internals-0-2-6"></a></h3>
<ul><li>Switch from <code>R6</code> to <code>S7</code> for the main <code>LLMMessage</code> class</li>
<li>Several bug-fixes for <code><a href="../reference/df_llm_message.html">df_llm_message()</a></code>
</li>
<li>API formatting methods are now in the code files for API providers</li>
<li>Rate-limit header extraction for tracking and streaming callback generation are now methods for <code>APIProvider</code> classes</li>
<li>All api-specific code is now in the <code>api_openai.R</code>,<code>api_gemini.R</code>,etc. files</li>
<li>Support for <code>as_tibble()</code> S3 Generic for <code>LLMMessage</code>
</li>
<li>Rate limit tracking and output for verbose mode in API-functions moved to a single function <code>track_rate_limit()</code>
</li>
<li>Unnecessary <code>.onattach()</code> removed</li>
<li>Bugfix in callback method of Gemini streaming responses (still not ideal, but works)</li>
<li>Embedding functions refactored to reduce repeated code</li>
<li>API-key check moved into API-object method</li>
<li>Slight refactoring for batch functions (there is still quite a bit of potential to reduce duplication)</li>
</ul></div>
<div class="section level3">
<h3 id="breaking-changes-0-2-6">Breaking Changes<a class="anchor" aria-label="anchor" href="#breaking-changes-0-2-6"></a></h3>
<ul><li>Old <code>R6</code>-based <code>LLMMessage</code>-objects are not compatible with the new version anymore! This also applies to saved objects, like lists of batch files.</li>
</ul></div>
<div class="section level3">
<h3 id="minor-features-0-2-6">Minor Features<a class="anchor" aria-label="anchor" href="#minor-features-0-2-6"></a></h3>
<ul><li>Google Gemini now supports working with multiple files in one message for the file upload functionality</li>
</ul><div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org//reference/here.html" class="external-link">here</a></span><span class="op">(</span><span class="st">"local_wip"</span>,<span class="st">"example.mp3"</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/gemini_upload_file.html">gemini_upload_file</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org//reference/here.html" class="external-link">here</a></span><span class="op">(</span><span class="st">"local_wip"</span>,<span class="st">"legrille.mp4"</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/gemini_upload_file.html">gemini_upload_file</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">file_tibble</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/gemini_list_files.html">gemini_list_files</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What are these two files about?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/gemini_chat.html">gemini_chat</a></span><span class="op">(</span>.fileid<span class="op">=</span><span class="va">file_tibble</span><span class="op">$</span><span class="va">name</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.2.5" id="version-025">Version 0.2.5<a class="anchor" aria-label="anchor" href="#version-025"></a></h2>
<div class="section level3">
<h3 id="major-features-0-2-5">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-2-5"></a></h3>
<p>Better embedding functions with improved output and error handling and new documentation. New article on using embeddings with <strong>tidyllm</strong>. Support for embedding models on azure with <code><a href="../reference/azure_openai_embedding.html">azure_openai_embedding()</a></code></p>
</div>
<div class="section level3">
<h3 id="breaking-changes-0-2-5">Breaking Changes<a class="anchor" aria-label="anchor" href="#breaking-changes-0-2-5"></a></h3>
<ul><li>The output format of <code><a href="../reference/embed.html">embed()</a></code> and the related API-specific functions was changed from a matrix to a tibble with an input column and a list column containing one embedding vector and one input per row.</li>
</ul></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.2.4" id="version-024">Version 0.2.4<a class="anchor" aria-label="anchor" href="#version-024"></a></h2>
<div class="section level3">
<h3 id="refinements-of-the-new-interface-0-2-4">Refinements of the new interface<a class="anchor" aria-label="anchor" href="#refinements-of-the-new-interface-0-2-4"></a></h3>
<p>One disadvantage of the first iteration of the new interface was that all arguements that needed to be passed to provider-specific functions, were going through the provider function. This feels, unintuitive, because users expect common arguments (e.g., .model, .temperature) to be set directly in main verbs like <code><a href="../reference/chat.html">chat()</a></code> or <code><a href="../reference/send_batch.html">send_batch()</a></code>.Moreover, provider functions don’t expose arguments for autocomplete, making it harder for users to explore options. Therefore, the main API verbs now directly accept common arguements, and check them against the available arguements for each API.</p>
</div>
<div class="section level3">
<h3 id="bug-fixes-0-2-4">Bug-fixes<a class="anchor" aria-label="anchor" href="#bug-fixes-0-2-4"></a></h3>
<ul><li>New error message for not setting a provider in main verbs</li>
<li>Missing export of main verbs fixed</li>
<li>Wrong documentation fixed</li>
</ul></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.2.3" id="version-023">Version 0.2.3<a class="anchor" aria-label="anchor" href="#version-023"></a></h2>
<div class="section level3">
<h3 id="major-interface-overhaul-0-2-3">Major Interface Overhaul<a class="anchor" aria-label="anchor" href="#major-interface-overhaul-0-2-3"></a></h3>
<p><code>tidyllm</code> has introduced a verb-based interface overhaul to provide a more intuitive and flexible user experience. Previously, provider-specific functions like <code><a href="../reference/claude.html">claude()</a></code>, <code><a href="../reference/openai.html">openai()</a></code>, and others were directly used for chat-based workflows. Now, these functions primarily serve as provider configuration for some general verbs like <code><a href="../reference/chat.html">chat()</a></code>.</p>
<div class="section level4">
<h4 id="key-changes-0-2-3">Key Changes:<a class="anchor" aria-label="anchor" href="#key-changes-0-2-3"></a></h4>
<ul><li>
<strong>New Verb-Based Interface</strong>: Users can now use verbs like <code><a href="../reference/chat.html">chat()</a></code>, <code><a href="../reference/embed.html">embed()</a></code>, <code><a href="../reference/send_batch.html">send_batch()</a></code>, <code><a href="../reference/check_batch.html">check_batch()</a></code>, and <code><a href="../reference/fetch_batch.html">fetch_batch()</a></code> to interact with APIs. These functions always work with a combination of verbs and providers:
<ul><li>
<strong>Verbs</strong> (e.g., <code><a href="../reference/chat.html">chat()</a></code>, <code><a href="../reference/embed.html">embed()</a></code>, <code><a href="../reference/send_batch.html">send_batch()</a></code>) define the type of action you want to perform.</li>
<li>
<strong>Providers</strong> (e.g., <code><a href="../reference/openai.html">openai()</a></code>, <code><a href="../reference/claude.html">claude()</a></code>, <code><a href="../reference/ollama.html">ollama()</a></code>) are an arguement of verbs and specify the API to handle the action with and take provider-specific arguments</li>
</ul></li>
</ul><p>Each verb and provider combination routes the interaction to provider-specific functions like <code><a href="../reference/openai_chat.html">openai_chat()</a></code> or <code><a href="../reference/claude_chat.html">claude_chat()</a></code> that do the work in the background. These functions can also be called directly as an alternative more verbose and provider-specific interface.</p>
</div>
<div class="section level4">
<h4 id="old-usage-0-2-3">Old Usage:<a class="anchor" aria-label="anchor" href="#old-usage-0-2-3"></a></h4>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello World"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="new-usage-0-2-3">New Usage:<a class="anchor" aria-label="anchor" href="#new-usage-0-2-3"></a></h4>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Recommended Verb-Based Approach</span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello World"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co"># Or even configuring a provider outside</span></span>
<span><span class="va">my_ollama</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ollama.html">ollama</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"llama3.2-vision:90B"</span>,</span>
<span>       .ollama_server <span class="op">=</span> <span class="st">"https://ollama.example-server.de"</span>,</span>
<span>       .temperature <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello World"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chat.html">chat</a></span><span class="op">(</span><span class="va">my_ollama</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Alternative Approach is to use more verbose specific functions:</span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Hello World"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/openai_chat.html">openai_chat</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span></span></code></pre></div>
<ul><li>
<strong>Backward Compatibility</strong>:
<ul><li>The old functions (<code><a href="../reference/openai.html">openai()</a></code>, <code><a href="../reference/claude.html">claude()</a></code>, etc.) still work if you directly supply an <code>LLMMessage</code> as arguement, but issue deprecation warnings when used directly for chat.</li>
<li>Users are encouraged to transition to the new interface for future-proof workflows.</li>
</ul></li>
</ul></div>
</div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.2.2" id="version-022">Version 0.2.2<a class="anchor" aria-label="anchor" href="#version-022"></a></h2>
<div class="section level3">
<h3 id="major-features-0-2-2">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-2-2"></a></h3>
<ul><li>Added functions to work with the Google Gemini API, with the new <code><a href="../reference/gemini.html">gemini()</a></code> main API-function</li>
<li>Support for the file upload workflows for Gemini:</li>
</ul><div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Upload a file for use with gemini</span></span>
<span><span class="va">upload_info</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/gemini_upload_file.html">gemini_upload_file</a></span><span class="op">(</span><span class="st">"example.mp3"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#Make the file available during a Gemini API call</span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Summarize this speech"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/gemini.html">gemini</a></span><span class="op">(</span>.fileid <span class="op">=</span> <span class="va">upload_info</span><span class="op">$</span><span class="va">name</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co">#Delte the file from the Google servers</span></span>
<span><span class="fu"><a href="../reference/gemini_delete_file.html">gemini_delete_file</a></span><span class="op">(</span><span class="va">upload_info</span><span class="op">$</span><span class="va">name</span><span class="op">)</span></span></code></pre></div>
<ul><li>Brings video and audio support to tidyllm</li>
<li>Google Gemini is the second API to fully support <code><a href="../reference/tidyllm_schema.html">tidyllm_schema()</a></code>
</li>
<li>
<code><a href="../reference/gemini.html">gemini()</a></code>-requests allow for a wide range of file types that can be used for context in messages</li>
<li>Supported document formats for <code><a href="../reference/gemini.html">gemini()</a></code> file workflows:
<ul><li>
<strong>PDF</strong>: <code>application/pdf</code>
</li>
<li>
<strong>TXT</strong>: <code>text/plain</code>
</li>
<li>
<strong>HTML</strong>: <code>text/html</code>
</li>
<li>
<strong>CSS</strong>: <code>text/css</code>
</li>
<li>
<strong>Markdown</strong>: <code>text/md</code>
</li>
<li>
<strong>CSV</strong>: <code>text/csv</code>
</li>
<li>
<strong>XML</strong>: <code>text/xml</code>
</li>
<li>
<strong>RTF</strong>: <code>text/rtf</code>
</li>
</ul></li>
<li>Supported code formats for <code><a href="../reference/gemini.html">gemini()</a></code> file workflows:
<ul><li>
<strong>JavaScript</strong>: <code>application/x-javascript</code>, <code>text/javascript</code>
</li>
<li>
<strong>Python</strong>: <code>application/x-python</code>, <code>text/x-python</code>
</li>
</ul></li>
<li>Supported image formats for <code><a href="../reference/gemini.html">gemini()</a></code> file workflows:
<ul><li>
<strong>PNG</strong>: <code>image/png</code>
</li>
<li>
<strong>JPEG</strong>: <code>image/jpeg</code>
</li>
<li>
<strong>WEBP</strong>: <code>image/webp</code>
</li>
<li>
<strong>HEIC</strong>: <code>image/heic</code>
</li>
<li>
<strong>HEIF</strong>: <code>image/heif</code>
</li>
</ul></li>
<li>Supported video formats for <code><a href="../reference/gemini.html">gemini()</a></code> file workflows:
<ul><li>
<strong>MP4</strong>: <code>video/mp4</code>
</li>
<li>
<strong>MPEG</strong>: <code>video/mpeg</code>
</li>
<li>
<strong>MOV</strong>: <code>video/mov</code>
</li>
<li>
<strong>AVI</strong>: <code>video/avi</code>
</li>
<li>
<strong>FLV</strong>: <code>video/x-flv</code>
</li>
<li>
<strong>MPG</strong>: <code>video/mpg</code>
</li>
<li>
<strong>WEBM</strong>: <code>video/webm</code>
</li>
<li>
<strong>WMV</strong>: <code>video/wmv</code>
</li>
<li>
<strong>3GPP</strong>: <code>video/3gpp</code>
</li>
</ul></li>
<li>Supported audio formats for <code><a href="../reference/gemini.html">gemini()</a></code> file workflows:
<ul><li>
<strong>WAV</strong>: <code>audio/wav</code>
</li>
<li>
<strong>MP3</strong>: <code>audio/mp3</code>
</li>
<li>
<strong>AIFF</strong>: <code>audio/aiff</code>
</li>
<li>
<strong>AAC</strong>: <code>audio/aac</code>
</li>
<li>
<strong>OGG Vorbis</strong>: <code>audio/ogg</code>
</li>
<li>
<strong>FLAC</strong>: <code>audio/flac</code>
</li>
</ul></li>
</ul></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.2.1" id="version-021">Version 0.2.1<a class="anchor" aria-label="anchor" href="#version-021"></a></h2>
<div class="section level3">
<h3 id="major-features-0-2-1">Major Features:<a class="anchor" aria-label="anchor" href="#major-features-0-2-1"></a></h3>
<ul><li>Added <code><a href="../reference/get_metadata.html">get_metadata()</a></code> function to retrieve and format metadata from <code>LLMMessage</code> objects.</li>
<li>Enhanced the <code>print</code> method for <code>LLMMessage</code> to support printing metadata, controlled via the new <code>tidyllm_print_metadata</code> option or a new <code>.meta</code>-arguement for the print method.</li>
</ul><div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Write a short poem about software development"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co">#Get metdata on token usage and model as tibble  </span></span>
<span><span class="fu"><a href="../reference/get_metadata.html">get_metadata</a></span><span class="op">(</span><span class="va">conversation</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#or print it with the message</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">conversation</span>,.meta<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#Or allways print it</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>tidyllm_print_metadata<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="bug-fixes-0-2-1">Bug-fixes:<a class="anchor" aria-label="anchor" href="#bug-fixes-0-2-1"></a></h3>
<ul><li>Fixed a bug in <code><a href="../reference/send_openai_batch.html">send_openai_batch()</a></code> caused by a missing <code>.json</code>-arguement not being passed for messages without schema</li>
</ul></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.2.0" id="version-020">Version 0.2.0<a class="anchor" aria-label="anchor" href="#version-020"></a></h2><p class="text-muted">CRAN release: 2024-11-07</p>
<p>New CRAN release. Largest changes compared to <strong>0.1.0</strong>:</p>
<p><strong>Major Features:</strong></p>
<ul><li>Batch Request Support: Added support for batch requests with both Anthropic and OpenAI APIs, enabling large-scale request handling.</li>
<li>Schema Support: Improved structured outputs in JSON mode with advanced <code>.json_schema</code> handling in <code><a href="../reference/openai.html">openai()</a></code>, enhancing support for well-defined JSON responses.</li>
<li>Azure OpenAI Integration: Introduced <code><a href="../reference/azure_openai.html">azure_openai()</a></code> function for accessing the Azure OpenAI service, with full support for rate-limiting and batch operations tailored to Azure’s API structure.</li>
<li>Embedding Model Support: Added embedding generation functions for the OpenAI, Ollama, and Mistral APIs, supporting message content and media embedding.</li>
<li>Mistral API Integration: New <code><a href="../reference/mistral.html">mistral()</a></code> function provides full support for Mistral models hosted in the EU, including rate-limiting and streaming capabilities.</li>
<li>PDF Batch Processing: Introduced the <code><a href="../reference/pdf_page_batch.html">pdf_page_batch()</a></code> function, which processes PDFs page by page, allowing users to define page-specific prompts for detailed analysis.</li>
<li>Support for OpenAI-compatible APIs: Introduced a <code>.compatible</code> argument (and flexible url and path) in <code><a href="../reference/openai.html">openai()</a></code> to allow compatibility with third-party OpenAI-compatible APIs.</li>
</ul><p><strong>Improvements:</strong></p>
<ul><li>API Format Refactoring: Complete refactor of <code>to_api_format()</code> to reduce code duplication, simplify API format generation, and improve maintainability.</li>
<li>Improved Error Handling: Enhanced input validation and error messaging for all API-functions functions, making troubleshooting easier.</li>
<li>Rate-Limiting Enhancements: Updated rate limiting to use <code><a href="https://httr2.r-lib.org/reference/req_retry.html" class="external-link">httr2::req_retry()</a></code> in addition to the rate-limit tracking functions in tidyllm, using 429 headers to wait for rate limit resets.</li>
<li>Expanded Testing: Added comprehensive tests for API functions using <code>httptest2</code>
</li>
</ul><p><strong>Breaking Changes:</strong></p>
<ul><li>Redesigned Reply Functions: <code><a href="../reference/get_reply.html">get_reply()</a></code> was split into <code><a href="../reference/get_reply.html">get_reply()</a></code> for text outputs and <code><a href="../reference/get_reply_data.html">get_reply_data()</a></code> for structured outputs, improving type stability compared to an earlier function that had different outputs based on a <code>.json</code>-arguement.</li>
<li>Deprecation of <code><a href="../reference/chatgpt.html">chatgpt()</a></code>: The <code><a href="../reference/chatgpt.html">chatgpt()</a></code> function has been deprecated in favor of <code><a href="../reference/openai.html">openai()</a></code> for feature alignment and improved consistency.</li>
</ul><p><strong>Minor Updates and Bug Fixes:</strong></p>
<ul><li>Expanded PDF Support in <code><a href="../reference/llm_message.html">llm_message()</a></code>: Allows extraction of specific page ranges from PDFs, improving flexibility in document handling.</li>
<li>New <code><a href="../reference/ollama_download_model.html">ollama_download_model()</a></code> function to download models from the Ollama API</li>
<li>All sequential chat API functions now support streaming</li>
</ul></div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.11" id="version-0111">Version 0.1.11<a class="anchor" aria-label="anchor" href="#version-0111"></a></h2>
<div class="section level3">
<h3 id="major-features-0-1-11">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-1-11"></a></h3>
<ul><li>Support for both the Anthropic and the OpenAI batch request API added</li>
<li>New <code>.compatible</code>-arguement in <code><a href="../reference/openai.html">openai()</a></code> to allow working with compatible third party APIs</li>
</ul></div>
<div class="section level3">
<h3 id="improvements-0-1-11">Improvements<a class="anchor" aria-label="anchor" href="#improvements-0-1-11"></a></h3>
<ul><li>
<strong>Complete refactor of <code>to_api_format()</code></strong>: API format generation now has much less code duplication and is more maintainable.</li>
</ul></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.10" id="version-0110">Version 0.1.10<a class="anchor" aria-label="anchor" href="#version-0110"></a></h2>
<div class="section level3">
<h3 id="breaking-changes-0-1-10">Breaking Changes<a class="anchor" aria-label="anchor" href="#breaking-changes-0-1-10"></a></h3>
<ul><li>
<code><a href="../reference/get_reply.html">get_reply()</a></code> was split into two type-stable functions: <code><a href="../reference/get_reply.html">get_reply()</a></code> for text and <code><a href="../reference/get_reply_data.html">get_reply_data()</a></code> for structured outputs.</li>
</ul></div>
<div class="section level3">
<h3 id="improvements-0-1-10">Improvements<a class="anchor" aria-label="anchor" href="#improvements-0-1-10"></a></h3>
<ul><li>
<strong>Rate limiting updated to use <code><a href="https://httr2.r-lib.org/reference/req_retry.html" class="external-link">httr2::req_retry()</a></code></strong>: Rate limiting now uses the right 429 headers where they come.</li>
</ul></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.9" id="version-019">Version 0.1.9<a class="anchor" aria-label="anchor" href="#version-019"></a></h2>
<div class="section level3">
<h3 id="major-features-0-1-9">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-1-9"></a></h3>
<ul><li><p><strong>Enhanced Input Validation</strong>: All API functions now have improved input validation, ensuring better alignment with API documentation</p></li>
<li><p><strong>Improved error handling</strong> More human-readable error messages for failed requests from the API</p></li>
<li><p><strong>Advanced JSON Mode in <code><a href="../reference/openai.html">openai()</a></code></strong>: The <code><a href="../reference/openai.html">openai()</a></code> function now supports advanced <code>.json_schemas</code>, allowing structured output in JSON mode for more precise responses.</p></li>
<li><p><strong>Reasoning Models Support</strong>: Support for O1 reasoning models has been added, with better handling of system prompts in the <code><a href="../reference/openai.html">openai()</a></code> function.</p></li>
<li><p><strong>Streaming callback functions refactored:</strong> Given that the streaming callback format for Open AI, Mistral and Groq is nearly identical the three now rely on the same callback function.</p></li>
</ul></div>
<div class="section level3">
<h3 id="breaking-changes-0-1-9">Breaking Changes<a class="anchor" aria-label="anchor" href="#breaking-changes-0-1-9"></a></h3>
<ul><li>
<strong><code><a href="../reference/chatgpt.html">chatgpt()</a></code> Deprecated</strong>: The <code><a href="../reference/chatgpt.html">chatgpt()</a></code> function has been deprecated in favor of <code><a href="../reference/openai.html">openai()</a></code>. Users should migrate to <code><a href="../reference/openai.html">openai()</a></code> to take advantage of the new features and enhancements.</li>
</ul></div>
<div class="section level3">
<h3 id="improvements-0-1-9">Improvements<a class="anchor" aria-label="anchor" href="#improvements-0-1-9"></a></h3>
<ul><li>
<strong>Better Error Handling</strong>: The <code><a href="../reference/openai.html">openai()</a></code>, <code><a href="../reference/ollama.html">ollama()</a></code>, and <code><a href="../reference/claude.html">claude()</a></code> functions now return more informative error messages when API calls fail, helping with debugging and troubleshooting.</li>
</ul><hr></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.8" id="version-018">Version 0.1.8<a class="anchor" aria-label="anchor" href="#version-018"></a></h2>
<div class="section level3">
<h3 id="major-features-0-1-8">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-1-8"></a></h3>
<ul><li>
<strong>Embedding Models Support:</strong> Embedding model support for three APIs:
<ul><li>Embedding functions process message histories and combine text from message content and media attachments for embedding models.</li>
<li>
<code><a href="../reference/ollama_embedding.html">ollama_embedding()</a></code> to generate embeddings using the Ollama API.</li>
<li>
<code><a href="../reference/openai_embedding.html">openai_embedding()</a></code> to generate embeddings using the OpenAI API.</li>
<li>
<code><a href="../reference/mistral_embedding.html">mistral_embedding()</a></code> to generate embeddings using the Mistral API.</li>
</ul></li>
</ul></div>
<div class="section level3">
<h3 id="improvements-0-1-8">Improvements<a class="anchor" aria-label="anchor" href="#improvements-0-1-8"></a></h3>
<ul><li>
<strong>PDF Page Support in <code><a href="../reference/llm_message.html">llm_message()</a></code>:</strong> The <code><a href="../reference/llm_message.html">llm_message()</a></code> function now supports specifying a range of pages in a PDF by passing a list with <code>filename</code>, <code>start_page</code>, and <code>end_page</code>. This allows users to extract and process specific pages of a PDF.</li>
</ul><hr></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.7" id="version-017">Version 0.1.7<a class="anchor" aria-label="anchor" href="#version-017"></a></h2>
<div class="section level3">
<h3 id="major-features-0-1-7">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-1-7"></a></h3>
<ul><li>
<strong>PDF Page Batch Processing</strong>: Introduced the <code><a href="../reference/pdf_page_batch.html">pdf_page_batch()</a></code> function, which processes PDF files page by page, extracting text and converting each page into an image, allowing for a general prompt or page-specific prompts. The function generates a list of <code>LLMMessage</code> objects that can be sent to an API and work with the batch-API functions in <strong>tidyllm</strong>.</li>
</ul><hr></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.6" id="version-016">Version 0.1.6<a class="anchor" aria-label="anchor" href="#version-016"></a></h2>
<div class="section level3">
<h3 id="major-features-0-1-6">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-1-6"></a></h3>
<ul><li>
<strong>Support for the Mistral API</strong>: New <code><a href="../reference/mistral.html">mistral()</a></code> function to use Mistral Models on Le Platforme on servers hosted in the EU, with rate-limiting and streaming support.</li>
</ul><hr></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.5" id="version-015">Version 0.1.5<a class="anchor" aria-label="anchor" href="#version-015"></a></h2>
<div class="section level3">
<h3 id="major-features-0-1-5">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-1-5"></a></h3>
<ul><li>
<strong>Message Retrieval Functions</strong>: Added functions to retrieve single messages from conversations:
<ul><li>
<code><a href="../reference/get_user_message.html">last_user_message()</a></code> pulls the last message the user sent.</li>
<li>
<code><a href="../reference/get_reply.html">get_reply()</a></code> gets the assistant reply at a given index of assistant messages.</li>
<li>
<code><a href="../reference/get_user_message.html">get_user_message()</a></code> gets the user message at a given index of user messages.</li>
</ul></li>
</ul></div>
<div class="section level3">
<h3 id="improvements-0-1-5">Improvements<a class="anchor" aria-label="anchor" href="#improvements-0-1-5"></a></h3>
<ul><li>
<strong>Easier Troubleshooting in API-function</strong>: All API functions now support the <code>.dry_run</code> argument, allowing users to generate an <code>httr2</code>-request for easier debugging and inspection.</li>
<li>
<strong>API Function Tests:</strong> Implemented <code>httptest2</code>-based tests with mock responses for all API functions, covering both basic functionality and rate-limiting.</li>
</ul><hr></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.4" id="version-014">Version 0.1.4<a class="anchor" aria-label="anchor" href="#version-014"></a></h2>
<div class="section level3">
<h3 id="major-features-0-1-4">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-1-4"></a></h3>
<ul><li>
<strong>New Ollama functions</strong>:
<ul><li>
<strong>Model Download:</strong> Introduced the <code><a href="../reference/ollama_download_model.html">ollama_download_model()</a></code> function to download models from the Ollama API. It supports a streaming mode that provides live progress bar updates on the download progress.</li>
</ul></li>
</ul></div>
<div class="section level3">
<h3 id="improvements-0-1-4">Improvements<a class="anchor" aria-label="anchor" href="#improvements-0-1-4"></a></h3>
<ul><li>Refactoring of <code><a href="../reference/llm_message.html">llm_message()</a></code>
</li>
</ul><hr></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.3" id="version-013">Version 0.1.3<a class="anchor" aria-label="anchor" href="#version-013"></a></h2>
<div class="section level3">
<h3 id="major-features-0-1-3">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-1-3"></a></h3>
<ul><li>The <code><a href="../reference/groq.html">groq()</a></code> function now supports images.</li>
<li>More complete streaming support across API-functions.</li>
</ul></div>
<div class="section level3">
<h3 id="breaking-changes-0-1-3">Breaking Changes<a class="anchor" aria-label="anchor" href="#breaking-changes-0-1-3"></a></h3>
<ul><li>
<strong>Groq Models</strong>: System prompts are no longer sent for Groq models, since many models on Groq do not support them and all multimodal models on Groq disallow them.</li>
</ul><hr></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.2" id="version-012">Version 0.1.2<a class="anchor" aria-label="anchor" href="#version-012"></a></h2>
<div class="section level3">
<h3 id="improvements-0-1-2">Improvements<a class="anchor" aria-label="anchor" href="#improvements-0-1-2"></a></h3>
<ul><li>
<strong>New unit tests for <code><a href="../reference/llm_message.html">llm_message()</a></code></strong>.</li>
<li>Improvements in streaming functions.</li>
</ul><hr></div>
</div>
    <div class="section level2">
<h2 class="pkg-version" data-toc-text="0.1.1" id="version-011">Version 0.1.1<a class="anchor" aria-label="anchor" href="#version-011"></a></h2>
<div class="section level3">
<h3 id="major-features-0-1-1">Major Features<a class="anchor" aria-label="anchor" href="#major-features-0-1-1"></a></h3>
<ul><li><p><strong>JSON Mode</strong>: JSON mode is now more widely supported across all API functions, allowing for structured outputs when APIs support them. The <code>.json</code> argument is now passed only to API functions, specifying how the API should respond, and it is not needed anymore in <code><a href="../reference/get_reply.html">last_reply()</a></code>.</p></li>
<li><p><strong>Improved <code><a href="../reference/get_reply.html">last_reply()</a></code> Behavior</strong>: The behavior of the <code><a href="../reference/get_reply.html">last_reply()</a></code> function has changed. It now automatically handles JSON replies by parsing them into structured data and falling back to raw text in case of errors. You can still force raw text replies even for JSON output using the <code>.raw</code> argument.</p></li>
</ul></div>
<div class="section level3">
<h3 id="breaking-changes-0-1-1">Breaking Changes<a class="anchor" aria-label="anchor" href="#breaking-changes-0-1-1"></a></h3>
<ul><li>
<strong><code><a href="../reference/get_reply.html">last_reply()</a></code></strong>: The <code>.json</code> argument is no longer used, and JSON replies are automatically parsed. Use <code>.raw</code> to force raw text replies.</li>
</ul></div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Eduard Brüll.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

